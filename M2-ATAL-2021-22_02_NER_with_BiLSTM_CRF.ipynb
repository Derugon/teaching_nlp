{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolashernandez/teaching_nlp/blob/main/M2-ATAL-2021-22_02_NER_with_BiLSTM_CRF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWNr2Oc6qJHX"
      },
      "source": [
        "---\n",
        "#¬†Recent Advances in Sequence Labeling from Deep Learning Models\n",
        "\n",
        "Les approches pour l'√©tiquetage de s√©quence fond√©es sur les r√©seaux de neurones profonds compte trois √©tapes :\n",
        "1. The embedding module is the first stage that maps words into their distributed representations (pretrained word embeddings, character-\n",
        "level representations, hand-crafted features and sentence-level\n",
        "representations). \n",
        "2. The context encoder module extracts contextual features (e.g. RNN/Bi-LSTM, CNN)\n",
        "3. and the inference module predict labels and generate optimal label sequence as output of the model (e.g. SoftMax, CRF, RNN). \n",
        "\n",
        "[Zhiyong He, Zanbo Wang, Sheng Jiang. A Survey on Recent Advances in Sequence Labeling from Deep Learning Models. Published 13 November 2020. Computer Science. ArXiv](https://arxiv.org/pdf/2011.06727.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0boga_k7vMTc"
      },
      "source": [
        "---\n",
        "# Bref historique des syst√®mes de NER neuronaux\n",
        "\n",
        "On ne vous demande pas de lire les articles suivants mais √† minima de lire ce bref historique et de jeter un oeil aux sections 2.2 √† 2.5 de (Huang et al., 2015) pour comprendre le mod√®le Bi-LSTM_CRF.\n",
        "\n",
        "* L'architecture \"SENNA\", novatrice dans l'id√©e de la r√©solution des t√¢ches du TAL avec un mod√®le de langue neuronal (incluant notamment une m√©thode de construction de \"pretrained word embeddings\") : R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Language Processing (Almost) from Scratch, Journal of Machine Learning Research (JMLR), 2011. ; [[article]](http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf) ; [[impl√©mentation]](https://ronan.collobert.com/senna/)\n",
        "* Premier article √† appliquer les BiLSTM-CRF au NER : Zhiheng Huang, Wei Xu, Kai Yu, Bidirectional LSTM-CRF Models for Sequence Tagging, Arxiv, Computation and Language, Submitted on 9 Aug 2015 ; [[article]](https://arxiv.org/pdf/1508.01991.pdf) ; [[impl√©mentation1]](https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html) (tutoriel avanc√© de pytorch) ; [[impl√©mentation2]](https://github.com/ZubinGou/NER-BiLSTM-CRF-PyTorch) (inclut aussi un mod√®le Bi-LSTM-CNN-CRF) ; [[impl√©mentation3]](https://github.com/jidasheng/bi-lstm-crf)  ; [[impl√©mentation4]](http://www.gabormelli.com/RKB/index.php?title=Bidirectional_LSTM/CRF_(BiLTSM-CRF)_Training_System) ; [[impl√©mentation5]](https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html) (avec tensorflow)\n",
        "* BiLSTM-CNN-CRF Implementation for Sequence Tagging (extension with the ELMo representations) : Reimers, Nils, and Gurevych, Iryna, Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP), September 2017, Copenhagen, Denmark, 338-348 ; [[article]](http://aclweb.org/anthology/D17-1035) ; [[impl√©mentation]](https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf)\n",
        "* Le 3e mod√®le le plus performant en 2020 sur la t√¢che NER sans ressources externes : Ying Luo, Fengshun Xiao, and Hai Zhao. Hierarchical contextualized representation for named entity recognition. In AAAI, pages 8441‚Äì8448, 2020 ; [[impl√©mentation]](https://github.com/cslydia/Hire-NER) ; Utilise [NCRF++: An Open-source Neural Sequence Labeling Toolkit](https://github.com/jiesutd/NCRFpp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inik1ZFZ2tOa"
      },
      "source": [
        "---\n",
        "#¬†Bidirectional LSTM-CRF Impl√©mentation de (Huang et al., 2015)\n",
        "\n",
        "Le code dans les cellules suivantes provient de l'[impl√©mentation 3](https://github.com/jidasheng/bi-lstm-crf/) de (Huang et al., 2015). Celle-ci s'appuie sur la biblioth√®que pytorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLo59Npr9S5-"
      },
      "source": [
        "\n",
        "### VOTRE TRAVAIL \n",
        "* Ex√©cutez les cellules sans passer trop de temps √† comprendre les d√©tails de l'impl√©mentation. R√©pondez aux questions quand vous y √™tes invit√©.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZoON5ru4-Ed"
      },
      "source": [
        "##¬†Installation des d√©pendances \n",
        "\n",
        "Passez en type d'ex√©cution \"gpu\". Plus tard vous ferez un test en type \"None\" c'est-√†-dire \"cpu\" afin d'avoir une id√©e des temps d'entra√Ænement de l'architecture.\n",
        "\n",
        "üí° La cellule suivante requiert 2 ex√©cutions. Executez la une fois puis lancer une ex√©cution de \"tout\". Vous gagnerez un peu de temps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr7DMncsp_uN",
        "outputId": "99275390-5622-4a25-dd93-91ff6a6b3de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Requirement already satisfied: torchvision==0.7.0 in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "#¬†https://stackoverflow.com/questions/54358280/packed-padded-sequence-gives-error-when-used-with-gpu\n",
        "!pip install torch==1.6.0 torchvision==0.7.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQK2lrEy5PSH"
      },
      "source": [
        "V√©rifie que le hardware de votre machine dispose d'un gpu. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOVf1iODp5Gj",
        "outputId": "48a9edf2-72fa-44f5-b8a5-fcce822ba542"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import torch\n",
        "# Get cpu or gpu device for training.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSzPQYDlMtsu"
      },
      "source": [
        "## D√©finition d'une cellule neuronale CRF \n",
        "\n",
        "* Trouve la s√©quence d'√©tiquettes la plus probable correspondant √† une s√©quence de mots donn√©e\n",
        "* Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/model/crf.py\n",
        "* A l'aide d'un treillis mots x etiquettes, l'_algorithme Viterbi_ retourne la s√©quence d'√©tiquettes la plus probables pour une s√©quence de mots (d'une phrase) donn√©e.\n",
        "* Les _probabilit√©s de transition_ sont des probabilit√©s conditionnelles. Il s'agit de la probabilit√© d'avoir une √©tiquette sachant 1 historique d'√©tiquettes `P(t_i|t_i-1)` (ici dans un mod√®le bigramme). Les _probabilit√©s d'√©mission_ sont les probabilit√©s des mots `P(w_i | t_i)` √† √™tre g√©n√©r√©s par leur propre √©tiquette. \n",
        "* En savoir plus sur le [\"Sequence Labeling\"](https://courses.engr.illinois.edu/cs447/fa2018/Slides/Lecture07.pdf). \n",
        "* En savoir plus sur le [\"CRF\"](http://www.cs.columbia.edu/~mcollins/crf.pdf).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyEIEbBGMp8j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def log_sum_exp(x):\n",
        "    \"\"\"calculate log(sum(exp(x))) = max(x) + log(sum(exp(x - max(x))))\n",
        "    \"\"\"\n",
        "    max_score = x.max(-1)[0]\n",
        "    return max_score + (x - max_score.unsqueeze(-1)).exp().sum(-1).log()\n",
        "\n",
        "\n",
        "IMPOSSIBLE = -1e4\n",
        "\n",
        "class CRF(nn.Module):\n",
        "    \"\"\"General CRF module.\n",
        "    The CRF module contain a inner Linear Layer which transform the input from features space to tag space.\n",
        "\n",
        "    :param in_features: number of features for the input\n",
        "    :param num_tag: number of tags. DO NOT include START, STOP tags, they are included internal.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, num_tags):\n",
        "        super(CRF, self).__init__()\n",
        "\n",
        "        self.num_tags = num_tags + 2\n",
        "        self.start_idx = self.num_tags - 2\n",
        "        self.stop_idx = self.num_tags - 1\n",
        "\n",
        "        self.fc = nn.Linear(in_features, self.num_tags)\n",
        "\n",
        "        # transition factor, Tij mean transition from j to i\n",
        "        self.transitions = nn.Parameter(torch.randn(self.num_tags, self.num_tags), requires_grad=True)\n",
        "        self.transitions.data[self.start_idx, :] = IMPOSSIBLE\n",
        "        self.transitions.data[:, self.stop_idx] = IMPOSSIBLE\n",
        "\n",
        "    def forward(self, features, masks):\n",
        "        \"\"\"decode tags\n",
        "\n",
        "        :param features: [B, L, C], batch of unary scores\n",
        "        :param masks: [B, L] masks\n",
        "        :return: (best_score, best_paths)\n",
        "            best_score: [B]\n",
        "            best_paths: [B, L]\n",
        "        \"\"\"\n",
        "        features = self.fc(features)\n",
        "        return self.__viterbi_decode(features, masks[:, :features.size(1)].float())\n",
        "\n",
        "    def loss(self, features, ys, masks):\n",
        "        \"\"\"negative log likelihood loss\n",
        "        B: batch size, L: sequence length, D: dimension\n",
        "\n",
        "        :param features: [B, L, D]\n",
        "        :param ys: tags, [B, L]\n",
        "        :param masks: masks for padding, [B, L]\n",
        "        :return: loss\n",
        "        \"\"\"\n",
        "        features = self.fc(features)\n",
        "\n",
        "        L = features.size(1)\n",
        "        masks_ = masks[:, :L].float()\n",
        "\n",
        "        forward_score = self.__forward_algorithm(features, masks_)\n",
        "        gold_score = self.__score_sentence(features, ys[:, :L].long(), masks_)\n",
        "        loss = (forward_score - gold_score).mean()\n",
        "        return loss\n",
        "\n",
        "    def __score_sentence(self, features, tags, masks):\n",
        "        \"\"\"Gives the score of a provided tag sequence\n",
        "\n",
        "        :param features: [B, L, C]\n",
        "        :param tags: [B, L]\n",
        "        :param masks: [B, L]\n",
        "        :return: [B] score in the log space\n",
        "        \"\"\"\n",
        "        B, L, C = features.shape\n",
        "\n",
        "        # emission score\n",
        "        emit_scores = features.gather(dim=2, index=tags.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # transition score\n",
        "        start_tag = torch.full((B, 1), self.start_idx, dtype=torch.long, device=tags.device)\n",
        "        tags = torch.cat([start_tag, tags], dim=1)  # [B, L+1]\n",
        "        trans_scores = self.transitions[tags[:, 1:], tags[:, :-1]]\n",
        "\n",
        "        # last transition score to STOP tag\n",
        "        last_tag = tags.gather(dim=1, index=masks.sum(1).long().unsqueeze(1)).squeeze(1)  # [B]\n",
        "        last_score = self.transitions[self.stop_idx, last_tag]\n",
        "\n",
        "        score = ((trans_scores + emit_scores) * masks).sum(1) + last_score\n",
        "        return score\n",
        "\n",
        "    def __viterbi_decode(self, features, masks):\n",
        "        \"\"\"decode to tags using viterbi algorithm\n",
        "\n",
        "        :param features: [B, L, C], batch of unary scores\n",
        "        :param masks: [B, L] masks\n",
        "        :return: (best_score, best_paths)\n",
        "            best_score: [B]\n",
        "            best_paths: [B, L]\n",
        "        \"\"\"\n",
        "        B, L, C = features.shape\n",
        "\n",
        "        bps = torch.zeros(B, L, C, dtype=torch.long, device=features.device)  # back pointers\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        max_score = torch.full((B, C), IMPOSSIBLE, device=features.device)  # [B, C]\n",
        "        max_score[:, self.start_idx] = 0\n",
        "\n",
        "        for t in range(L):\n",
        "            mask_t = masks[:, t].unsqueeze(1)  # [B, 1]\n",
        "            emit_score_t = features[:, t]  # [B, C]\n",
        "\n",
        "            # [B, 1, C] + [C, C]\n",
        "            acc_score_t = max_score.unsqueeze(1) + self.transitions  # [B, C, C]\n",
        "            acc_score_t, bps[:, t, :] = acc_score_t.max(dim=-1)\n",
        "            acc_score_t += emit_score_t\n",
        "            max_score = acc_score_t * mask_t + max_score * (1 - mask_t)  # max_score or acc_score_t\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        max_score += self.transitions[self.stop_idx]\n",
        "        best_score, best_tag = max_score.max(dim=-1)\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_paths = []\n",
        "        bps = bps.cpu().numpy()\n",
        "        for b in range(B):\n",
        "            best_tag_b = best_tag[b].item()\n",
        "            seq_len = int(masks[b, :].sum().item())\n",
        "\n",
        "            best_path = [best_tag_b]\n",
        "            for bps_t in reversed(bps[b, :seq_len]):\n",
        "                best_tag_b = bps_t[best_tag_b]\n",
        "                best_path.append(best_tag_b)\n",
        "            # drop the last tag and reverse the left\n",
        "            best_paths.append(best_path[-2::-1])\n",
        "\n",
        "        return best_score, best_paths\n",
        "\n",
        "    def __forward_algorithm(self, features, masks):\n",
        "        \"\"\"calculate the partition function with forward algorithm.\n",
        "        TRICK: log_sum_exp([x1, x2, x3, x4, ...]) = log_sum_exp([log_sum_exp([x1, x2]), log_sum_exp([x3, x4]), ...])\n",
        "\n",
        "        :param features: features. [B, L, C]\n",
        "        :param masks: [B, L] masks\n",
        "        :return:    [B], score in the log space\n",
        "        \"\"\"\n",
        "        B, L, C = features.shape\n",
        "\n",
        "        scores = torch.full((B, C), IMPOSSIBLE, device=features.device)  # [B, C]\n",
        "        scores[:, self.start_idx] = 0.\n",
        "        trans = self.transitions.unsqueeze(0)  # [1, C, C]\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for t in range(L):\n",
        "            emit_score_t = features[:, t].unsqueeze(2)  # [B, C, 1]\n",
        "            score_t = scores.unsqueeze(1) + trans + emit_score_t  # [B, 1, C] + [1, C, C] + [B, C, 1] => [B, C, C]\n",
        "            score_t = log_sum_exp(score_t)  # [B, C]\n",
        "\n",
        "            mask_t = masks[:, t].unsqueeze(1)  # [B, 1]\n",
        "            scores = score_t * mask_t + scores * (1 - mask_t)\n",
        "        scores = log_sum_exp(scores + self.transitions[self.stop_idx])\n",
        "        return scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byc18sR5FNSu"
      },
      "source": [
        "### VOTRE TRAVAIL\n",
        "\n",
        "Dans GColab, Faire Outils > Param√®tres > Cocher \"affichage de la num√©rotation des lignes\"\n",
        "\n",
        "* Quel est le nom de la _loss function_ ? A quelle ligne est-ce sp√©cifi√©e ?\n",
        "* En quelques mots, √† quoi sert l'algorithme de Viterbi ? Cherchez sur le web...\n",
        "\n",
        "Eventuellement, en apprendre davantage sur quelques [_loss functions_](https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LZZnpMvM20E"
      },
      "source": [
        "## D√©finition d'une architecture neuronale Bi-LSTM CRF\n",
        "\n",
        "La classe suivante impl√©mente un mod√®le Bi-LSTM CRF\n",
        "- Construction des embeddings de la s√©quence\n",
        "- Capture du contexte avec une cellule RNN \n",
        "- Pr√©diction de la s√©quence d'√©tiquetage √† l'aide de la cellule CRF qui prend comme input la sortie du RNN\n",
        "\n",
        "Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/model/model.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvjvPnHDM3Nx"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class BiRnnCrf(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, num_rnn_layers=1, rnn=\"lstm\"):\n",
        "        super(BiRnnCrf, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tagset_size = tagset_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        RNN = nn.LSTM if rnn == \"lstm\" else nn.GRU\n",
        "        self.rnn = RNN(embedding_dim, hidden_dim // 2, num_layers=num_rnn_layers,\n",
        "                       bidirectional=True, batch_first=True)\n",
        "        self.crf = CRF(hidden_dim, self.tagset_size)\n",
        "\n",
        "    def __build_features(self, sentences):\n",
        "        masks = sentences.gt(0)\n",
        "        embeds = self.embedding(sentences.long())\n",
        "        \n",
        "        seq_length = masks.sum(1)\n",
        "        sorted_seq_length, perm_idx = seq_length.sort(descending=True)\n",
        "        embeds = embeds[perm_idx, :]\n",
        "\n",
        "        pack_sequence = pack_padded_sequence(embeds, lengths=sorted_seq_length,  batch_first=True)\n",
        "        packed_output, _ = self.rnn(pack_sequence)\n",
        "        lstm_out, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "        _, unperm_idx = perm_idx.sort()\n",
        "        lstm_out = lstm_out[unperm_idx, :]\n",
        "        return lstm_out, masks\n",
        "\n",
        "    def loss(self, xs, tags):\n",
        "        features, masks = self.__build_features(xs)\n",
        "        loss = self.crf.loss(features, tags, masks=masks)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, xs):\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        features, masks = self.__build_features(xs)\n",
        "        scores, tag_seq = self.crf(features, masks)\n",
        "        return scores, tag_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcA7yrXSDkPP"
      },
      "source": [
        "### VOTRE TRAVAIL\n",
        "\n",
        "Dans GColab, Faire Outils > Param√®tres > Cocher \"affichage de la num√©rotation des lignes\"\n",
        "\n",
        "* Une couche d'[Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) est une table qui associe √† un mot du vocabulaire (en fait son indice num√©rique) un vecteur d'embeddings. Les valeurs des embeddings sont initialement tir√©es al√©atoirement. Elles peuvent √™tre surcharg√©es en chargeant des embeddings pre-entra√Æn√©es avec Word2Vec, Glove ou FastText par exemple. Par d√©faut (`embedding.weight.requires_grad = True`), ces vecteurs seront consid√©r√©s comme des param√®tres du mod√®le et ils seront \"_fine-tuned_\" durant l'entra√Ænement (`train`) par _\"backpropagation\"_. Indiquez le num√©ro de ligne qui d√©finit la couche d'embedding et celui de la ligne o√π les embeddings sont initialis√©es. \n",
        "Plus d'information sur [embedding-in-pytorch](https://stackoverflow.com/questions/50747947/embedding-in-pytorch) (stackoverflow).\n",
        "* L'impl√©mentation offre deux types de cellules RNN possibles. Indiquez la ligne o√π ce choix est possible. Indiquez la ligne qui sp√©cifie le choix par d√©faut.\n",
        "* Apr√®s la repr√©sentation en embeddings des phrase et avant le passage √† la cellule RNN, quel type de traitement est r√©alis√© ? Indiquez le num√©ro de ligne o√π ce traitement est sp√©cifi√©. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGl1Xe3C9rVd"
      },
      "source": [
        "---\n",
        "##¬†D√©finition des pr√©traitement des donn√©es \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViLNx_efOyez"
      },
      "source": [
        "D'abord la d√©finition de m√©thodes \"utils\" pour la phase de pr√©traitement √† savoir la sauvegarde et le chargement de fichiers de configuration e.g. vocabulaire, jeu d'√©tiquettes, param√®tres du mod√®le neuronal (dimension des embeddings...), partition des donn√©es g√©n√©r√©es...\n",
        "\n",
        "Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/app/preprocessing/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUPqw3j6Oyte"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "\n",
        "PAD = \"<PAD>\"\n",
        "OOV = \"<OOV>\"\n",
        "\n",
        "\n",
        "def save_json_file(obj, file_path):\n",
        "    with open(file_path, \"w\", encoding=\"utf8\") as f:\n",
        "        f.write(json.dumps(obj, ensure_ascii=False))\n",
        "\n",
        "\n",
        "def load_json_file(file_path):\n",
        "    with open(file_path, encoding=\"utf8\") as f:\n",
        "        return json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhsxcoePO-4C"
      },
      "source": [
        "Puis la classe de pr√©-traitement des donn√©es qui sera initialis√© √† l'aide des chemins des fichiers contenant le vocabulaire, le jeu d'√©tiquettes et les donn√©es annot√©es (phrases d√©coup√©es en mots avec √©tiquettes). Outre charger ces fichiers de configuration et donn√©es, la classe partitionne les donn√©es en ensemble d'entrainement, de validation et de tests (d'apr√®s les param√®tres sp√©cifi√©s par d√©faut ou √† l'appel du syst√®me). Les donn√©es sont aussi \"vectoris√©es\". Il s'agit essentiellement d'une substitution des mots des phrases par leur identifiant num√©rique correspondant √† une entr√©e dans le vocabulaire donn√©.\n",
        "\n",
        "https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/app/preprocessing/preprocess.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyRopi-JO799"
      },
      "outputs": [],
      "source": [
        "from os.path import join, exists\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "#FILE_VOCAB = \"vocab.json\"\n",
        "#FILE_TAGS = \"tags.json\"\n",
        "#FILE_DATASET = \"dataset.txt\"\n",
        "#FILE_DATASET_CACHE = \"dataset_cache_{}.npz\"\n",
        "\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, config_dir, save_config_dir=None, verbose=True):\n",
        "        self.config_dir = config_dir\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.vocab, self.vocab_dict = self.__load_list_file(FILE_VOCAB, offset=1, verbose=verbose)\n",
        "        self.tags, self.tags_dict = self.__load_list_file(FILE_TAGS, verbose=verbose)\n",
        "        if save_config_dir:\n",
        "            self.__save_config(save_config_dir)\n",
        "\n",
        "        self.PAD_IDX = 0\n",
        "        self.OOV_IDX = len(self.vocab)\n",
        "        self.__adjust_vocab()\n",
        "\n",
        "    def __load_list_file(self, file_name, offset=0, verbose=False):\n",
        "        file_path = join(self.config_dir, file_name)\n",
        "        if not exists(file_path):\n",
        "            raise ValueError('\"{}\" file does not exist.'.format(file_path))\n",
        "        else:\n",
        "            elements = load_json_file(file_path)\n",
        "            elements_dict = {w: idx + offset for idx, w in enumerate(elements)}\n",
        "            if verbose:\n",
        "                print(\"config {} loaded\".format(file_path))\n",
        "            return elements, elements_dict\n",
        "\n",
        "    def __adjust_vocab(self):\n",
        "        self.vocab.insert(0, PAD)\n",
        "        self.vocab_dict[PAD] = 0\n",
        "\n",
        "        self.vocab.append(OOV)\n",
        "        self.vocab_dict[OOV] = len(self.vocab) - 1\n",
        "\n",
        "    def __save_config(self, dst_dir):\n",
        "        char_file = join(dst_dir, FILE_VOCAB)\n",
        "        save_json_file(self.vocab, char_file)\n",
        "\n",
        "        tag_file = join(dst_dir, FILE_TAGS)\n",
        "        save_json_file(self.tags, tag_file)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"tag dict file => {}\".format(tag_file))\n",
        "            print(\"tag dict file => {}\".format(char_file))\n",
        "\n",
        "    @staticmethod\n",
        "    def __cache_file_path(corpus_dir, max_seq_len):\n",
        "        return join(corpus_dir, FILE_DATASET_CACHE.format(max_seq_len))\n",
        "\n",
        "    def load_dataset(self, corpus_dir, val_split, test_split, max_seq_len):\n",
        "        \"\"\"load the train set\n",
        "        :return: (xs, ys)\n",
        "            xs: [B, L]\n",
        "            ys: [B, L, C]\n",
        "        \"\"\"\n",
        "        ds_path = self.__cache_file_path(corpus_dir, max_seq_len)\n",
        "        if not exists(ds_path):\n",
        "            xs, ys = self.__build_corpus(corpus_dir, max_seq_len)\n",
        "        else:\n",
        "            print(\"loading dataset {} ...\".format(ds_path))\n",
        "            dataset = np.load(ds_path)\n",
        "            xs, ys = dataset[\"xs\"], dataset[\"ys\"]\n",
        "\n",
        "        xs, ys = map(\n",
        "            torch.tensor, (xs, ys)\n",
        "        )\n",
        "\n",
        "        # split the dataset\n",
        "        total_count = len(xs)\n",
        "        assert total_count == len(ys)\n",
        "        val_count = int(total_count * val_split)\n",
        "        test_count = int(total_count * test_split)\n",
        "        train_count = total_count - val_count - test_count\n",
        "        assert train_count > 0 and val_count > 0\n",
        "\n",
        "        indices = np.cumsum([0, train_count, val_count, test_count])\n",
        "        datasets = [(xs[s:e], ys[s:e]) for s, e in zip(indices[:-1], indices[1:])]\n",
        "        print(\"datasets loaded:\")\n",
        "        for (xs_, ys_), name in zip(datasets, [\"train\", \"val\", \"test\"]):\n",
        "            print(\"\\t{}: {}, {}\".format(name, xs_.shape, ys_.shape))\n",
        "        return datasets\n",
        "\n",
        "    def decode_tags(self, batch_tags):\n",
        "        batch_tags = [\n",
        "            [self.tags[t] for t in tags]\n",
        "            for tags in batch_tags\n",
        "        ]\n",
        "        return batch_tags\n",
        "\n",
        "    def sent_to_vector(self, sentence, max_seq_len=0):\n",
        "        max_seq_len = max_seq_len if max_seq_len > 0 else len(sentence)\n",
        "        for c in sentence[:max_seq_len]:\n",
        "          if not(c in self.vocab_dic):\n",
        "            print ('c not in vocab')\n",
        "        vec = [self.vocab_dict.get(c, self.OOV_IDX) for c in sentence[:max_seq_len]]\n",
        "\n",
        "        return vec + [self.PAD_IDX] * (max_seq_len - len(vec))\n",
        "\n",
        "    def tags_to_vector(self, tags, max_seq_len=0):\n",
        "        max_seq_len = max_seq_len if max_seq_len > 0 else len(tags)\n",
        "        vec = [self.tags_dict[c] for c in tags[:max_seq_len]]\n",
        "        return vec + [0] * (max_seq_len - len(vec))\n",
        "\n",
        "    def __build_corpus(self, corpus_dir, max_seq_len):\n",
        "      #¬†remove cache files !!!\n",
        "        file_path = join(corpus_dir, FILE_DATASET)\n",
        "        xs, ys = [], []\n",
        "        with open(file_path, encoding=\"utf8\") as f:\n",
        "            for idx, line in tqdm(enumerate(f), desc=\"parsing {}\".format(file_path)):\n",
        "                fields = line.strip().split(\"\\t\")\n",
        "                if len(fields) != 2:\n",
        "                    raise ValueError(\"format error in line {}, tabs count: {}\".format(idx + 1, len(fields) - 1))\n",
        "\n",
        "                sentence, tags = fields\n",
        "\n",
        "\n",
        "                try:\n",
        "                    if sentence[0] == \"[\":\n",
        "                        sentence = json.loads(sentence)\n",
        "                    tags = json.loads(tags)\n",
        "\n",
        "                    #print ('Debug: sentence', sentence)\n",
        "                    #print ('Debug: tags', tags)\n",
        "\n",
        "                    xs.append(self.sent_to_vector(sentence, max_seq_len=max_seq_len))\n",
        "                    ys.append(self.tags_to_vector(tags, max_seq_len=max_seq_len))\n",
        "                    if len(sentence) != len(tags):\n",
        "                        raise ValueError('\"sentence length({})\" != \"tags length({})\" in line {}\"'.format(\n",
        "                            len(sentence), len(tags), idx + 1))\n",
        "                except Exception as e:\n",
        "                    raise ValueError(\"exception raised when parsing line {}\\n\\t{}\\n\\t{}\".format(idx + 1, line, e))\n",
        "\n",
        "        xs, ys = np.asarray(xs), np.asarray(ys)\n",
        "\n",
        "        # save train set\n",
        "        cache_file = self.__cache_file_path(corpus_dir, max_seq_len)\n",
        "        np.savez(cache_file, xs=xs, ys=ys)\n",
        "        print(\"dataset cache({}, {}) => {}\".format(xs.shape, ys.shape, cache_file))\n",
        "        return xs, ys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS9TOT0hPgd4"
      },
      "source": [
        "## D√©finition du mod√®le et de la m√©thode d'entra√Ænement \n",
        "\n",
        "D'abord la d√©finition de la m√©thode qui instancie l'architecture. Les fichiers associ√©s (_model_ et _arguments_) seront sauv√©s (ou charg√©s si un pr√©c√©dent entra√Ænement a d√©j√† eu lieu)  depuis _model_dir_.\n",
        "\n",
        "Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/app/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQjVXdb8Pdjp"
      },
      "outputs": [],
      "source": [
        "from os.path import exists, join\n",
        "import torch\n",
        "\n",
        "#FILE_ARGUMENTS = \"arguments.json\"\n",
        "#FILE_MODEL = \"model.pth\"\n",
        "\n",
        "def arguments_filepath(model_dir):\n",
        "    return join(model_dir, FILE_ARGUMENTS)\n",
        "\n",
        "\n",
        "def model_filepath(model_dir):\n",
        "    return join(model_dir, FILE_MODEL)\n",
        "\n",
        "\n",
        "def build_model(args, processor, load=True, verbose=False):\n",
        "  # NH FIX not actived rnn_type by adding rnn=args['rnn_type']\n",
        "    model = BiRnnCrf(len(processor.vocab), len(processor.tags), embedding_dim=args['embedding_dim'], hidden_dim=args['hidden_dim'], num_rnn_layers=args['num_rnn_layers'], rnn=args['rnn_type'])\n",
        "\n",
        "    # weights\n",
        "    model_path = model_filepath(args['model_dir'])\n",
        "    if exists(model_path) and load:\n",
        "        state_dict = torch.load(model_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "        if verbose:\n",
        "            print(\"load model weights from {}\".format(model_path))\n",
        "    return model\n",
        "\n",
        "\n",
        "def running_device(device):\n",
        "    if torch.cuda.is_available():\n",
        "      print ('running_device gpu')\n",
        "    else:  print ('running_device cpu')\n",
        "    return device if device else torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znjmr60SPtZ4"
      },
      "source": [
        "Puis la d√©finition des m√©thodes d√©di√©es √† l'entra√Ænement du mod√®le\n",
        "\n",
        "Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/app/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zYXtKrJPssX"
      },
      "outputs": [],
      "source": [
        "from os import mkdir\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def __eval_model(model, device, dataloader, desc):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # eval\n",
        "        losses, nums = zip(*[\n",
        "            (model.loss(xb.to(device), yb.to(device)), len(xb))\n",
        "            for xb, yb in tqdm(dataloader, desc=desc)])\n",
        "        return np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "\n",
        "\n",
        "def __save_loss(losses, file_path):\n",
        "    pd.DataFrame(data=losses, columns=[\"epoch\", \"batch\", \"train_loss\", \"val_loss\"]).to_csv(file_path, index=False)\n",
        "\n",
        "\n",
        "def __save_model(model_dir, model):\n",
        "    model_path = model_filepath(model_dir)\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(\"save model => {}\".format(model_path))\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    model_dir = args['model_dir']\n",
        "    if not exists(model_dir):\n",
        "        mkdir(model_dir)\n",
        "#    save_json_file(vars(args), arguments_filepath(model_dir))\n",
        "    save_json_file(args, arguments_filepath(model_dir))\n",
        "\n",
        "    preprocessor = Preprocessor(config_dir=args['corpus_dir'], save_config_dir=args['model_dir'], verbose=True)\n",
        "    model = build_model(args, preprocessor, load=args['recovery'], verbose=True)\n",
        "\n",
        "    # loss\n",
        "    loss_path = join(args['model_dir'], \"loss.csv\")\n",
        "    losses = pd.read_csv(loss_path).values.tolist() if args['recovery'] and exists(loss_path) else []\n",
        "\n",
        "    # datasets\n",
        "    (x_train, y_train), (x_val, y_val), (x_test, y_test) = preprocessor.load_dataset(\n",
        "        args['corpus_dir'], args['val_split'], args['test_split'], max_seq_len=args['max_seq_len'])\n",
        "    train_dl = DataLoader(TensorDataset(x_train, y_train), batch_size=args['batch_size'], shuffle=True)\n",
        "    valid_dl = DataLoader(TensorDataset(x_val, y_val), batch_size=args['batch_size'] * 2)\n",
        "    test_dl = DataLoader(TensorDataset(x_test, y_test), batch_size=args['batch_size'] * 2)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
        "\n",
        "    device = running_device(args['device'])\n",
        "    model.to(device)\n",
        "\n",
        "    val_loss = 0\n",
        "    best_val_loss = 1e4\n",
        "    for epoch in range(args['num_epoch']):\n",
        "        # train\n",
        "        model.train()\n",
        "        bar = tqdm(train_dl)\n",
        "        for bi, (xb, yb) in enumerate(bar):\n",
        "            model.zero_grad()\n",
        "\n",
        "            loss = model.loss(xb.to(device), yb.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            bar.set_description(\"{:2d}/{} loss: {:5.2f}, val_loss: {:5.2f}\".format(\n",
        "                epoch+1, args['num_epoch'], loss, val_loss))\n",
        "            losses.append([epoch, bi, loss.item(), np.nan])\n",
        "\n",
        "        # evaluation\n",
        "        val_loss = __eval_model(model, device, dataloader=valid_dl, desc=\"eval\").item()\n",
        "        # save losses\n",
        "        losses[-1][-1] = val_loss\n",
        "        __save_loss(losses, loss_path)\n",
        "\n",
        "        # save model\n",
        "        if not args['save_best_val_model'] or val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            __save_model(args['model_dir'], model)\n",
        "            print(\"save model(epoch: {}) => {}\".format(epoch, loss_path))\n",
        "\n",
        "    # test\n",
        "    test_loss = __eval_model(model, device, dataloader=test_dl, desc=\"test\").item()\n",
        "    last_loss = losses[-1][:]\n",
        "    last_loss[-1] = test_loss\n",
        "    losses.append(last_loss)\n",
        "    __save_loss(losses, loss_path)\n",
        "    print(\"training completed. test loss: {:.2f}\".format(test_loss))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssoc_QOnVUVH"
      },
      "source": [
        "## D√©finition de la m√©thode de pr√©diction (utilisation du mod√®le)\n",
        "\n",
        "La classe WordsTagger effectue l'√©tiquetage √† proprement parler d'une nouvelle s√©quence de mots. Elle requiert le chemin vers un mod√®le.\n",
        "\n",
        "Source : https://github.com/jidasheng/bi-lstm-crf/blob/master/bi_lstm_crf/app/predict.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aPsc2UMVTSH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class WordsTagger:\n",
        "    def __init__(self, model_dir, device=None):\n",
        "        args = load_json_file(arguments_filepath(model_dir))\n",
        "        #args = dict()\n",
        "        args['model_dir'] = model_dir\n",
        "        self.args = args\n",
        "\n",
        "        self.preprocessor = Preprocessor(config_dir=model_dir, verbose=False)\n",
        "        self.model = build_model(self.args, self.preprocessor, load=True, verbose=False)\n",
        "        self.device = running_device(device)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def __call__(self, sentences, begin_tags=\"BS\"):\n",
        "        \"\"\"predict texts\n",
        "        :param sentences: a text or a list of text\n",
        "        :param begin_tags: begin tags for the beginning of a span\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if not isinstance(sentences, (list, tuple)):\n",
        "            raise ValueError(\"sentences must be a list of sentence\")\n",
        "\n",
        "        try:\n",
        "            sent_tensor = np.asarray([self.preprocessor.sent_to_vector(s) for s in sentences])\n",
        "            sent_tensor = torch.from_numpy(sent_tensor).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                _, tags = self.model(sent_tensor)\n",
        "            tags = self.preprocessor.decode_tags(tags)\n",
        "        except RuntimeError as e:\n",
        "            print(\"*** runtime error: {}\".format(e))\n",
        "            raise e\n",
        "        return tags, self.tokens_from_tags(sentences, tags, begin_tags=begin_tags)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokens_from_tags(sentences, tags_list, begin_tags):\n",
        "        \"\"\"extract entities from tags\n",
        "        :param sentences: a list of sentence\n",
        "        :param tags_list: a list of tags\n",
        "        :param begin_tags:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if not tags_list:\n",
        "            return []\n",
        "\n",
        "        def _tokens(sentence, ts):\n",
        "            # begins: [(idx, label), ...]\n",
        "            all_begin_tags = begin_tags + \"O\"\n",
        "            begins = [(idx, t[2:]) for idx, t in enumerate(ts) if t[0] in all_begin_tags]\n",
        "            begins = [\n",
        "                         (idx, label)\n",
        "                         for idx, label in begins\n",
        "                         if ts[idx] != \"O\" or (idx > 0 and ts[idx - 1] != \"O\")\n",
        "                     ] + [(len(ts), \"\")]\n",
        "\n",
        "            tokens_ = [(sentence[s:e], label) for (s, label), (e, _) in zip(begins[:-1], begins[1:]) if label]\n",
        "            return [((t, tag) if tag else t) for t, tag in tokens_]\n",
        "\n",
        "        tokens_list = [_tokens(sentence, ts) for sentence, ts in zip(sentences, tags_list)]\n",
        "        return tokens_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjl9xpPIIj54"
      },
      "source": [
        "---\n",
        "## Entrainement effectif du mod√®le "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_WA1wg_JdOP"
      },
      "source": [
        "### Pr√©paration des donn√©es d'entra√Ænement WikiNER et des fichiers de configuration requis\n",
        "\n",
        "R√©cup√©ration des donn√©es d'entra√Ænement Wikiner et pr√©pation des fichiers de configuration : vocabulaire, √©tiquettes et donn√©es au format du code utilis√©.\n",
        "\n",
        "Apr√®s ex√©cution de la cellule, consulter le r√©pertoire `data` pour y trouver les fichiers tagset, vocab et txt produits pour le syst√®me NER pr√©c√©demment d√©fini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXeEbpt4eJ2C",
        "outputId": "bc01d041-b79a-4c81-8dbc-7667561b8025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-05 23:41:29--  https://github.com/nicolashernandez/teaching_nlp/raw/main/data/wikiner_ud.joblib.bz2\n",
            "Resolving github.com (github.com)... 13.114.40.48\n",
            "Connecting to github.com (github.com)|13.114.40.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/nicolashernandez/teaching_nlp/main/data/wikiner_ud.joblib.bz2 [following]\n",
            "--2022-01-05 23:41:30--  https://raw.githubusercontent.com/nicolashernandez/teaching_nlp/main/data/wikiner_ud.joblib.bz2\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25546402 (24M) [application/octet-stream]\n",
            "Saving to: ‚Äòdata/wikiner_ud.joblib.bz2‚Äô\n",
            "\n",
            "wikiner_ud.joblib.b 100%[===================>]  24.36M   140MB/s    in 0.2s    \n",
            "\n",
            "2022-01-05 23:41:32 (140 MB/s) - ‚Äòdata/wikiner_ud.joblib.bz2‚Äô saved [25546402/25546402]\n",
            "\n",
            "#sentences:  132257\n",
            "#vocab:  108023\n",
            "#tags in tagset:  9\n",
            "first sentence: [('Il', 'PRON', 'O'), ('assure', 'VERB', 'O'), ('√†', 'ADP', 'O'), ('la', 'DET', 'O'), ('suite', 'NOUN', 'O'), ('de', 'ADP', 'I-PER'), ('Saussure', 'NOUN', 'I-PER'), ('le', 'DET', 'O'), ('cours', 'NOUN', 'O'), ('de', 'ADP', 'O'), ('grammaire', 'ADJ', 'O'), ('compar√©e', 'VERB', 'O'), (',', 'PUNCT', 'O'), (\"qu'\", 'SCONJ', 'O'), ('il', 'PRON', 'O'), ('compl√®te', 'VERB', 'O'), ('√†', 'ADP', 'O'), ('partir', 'VERB', 'O'), ('de', 'ADP', 'O'), ('1894', 'NUM', 'O'), ('par', 'ADP', 'O'), ('une', 'DET', 'O'), ('conf√©rence', 'NOUN', 'O'), ('sur', 'ADP', 'O'), (\"l'\", 'DET', 'O'), ('iranien', 'NOUN', 'O'), ('.', 'PUNCT', 'O')]\n",
            "tagset:  {'I-MISC', 'B-ORG', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-LOC', 'O', 'B-MISC'}\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data \n",
        "!wget -nc https://github.com/nicolashernandez/teaching_nlp/raw/main/data/wikiner_ud.joblib.bz2 -P data\n",
        "!bzip2 -dk data/wikiner_ud.joblib.bz2\n",
        "\n",
        "# Loading the corpus \n",
        "from joblib import load\n",
        "wikiner_corpus = load('data/wikiner_ud.joblib') \n",
        "\n",
        "# Aper√ßu du nombre de phrases et d'une phrase annot√©e (liste de tokens compos√©s de la forme, de la cat√©gorie grammaticale et de l'√©tiquette BIO correspondant en l'entit√© nomm√©e.\n",
        "print ('#sentences: ', len(wikiner_corpus))\n",
        "# 132257\n",
        "vocab = set()\n",
        "tagset = set()\n",
        "for s in wikiner_corpus:\n",
        "  for w,p,n in s:\n",
        "    vocab.add(w.lower())\n",
        "    tagset.add(n)\n",
        "print ('#vocab: ', len(vocab))\n",
        "print ('#tags in tagset: ', len(tagset))\n",
        "\n",
        "print ('first sentence:', wikiner_corpus[0]) \n",
        "\n",
        "print ('tagset: ', tagset)\n",
        "\n",
        "# {'B-LOC', 'B-ORG', 'I-ORG', 'B-MISC', 'I-MISC', 'I-LOC', 'B-PER', 'I-PER', 'O'}\n",
        "\n",
        "# export\n",
        "with open('data/wikiner_corpus.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in wikiner_corpus:\n",
        "      sentence = list()\n",
        "      tags = list()    \n",
        "      for w,p,n in line:\n",
        "        sentence.append(w.lower())\n",
        "        tags.append(n)\n",
        "      f.write('{}\\t{}\\n'.format(json.dumps(sentence), json.dumps(tags)))\n",
        "\n",
        "# export tagset au format bi_lstm_crf \n",
        "import json\n",
        "with open('data/wikiner_corpus_tagset.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(list(tagset), f, ensure_ascii=False)\n",
        "\n",
        "# export vocab au format bi_lstm_crf \n",
        "with open('data/wikiner_corpus_vocab.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(list(vocab), f, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGLGylxgJ8WP"
      },
      "source": [
        "D√©claration du r√©pertoire de donn√©es et des noms des fichiers de vocab, du jeu d'√©tiquettes et du corpus √©tiquet√©s. En fait les noms des repertoires des donn√©es et du mod√®les sont d√©finis un peu plus bas..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cW6hifzxd4Ra"
      },
      "outputs": [],
      "source": [
        "FILE_VOCAB = \"wikiner_corpus_vocab.json\"\n",
        "FILE_VOCAB = \"w2v_pretrained_embeddings_vocab.json\"\n",
        "FILE_TAGS = \"wikiner_corpus_tagset.json\"\n",
        "FILE_DATASET = \"wikiner_corpus.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8ismdbZLeRo"
      },
      "source": [
        "D√©claration du r√©pertoire du mod√®le qui sera g√©n√©r√© "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T177RCDzLXSD"
      },
      "outputs": [],
      "source": [
        "FILE_DATASET_CACHE = \"dataset_cache_{}.npz\"\n",
        "FILE_ARGUMENTS = \"arguments.json\"\n",
        "FILE_MODEL = \"model.pth\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeiBr7WGL3Rf"
      },
      "source": [
        "###¬†Ex√©cution de l'entra√Ænement √† partir de param√®tres du mod√®le donn√©s\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKMQ1SrmTTnl"
      },
      "source": [
        "#### VOTRE TRAVAIL\n",
        "\n",
        "* Avec le type d'ex√©cution \"cpu\", l'entra√Ænement peut prendre quelques minutes. Regardez le temps approximatif annonc√© pour 1 √©poque. Passez en type \"gpu\" et comparez le temps.\n",
        "* Avec les param√®tres par d√©faut, quelle score de loss obtenez-vous pour les donn√©es de validation suite √† la derni√®re √©poque d'entra√Ænement ? Et sur les donn√©es de test ?\n",
        "\n",
        "‚ö†Ô∏è Attention, l'impl√©mentation cherchera √† charger une configuration existante dans le r√©pertoire du mod√®le sp√©cifi√©. Si vous changez le param√©trage alors supprimer les fichiers sp√©cifiques au mod√®le ou bien sp√©cifier un nouveau r√©pertoire pour le nouveau mod√®le.\n",
        "L'erreur `RuntimeError: Error(s) in loading state_dict for BiRnnCrf:` est retourn√©e quand vous lancez un entra√Ænement (`train`) apr√®s avoir modifi√© des param√®tres qui ne sont plus en coh√©rence avec une configuration d√©j√† pr√©sente dans le r√©pertoire `model_dir`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-ZDugjeXRt_",
        "outputId": "7985596f-7f4d-48cd-d636-aeaaa51f84cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'model_*': No such file or directory\n",
            "config data/w2v_pretrained_embeddings_vocab.json loaded\n",
            "config data/wikiner_corpus_tagset.json loaded\n",
            "tag dict file => model_wikiner_vanilla/wikiner_corpus_tagset.json\n",
            "tag dict file => model_wikiner_vanilla/w2v_pretrained_embeddings_vocab.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "parsing data/wikiner_corpus.txt: 0it [00:00, ?it/s]"
          ]
        }
      ],
      "source": [
        "args = dict()\n",
        "args['corpus_dir'] = \"data\"  # the corpus directory\n",
        "args['model_dir'] = \"model_wikiner_vanilla\"       # the output directory for model files\n",
        "args['num_epoch'] = 5 # 5                # number of epoch to train\n",
        "args['lr'] = 1e-3                     #¬†learning rate\n",
        "args['weight_decay'] = 0.             # the L2 normalization parameter\n",
        "args['batch_size'] = 1000             # batch size for training\n",
        "args['device'] = None                 # the training device: \"cuda:0\", \"cpu:0\". It will be auto-detected by default\n",
        "args['max_seq_len'] = 100 #¬†100              #¬†max sequence length within training\n",
        "args['val_split'] = 0.2                  #¬†the split for the validation dataset\n",
        "args['test_split'] = 0.2                 # the split for the testing dataset\n",
        "args['recovery'] = \"store_true\"       #¬†continue to train from the saved model in model_dir\n",
        "args['save_best_val_model'] = \"store_true\" # save the model whose validation score is smallest\n",
        "args['embedding_dim'] = 200 # 100           #¬†the dimension of the embedding layer\n",
        "args['hidden_dim'] = 128              # the dimension of the RNN hidden state\n",
        "args['num_rnn_layers'] = 1 # 1            # the number of RNN layers\n",
        "args['rnn_type'] = \"lstm\"              # RNN type, choice: \"lstm\", \"gru\"\n",
        "#¬†print(args)\n",
        "\n",
        "#\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "!rm -r model_*\n",
        "train(args)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "# --- 162.1955807209015 seconds --- gpu 5 epochs max_seq_len 100 embedding_dim 100 num_rnn_layers 1 val_loss:  4.47 test_loss: 4.27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w08NdCkzR8mM"
      },
      "source": [
        "## Pr√©diction effective du mod√®le"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CLRzOY3QZOR"
      },
      "source": [
        "### Pr√©paration des donn√©es de tests WiNER\n",
        "\n",
        "D'abord la d√©finition de quelques m√©thodes utiles pour la pr√©paration des donn√©es de tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmrbXjoIQrh-"
      },
      "outputs": [],
      "source": [
        "# utilities \n",
        "def flatten(t):\n",
        "  # applatie une liste de listes en une unique liste... \n",
        "  #¬†[[a, b], [c], [d, e, f]] -> [a, b, c, d, e, f]\n",
        "  return [item for sublist in t for item in sublist]\n",
        "\n",
        "import re \n",
        "def normalise_labels(sentences):\n",
        "  # normalise les sorties des √©tiquettes NER utilis√©es par les diff√©rents \n",
        "  # syst√®mes afin de les rendre comparable\n",
        "  new_sentences = list()\n",
        "  for sentence in sentences:\n",
        "    new_sentence = list()\n",
        "    for label in sentence:\n",
        "      if label != 'O':\n",
        "        label = re.sub('^[A-Z]-','', label)\n",
        "      new_sentence.append(label)\n",
        "    new_sentences.append(new_sentence)\n",
        "  return new_sentences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmZ4UIFORdVZ"
      },
      "source": [
        "Pr√©paration des donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8aIpI_0sIqW"
      },
      "outputs": [],
      "source": [
        "# load the test corpus\n",
        "!mkdir -p data\n",
        "!wget -nc https://github.com/nicolashernandez/teaching_nlp/raw/main/data/winer_dev.joblib -P data\n",
        "from joblib import load\n",
        "winer_corpus = load('data/winer_dev.joblib')\n",
        "\n",
        "# get the tokens of each text\n",
        "# liste chaque forme de surface de chaque mot de chaque phrase\n",
        "winer_tokens = [[token for token, pos, label in text] for text in winer_corpus]\n",
        "# liste chaque √©tiquette (label) de chaque mot de chaque phrase\n",
        "winer_ref = [[label for token, pos, label in text] for text in winer_corpus]\n",
        "labels = list(set(flatten(winer_ref)))\n",
        "\n",
        "#\n",
        "print ('#texts:', len(winer_corpus))\n",
        "print ('labels:', labels)\n",
        "\n",
        "print ('sample of annotated texts:', winer_corpus[0])   \n",
        "print ('sample of tokenized text:', winer_tokens[0])   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONcPIZxFQ8HZ"
      },
      "source": [
        "###¬†D√©finition de la m√©thode d'√©valuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq_F2cQAQ8m8"
      },
      "outputs": [],
      "source": [
        "# Measures definition\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def results_per_class(labels, y_ref, y_hyp):\n",
        "  # Inspect per-class results in more detail:\n",
        "  sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        "  )\n",
        "  # print ('y_ref', len(y_ref), 'y_hyp', len(y_hyp), 'sorted_labels', len(sorted_labels))\n",
        "  return classification_report(flatten(y_ref), flatten(y_hyp), labels=sorted_labels, digits=3)\n",
        "\n",
        "# Il y a beaucoup plus d'entit√©s 'O' que les autres dans le corpus, \n",
        "#¬†mais nous sommes davantage int√©ress√©s par les autres entit√©s. \n",
        "#¬†Pour ne pas biaiser les scores de moyenne, on retire les √©tiquettes qui ne nous int√©ressent pas.\n",
        "print (\"before removing:\", labels)\n",
        "labels_to_remove = ['O', 'Event', 'Date', 'Hour']\n",
        "for l in labels_to_remove:\n",
        "  if l in labels: labels.remove(l)\n",
        "print (\"after removing:\", labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLwtv_AAUuDZ"
      },
      "source": [
        "Pr√©diction sur une phrase exemple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDbE4MwVvtTr"
      },
      "outputs": [],
      "source": [
        "model = WordsTagger(model_dir=\"model_wikiner_vanilla\")\n",
        "tags, sequences = model([['George', 'W.', 'Bush', 'fut', 'pr√©sident', 'des', '√âtats-Unis', \"d'\", 'Am√©rique', '.']])  # CHAR-based model\n",
        "print(tags)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW3LdyPBUymu"
      },
      "source": [
        "###¬†Ex√©cution de la pr√©diction sur les donn√©es de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3aL5ctwsXTc"
      },
      "outputs": [],
      "source": [
        "# predict\n",
        "#from bi_lstm_crf.app import WordsTagger\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "bilstmcrf_model = WordsTagger(model_dir=\"model_wikiner_vanilla\") #_vanilla\n",
        "\n",
        "bilstmcrf_hyp = []\n",
        "# pour chaque phrase de wikiner\n",
        "for text in winer_tokens:\n",
        "    tags, sequences = bilstmcrf_model([text])    \n",
        "    bilstmcrf_hyp.append(tags[0])\n",
        "    #print (tags)\n",
        "    #break\n",
        "\n",
        "#\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "# --- 40.24239158630371 seconds ---\n",
        "# --- 144.3440752029419 seconds ---\n",
        "# --- 33.92570495605469 seconds ---\n",
        "\n",
        "# normalize the hyp labels\n",
        "print ('bilstmcrf_hyp', bilstmcrf_hyp[0])\n",
        "normalized_bilstmcrf_hyp = normalise_labels(bilstmcrf_hyp)\n",
        "print ('normalized_bilstmcrf_hyp', normalized_bilstmcrf_hyp[0])\n",
        "print ('winer_ref', winer_ref[0])\n",
        "print()\n",
        "\n",
        "# Evaluate on data \n",
        "print (args)\n",
        "print (results_per_class(labels, winer_ref, normalized_bilstmcrf_hyp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWbW17grVzQg"
      },
      "source": [
        "avec les valeurs par d√©faut\n",
        "\n",
        "```\n",
        "running_device gpu\n",
        "--- 49.18917155265808 seconds ---\n",
        "['O', 'O', 'O', 'O', 'I-PER', 'I-MISC', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'I-LOC', 'O', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'I-LOC', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'I-LOC', 'I-LOC', 'I-LOC', 'O', 'I-LOC', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O']\n",
        "['O', 'O', 'O', 'O', 'PER', 'MISC', 'O', 'O', 'O', 'O', 'MISC', 'MISC', 'O', 'O', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'O', 'O', 'LOC', 'LOC', 'LOC', 'LOC', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORG', 'ORG', 'ORG', 'O', 'ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'MISC', 'MISC', 'MISC', 'O', 'MISC', 'MISC', 'MISC', 'O', 'O', 'O', 'O', 'PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'LOC', 'O', 'O', 'MISC', 'MISC', 'MISC', 'O', 'O', 'LOC', 'O', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORG', 'O', 'O', 'LOC', 'O', 'O', 'O', 'LOC', 'LOC', 'LOC', 'O', 'LOC', 'O', 'O', 'O', 'ORG', 'ORG', 'ORG', 'ORG', 'O', 'O', 'O']\n",
        "['O', 'O', 'O', 'O', 'PER', 'PER', 'Date', 'Date', 'Date', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORG', 'ORG', 'ORG', 'O', 'ORG', 'O', 'O', 'O', 'O', 'Date', 'O', 'O', 'O', 'O', 'O', 'Date', 'O', 'O', 'O', 'ORG', 'ORG', 'ORG', 'O', 'ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'O', 'PER', 'PER', 'O', 'O', 'ORG', 'O', 'O', 'PER', 'PER', 'PER', 'O', 'O', 'ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Date', 'O', 'O', 'O', 'ORG', 'O', 'O', 'O', 'ORG', 'ORG', 'ORG', 'O', 'O', 'O', 'O', 'O', 'ORG', 'O', 'O', 'O', 'O', 'Date', 'O']\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         PER      0.287     0.659     0.400      4483\n",
        "        MISC      0.012     0.625     0.024       443\n",
        "         LOC      0.341     0.502     0.406      4724\n",
        "         ORG      0.331     0.232     0.273      3816\n",
        "\n",
        "   micro avg      0.154     0.482     0.233     13466\n",
        "   macro avg      0.243     0.504     0.276     13466\n",
        "weighted avg      0.309     0.482     0.354     13466\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rEMG4w79NWt"
      },
      "source": [
        "---\n",
        "# VOTRE TRAVAIL\n",
        "\n",
        "* Pourquoi deux entra√Ænements avec les m√™mes hyper-param√®tres donnent des pr√©dictions donnant des performances diff√©rentes ? \n",
        "* Jouez avec le param√©trage d'entra√Ænement du mod√®le (par exemple en doublant les valeurs par d√©faut): nombre d'√©poque, taille des phrases consid√©r√©es (max_seq_len), dimension des embeddings (embedding_dim), nombre de couches RNN (num_rnn_layers), type de cellule RNN (rnn_type), nombre de dimension du RNN (hidden state), le taux d'apprentissage (lr)... D√©terminer l'apport de chaque param√®tre. Discuter les performances en termes de pr√©cision, rappel et micro/macro-F1.\n",
        "* Dans vos exp√©riences, rencontrez-vous des limites avec le hardware mis √† disposition par gcolab ? \n",
        "* Exp√©rimenter une modification en profondeur (au choix, d'autres sont possibles)\n",
        "  * Modifiez le code pour utiliser des _pre-trained word embeddings_. Mesurez leur apport. Exp√©rimentez √† minima le mod√®le [word2vec de 200 dimensions construit avec skipgram sur le corpus FrWac, et mis √† disposition par Jean-Philippe Fauconnier](https://fauconnier.github.io/#data)  \n",
        "  * Ajouter les traits sur la surface des mots\n",
        "* Faire un retour sur les diff√©rents mod√®les que vous avez impl√©ment√©s (y compris √† base de CRF pur).\n",
        "* Suivant votre avancement, d'autres word embeddings peuvent √™tre test√©s (e.g. glove), une architecture [BERT-CRF](https://github.com/jidasheng/bi-lstm-crf) (cf. fin du README)...\n",
        "\n",
        "Ci dessous quelques pointeurs sur comment utiliser des mod√®les pr√©-entra√Æn√©s avec pytorch\n",
        "* https://stackoverflow.com/questions/49710537/pytorch-gensim-how-to-load-pre-trained-word-embeddings\n",
        "* https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76 \n",
        "* https://towardsdatascience.com/deep-learning-for-nlp-with-pytorch-and-torchtext-4f92d69052f\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Utiliser des embeddings pr√©-entrain√©s\n",
        "Afin de mettre sur la voie...\n",
        "\n",
        "Jean-Philippe Fauconnier met √† dispositions des [mod√®les d'embeddings pr√©-entra√Æn√©s pour le fran√ßais](https://fauconnier.github.io/#data).\n"
      ],
      "metadata": {
        "id": "SLGmWeG5ooO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://s3.us-east-2.amazonaws.com/embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin -P embeddings\n",
        "#!wget -nc https://s3.us-east-2.amazonaws.com/embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_500_skip_cut100.bin -P embeddings"
      ],
      "metadata": {
        "id": "MhbccXJpo8vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialiser une couche d'Embedding √† l'aide d'un mod√®le pr√©-entra√Æn√© (cf. [Charger des embeddings pr√©-entra√Æn√©s dans pytorch](https://stackoverflow.com/questions/49710537/pytorch-gensim-how-to-load-pre-trained-word-embeddings))"
      ],
      "metadata": {
        "id": "Hwc1FFUrqeJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#¬†Chargement du mod√®le d'embeddings pr√©-entra√Æn√©s\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "w2v_pretrained_embeddings_path = \"embeddings/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin\"\n",
        "#w2v_pretrained_embeddings_path = \"embeddings/frWac_non_lem_no_postag_no_phrase_500_skip_cut100.bin\"\n",
        "\n",
        "w2v_pretrained_embeddings = KeyedVectors.load_word2vec_format(w2v_pretrained_embeddings_path, binary=True, unicode_errors=\"ignore\")\n",
        "#print (w2v_pretrained_embeddings.vectors[0])\n",
        "\n",
        "import torch\n",
        "weights = torch.FloatTensor(w2v_pretrained_embeddings.vectors) \n",
        "\n",
        "import torch.nn as nn\n",
        "embedding = nn.Embedding.from_pretrained(weights)"
      ],
      "metadata": {
        "id": "WCio316Sps9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acc√©der au vocabulaire"
      ],
      "metadata": {
        "id": "k34xlLUwptIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print ('#vocab',len(w2v_pretrained_embeddings.vectors))\n",
        "w2v_pretrained_embeddings_vocab = list(w2v_pretrained_embeddings.vocab)\n",
        "#print (weights[0])\n",
        "# export vocab au format bi_lstm_crf \n",
        "!mkdir data\n",
        "import json\n",
        "with open('data/w2v_pretrained_embeddings_vocab.json', 'w', encoding='utf-8') as f:\n",
        "  json.dump(w2v_pretrained_embeddings_vocab, f, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "uS12U-Kjpuat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "2DeQaqt-pSeu"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "M2-ATAL-2021-22_02_NER with BiLSTM-CRF.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMJHkOA596ZJrSdEVLY3v/x",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}