{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "M2-ATAL-2021-22_02_Machine Learning by feature engineering - Use case : NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolashernandez/teaching_nlp/blob/main/M2-ATAL-2021-22_02_Machine_Learning_by_feature_engineering_Use_case_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pUMMqbLw3g0"
      },
      "source": [
        "# Objectifs\n",
        "\n",
        "* Vérifier les comparatifs de performance des systèmes états de l'art en reconnaissance d'entités nommées en les testant sur de nouvelles données  \n",
        "* En utilisant le même corpus d'entraînement que les systèmes état de l'art, tenter de faire mieux avec une méthode d'apprentissage supervisée à base de traits.\n",
        "\n",
        "Les systèmes état de l'art considérés sont spaCy et Stanza."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAZNCT_Y4RJl"
      },
      "source": [
        "# Installation\n",
        "\n",
        "Ne pas oublier de redémarrer l'environnement d'exécution après exécution du code ci-dessous."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3mNTgir0dsf"
      },
      "source": [
        "# Download the \"small\" french spacy nlp model\n",
        "# Ne pas oublier de redémarrer l'environnement d'exécution\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx8jR44x6swV"
      },
      "source": [
        "# installation de stanza\n",
        "!pip install stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agGfNkhgS-JU"
      },
      "source": [
        "# retrieve a list of french stop words \n",
        "# \"ça peut toujours servir...\"\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "fr_stop_words = nltk.corpus.stopwords.words('french')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq8ZNkaFYvPt"
      },
      "source": [
        "# utilities \n",
        "\n",
        "def flatten(t):\n",
        "  # applatie une liste de listes en une unique liste... \n",
        "  return [item for sublist in t for item in sublist]\n",
        "\n",
        "import re \n",
        "def normalise_labels(sentences):\n",
        "  # normalise les sorties des étiquettes NER utilisées par les différents \n",
        "  # systèmes afin de les rendre comparable\n",
        "  new_sentences = list()\n",
        "  for sentence in sentences:\n",
        "    new_sentence = list()\n",
        "    for label in sentence:\n",
        "      if label != 'O':\n",
        "        label = re.sub('^[A-Z]-','', label)\n",
        "      new_sentence.append(label)\n",
        "    new_sentences.append(new_sentence)\n",
        "  return new_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKCTdrbWLpCF"
      },
      "source": [
        "# Protocole d'évaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s6lRJNstx0d"
      },
      "source": [
        "## WiNER, données de développement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxSGvi2GuuDm"
      },
      "source": [
        "[WiNER](https://github.com/YoannDupont/WiNER-fr) is a free corpus for Named Entity Recognition based on Wikinews texts.\n",
        "\n",
        "The current named entity tagset is:\n",
        "* Date (absolute dates.)\n",
        "* Event (conferences, sports events, annual events, celebration days, named climatic events, etc.)\n",
        "*  Hour (absolute hours. If present, they include time zones, UTC, GMT, etc.)\n",
        "* Location (countries, towns, regions, addresses, astophysicals objects, hydrophysical objects, etc.)\n",
        "* Organization (non profit organizations, companies, medias, etc.)\n",
        "* Person (human individuals, without their title or function. They can be actual people or fictional characters.)\n",
        "* Product (physical objects, brands, softwares.)\n",
        "\n",
        "The tagset was defined using various campaigns or resources: MUC-6, CoNLL-2003, Named Entity annotated French Treebank and Quaero. Every type defined in this tagset is directly taken from one of those.\n",
        "\n",
        "> Yoann Dupont. Un corpus libre, évolutif et versionné en entités nommées du français. TALN 2019 - Traitement Automatique des Langues Naturelles, Jul 2019, Toulouse, France. \n",
        "https://hal.archives-ouvertes.fr/hal-02448590\n",
        "\n",
        "Le système d'annotation originale est de type **_stand off annotation_**. _standoff_ désigne les formats où les annotations sont stockées séparément du contenu annoté, lequel n’est jamais modifié par l'outil annotateur. En général les annotations référence les contenus annotés via des offsets (indice de caractères de début et de fin) ou des indices de tokens. La lecture de ces formats par un humain n’est pas forcément aisée, leur manipulation (édition et visualisation) requièrent des développements spécifiques mais ils présentent l’intérêt de gérer la concurrence entre annotations, de ne pas être contraint par la nature de l’information ajoutée (span, relation, ...) et surtout de ne perdre aucune information du document original (ne serait ce que sa mis en forme). \n",
        "\n",
        "Ici un [exemple de texte extrait du corpus](https://github.com/YoannDupont/WiNER-fr/blob/master/2016/01/2016_01_01-001.txt) et son [fichier associé qui contient les annotations \"standoff\"](https://github.com/YoannDupont/WiNER-fr/blob/master/2016/01/2016_01_01-001.ann).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h68O1gwaNTk2"
      },
      "source": [
        "### QUESTION\n",
        "\n",
        "* Le corpus Winer original utilise-t-il des offsets ou des indices de tokens ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA7UyIc7NXT0"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3uLpyBSX3p"
      },
      "source": [
        "### Récupération et préparation des données\n",
        "\n",
        "Pour les besoins de ce travail, nous utiliserons un extrait du corpus, tokenisé par spaCy, avec les annotations projetées sur les tokens. \n",
        "\n",
        "Ce sont ces classes LOC, ORG, PER et MISC qui nous intéresseront.\n",
        "\n",
        "La classe _Location_ a été renommée en _LOC_, _Organization_ en _ORG_, _Person_ en _PER_. Stanza et spaCy reconnaissent aussi des entités de type MISC. Sans vraiment chercher ce qu'elle désigne, nous avons pris le partie de renommer _Product_ en _MISC_. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmkLTqsMurqN"
      },
      "source": [
        "!mkdir -p data\n",
        "!wget -nc https://github.com/nicolashernandez/teaching_nlp/raw/main/data/winer_dev.joblib -P data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVOnQ4r7t-Ht"
      },
      "source": [
        "# load the test corpus\n",
        "from joblib import load\n",
        "winer_corpus = load('data/winer_dev.joblib')\n",
        "\n",
        "# get the tokens of each text\n",
        "winer_ref = [[label for token, pos, label in text] for text in winer_corpus]\n",
        "labels = list(set(flatten(winer_ref)))\n",
        "winer_tokens = [[token for token, pos, label in text] for text in winer_corpus]\n",
        "\n",
        "#\n",
        "print ('#texts:', len(winer_corpus))\n",
        "print ('labels:', labels)\n",
        "\n",
        "print ('sample of annotated texts:', winer_corpus[0])   \n",
        "print ('sample of tokenized text:', winer_tokens[0])   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmsI84MBw3g2"
      },
      "source": [
        "## Métriques d'évaluation\n",
        "\n",
        "La reconnaissance des entités nommées est un problème de classification multi-classes. On peut donc utiliser toutes les mesures de classification. En général, on privilégie la **micro-moyenne de scores F1** (_micro-averaged F1_). \n",
        "\n",
        "La micro-moyenne d'une métrique aggrège d'abord les contributions de toutes les classes avant de calculer la métrique tandis que la macro-moyenne calcule la métrique indépendamment pour chaque classe puis en fait la moyenne. \n",
        "\n",
        "La macro-moyenne traite toutes les classes de manière égale. \n",
        "Dans le cas où le système n'est bon que pour reconnaître une classe (la classe dominante), la micro-moyenne peut être élevée car les contributions des autres classes seront insignifiantes. Dans ce cas, la macro-moyenne sera basse.\n",
        "\n",
        "Dans le cas où le système n'est mauvais que pour la reconnaissance d'une seule classe, la macro-moyenne gommera son effet. La micro-moyenne peut être élevée même si le système donne de mauvais résultat sur des classes rares, car il donne plus de poids aux classes communes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7ZBrBrjw3g2"
      },
      "source": [
        "# Measures definition\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def results_per_class(labels, y_ref, y_hyp):\n",
        "  # Inspect per-class results in more detail:\n",
        "  sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        "  )\n",
        "  # print ('y_ref', len(y_ref), 'y_hyp', len(y_hyp), 'sorted_labels', len(sorted_labels))\n",
        "  return classification_report(flatten(y_ref), flatten(y_hyp), labels=sorted_labels, digits=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5594hie-zr3"
      },
      "source": [
        "# Il y a beaucoup plus d'entités 'O' que les autres dans le corpus, \n",
        "# mais nous sommes davantage intéressés par les autres entités. \n",
        "# Pour ne pas biaiser les scores de moyenne, on retire les étiquettes qui ne nous intéressent pas.\n",
        "print (\"before removing:\", labels)\n",
        "labels_to_remove = ['O', 'Event', 'Date', 'Hour']\n",
        "for l in labels_to_remove:\n",
        "  if l in labels:\n",
        "    labels.remove(l)\n",
        "print (\"after removing:\", labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jciW0FC0XY3"
      },
      "source": [
        "# Prédiction et évaluation de spaCy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW6stvcd92xZ"
      },
      "source": [
        "## Construction de la chaîne de traitements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDoSAIMI0cM-"
      },
      "source": [
        "# Create my whitespace tokenizer\n",
        "#\n",
        "# one trick to make spaCy consider a tokenization performed by a third-party component: \n",
        "# use whitespace as separator for marking the third-party component tokenization \n",
        "# and force spacy to use a whitespace tokenizer...\n",
        "\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "class WhitespaceTokenizer(object):\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __call__(self, text):\n",
        "        words = text.split(' ')\n",
        "        # All tokens 'own' a subsequent space character in this tokenizer\n",
        "        spaces = [True] * len(words)\n",
        "        return Doc(self.vocab, words=words, spaces=spaces)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkZjR_jx0zfj"
      },
      "source": [
        "# Build a spacy nlp pipeline with a ner model and the whitespace tokenizer\n",
        "\n",
        "import spacy\n",
        "\n",
        "# load a nlp model\n",
        "spacy_nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "# declare the whitespace tokenizer\n",
        "spacy_nlp.tokenizer = WhitespaceTokenizer(spacy_nlp.vocab)\n",
        "\n",
        "#\n",
        "#print ('nlp pipeline='+str([name for (name, proc) in spacy_nlp.pipeline]))\n",
        "# pipeline['tagger', 'parser', 'ner']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccywkwCC-Avd"
      },
      "source": [
        "## Prédiction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQlf8y1V06Hi"
      },
      "source": [
        "# Run spacy nlp pipeline made of ner component and evaluate it\n",
        "\n",
        "#\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# \n",
        "spacy_hyp = []\n",
        "for text in winer_tokens:\n",
        "    doc = spacy_nlp(' '.join(text), disable=[\"tagger\", \"parser\"])\n",
        "    #print (doc)\n",
        "    spacy_hyp.append([(token.ent_iob_+'-'+token.ent_type_) for token in doc])\n",
        "    #break\n",
        "\n",
        "#\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "# normalize the hyp labels\n",
        "normalized_spacy_hyp = normalise_labels(spacy_hyp)\n",
        "#print (winer_tokens[0])\n",
        "#print (winer_ref[0])\n",
        "#print (spacy_hyp[0])\n",
        "#print (normalized_spacy_hyp[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAbKPlCh-Ulm"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE8b56iSw3g3"
      },
      "source": [
        "# Evaluate on data \n",
        "print (results_per_class(labels, winer_ref, normalized_spacy_hyp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIMf-oRE-sEf"
      },
      "source": [
        "### QUESTION\n",
        "\n",
        "La colonne _support_ indique le nombre d'instances par classe. \n",
        "\n",
        "* Le f1-score de la classe _MISC_ est très mauvais. Pour rappel, la classe MISC à reconnaître dans le corpus WiNER correspond à sa classe Product que nous avons renommé. Pour rappel encore, spaCy a été entraîné avec le corpus Wikiner. En vous rapportant à la définition de la classe Product du [corpusWiNER](https://github.com/YoannDupont/WiNER-fr) et en la comparant avec la définition de la classe MISC du [corpus Wikiner](https://www.sciencedirect.com/science/article/pii/S0004370212000276?via%3Dihub\n",
        "), émettez une hypothèse qui puisse expliquer la mauvaise qualité des résultats.  \n",
        "* Retirez la classe MISC du décompte ou bien vérifiez votre hypothèse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snAo20GS-vZM"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GylZoaI6kql"
      },
      "source": [
        "# Prédiction et évaluation de Stanza\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAmH3zkQ98eW"
      },
      "source": [
        "## Construction de la chaîne de traitements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge9GGh1P6lTu"
      },
      "source": [
        "# Build a stanza nlp pipeline with a ner model and the whitespace tokenizer\n",
        "# --- 1848.740861415863 seconds ---\n",
        "\n",
        "import stanza\n",
        "\n",
        "# download French model\n",
        "stanza.download('fr') \n",
        "\n",
        "# https://stanfordnlp.github.io/stanza/tokenize.html#start-with-pretokenized-text \n",
        "# pretokenized (and sentence split) text correspond to use newline (\\n) to separated sentences and each sentence is space separated tokens\n",
        "stanza_nlp = stanza.Pipeline('fr', processors='tokenize,ner', tokenize_pretokenized=True) # initialize French neural pipeline\n",
        "\n",
        "# run on sample \n",
        "#doc = stanza_nlp(\"Barack Obama est né à Hawaii .\") # run annotation over a sentence\n",
        "#print(doc)\n",
        "#print(doc.entities)\n",
        "#print(*[f'token: {token.text}\\tner: {token.ner}' for sent in doc.sentences for token in sent.tokens], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpzcULpG-Ms_"
      },
      "source": [
        "## Prédiction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxTBxlqB693N"
      },
      "source": [
        "# Running stanza nlp pipeline made of ner component and evaluate it\n",
        "\n",
        "#\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# \n",
        "stanza_hyp = []\n",
        "for text in winer_tokens:\n",
        "    doc = stanza_nlp(' '.join(text))\n",
        "    stanza_hyp.append([token.ner for sent in doc.sentences for token in sent.tokens])\n",
        "\n",
        "#\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "# normalize the hyp labels\n",
        "normalized_stanza_hyp = normalise_labels(stanza_hyp)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY_IqFey8RoC"
      },
      "source": [
        "Comme stanza prend un certain temps d'exécution... vous pouvez directement charger la prédiction précédemment sauvée pour vous..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18QLibn38EVj"
      },
      "source": [
        "# dump\n",
        "#from joblib import dump\n",
        "#dump(normalized_stanza_hyp, 'data/winer_stanza_hyp.joblib')\n",
        "\n",
        "#load\n",
        "!mkdir -p data\n",
        "!wget -nc https://github.com/nicolashernandez/teaching_nlp/raw/main/data/winer_stanza_hyp.joblib -P data\n",
        "from joblib import load\n",
        "normalized_stanza_hyp = load('data/winer_stanza_hyp.joblib')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsSvpgly-Qjl"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSFOS1zO-bI1"
      },
      "source": [
        "# Evaluate on data \n",
        "print (results_per_class(labels, winer_ref, normalized_stanza_hyp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uRvRT2zJosj"
      },
      "source": [
        "### QUESTION\n",
        "\n",
        "* En termes de rapidité de traitement, quel est le meilleur des deux systèmes spaCy ou Stanza ? \n",
        "* En termes de qualité de traitement,  quel est le meilleur des deux systèmes spaCy ou Stanza ?\n",
        "* Est-ce que ces observations correspondent à ce qui est relevé sur le [comparatif de performance du site de spaCy](https://spacy.io/usage/facts-figures) ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIhEI6hUJzPY"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9xUDe1jAF5q"
      },
      "source": [
        "# Construction d'un système NER selon une approche supervisée (CRF) à base de traits \n",
        "\n",
        "Les cellules ci-dessous implémentent une solution de bout en bout pour\n",
        "- entraîner un modèle de prédiction à partir des données d'entraînement wikiner; \n",
        "- appliquer le modèle construit pour la prédiction d'entités nommées sur le corpus de dev winer;\n",
        "- évaluer les résultats.\n",
        "\n",
        "Avant de la mettre en oeuvre, lisez la suite.\n",
        "\n",
        "Dans une méthode d'apprentissage supervisée à base de traits, c'est au \"scientifique\" ou \"ingénieur de la donnée\" de définir et d'implémenter les traits à observer dans la donnée et à donner en entrée du système d'apprentissage.\n",
        "\n",
        "Les premières cellules de code ci-dessous proposent une implémentation d'une méthode d'extraction de traits du mot courant à savoir la méthode `word2features`.\n",
        "Cette méthode doit être appelées aussi bien sur les données d'entraînement que sur les données où l'on cherche à appliquer un modèle construit pour obtenir une prédiction.\n",
        "\n",
        "Accessoirement l'algorithme d'apprentissage est un *CRF (Conditional Random Fields)* de l'implémentation `sklearn_crfsuite.CRF`.\n",
        "L'implémentation du système ci-dessous suit la démarche décrite dans le turtoriel de l'implémentation [sklearn-crfsuite](https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html).\n",
        "L'optimisation des paramètres est possible (cf. le tutoriel).\n",
        "\n",
        "Poursuivez votre lecture mais sachez que le système fonctionne en l'état."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNyQNZ48w3go"
      },
      "source": [
        "## Wikiner, données d'entraînement\n",
        "\n",
        "Pour entraîner notre système, nous allons utiliser les mêmes données d'entraînement de que les composants NER de spaCy ou Stanza à savoir les données du corpus Wikiner.\n",
        "\n",
        "```\n",
        "@Article{nothman2012:artint:wikiner,\n",
        "  author = {Joel Nothman and Nicky Ringland and Will Radford and Tara Murphy and James R. Curran},\n",
        "  title = {Learning multilingual named entity recognition from {Wikipedia}},\n",
        "  journal = {Artificial Intelligence},\n",
        "  publisher = {Elsevier},\n",
        "  volume = {194},\n",
        "  pages = {151--175},\n",
        "  year = {2012},\n",
        "  doi = {10.1016/j.artint.2012.03.006},\n",
        "  url = {http://dx.doi.org/10.1016/j.artint.2012.03.006}\n",
        "}\n",
        "```\n",
        "https://www.sciencedirect.com/science/article/pii/S0004370212000276?via%3Dihub\n",
        "\n",
        "Les données utilisées proviennent de l'annotation \"silver standard\" du [corpus Wikiner généré à partir de la Wikipedia et téléchargeable ici](https://github.com/dice-group/FOX/tree/master/input/Wikiner). \n",
        "\n",
        "Le pos tagging a été modifié pour adopter le système d'étiquettes de l'[universal dependencies (UD)](https://universaldependencies.org/u/pos/) (grâce à spaCy).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv9TkbDxw3gp"
      },
      "source": [
        "# Getting the data\n",
        "!mkdir -p data \n",
        "!wget -nc https://github.com/nicolashernandez/teaching_nlp/raw/main/data/wikiner_ud.joblib.bz2 -P data\n",
        "!bzip2 -dk data/wikiner_ud.joblib.bz2\n",
        "\n",
        "# Loading the corpus \n",
        "from joblib import load\n",
        "wikiner_corpus = load('data/wikiner_ud.joblib') \n",
        "\n",
        "\n",
        "# Aperçu du nombre de phrases et d'une phrase annotée (liste de tokens composés de la forme, de la catégorie grammaticale et de l'étiquette BIO correspondant en l'entité nommée.\n",
        "print (len(wikiner_corpus))\n",
        "print (wikiner_corpus[0])  \n",
        "# [('Il', 'PRO:PER', 'O'), ('assure', 'VER:pres', 'O'), ('à', 'VER:pper', 'O'), ('la', 'DET:ART', 'O'), ('suite', 'NOM', 'O'), ('de', 'PRP', 'I-PER'), ('Saussure', 'NAM', 'I-PER'), ('le', 'DET:ART', 'O'), ('cours', 'NOM', 'O'), ('de', 'PRP', 'O'), ('grammaire', 'NOM', 'O'), ('comparée', 'ADJ', 'O'), (',', 'PUN', 'O'), (\"qu'\", 'PRO:REL', 'O'), ('il', 'PRO:PER', 'O'), ('complète', 'VER:subp', 'O'), ('à', 'VER:pper', 'O'), ('partir', 'VER:infi', 'O'), ('de', 'PRP', 'O'), ('1894', 'NUM', 'O'), ('par', 'PRP', 'O'), ('une', 'DET:ART', 'O'), ('conférence', 'NOM', 'O'), ('sur', 'PRP', 'O'), (\"l'\", 'DET:ART', 'O'), ('iranien', 'ADJ', 'O'), ('.', 'SENT', 'O')]\n",
        "# [('Il', 'PRON', 'O'), ('assure', 'VERB', 'O'), ('à', 'ADP', 'O'), ('la', 'DET', 'O'), ('suite', 'NOUN', 'O'), ('de', 'ADP', 'I-PER'), ('Saussure', 'NOUN', 'I-PER'), ('le', 'DET', 'O'), ('cours', 'NOUN', 'O'), ('de', 'ADP', 'O'), ('grammaire', 'ADJ', 'O'), ('comparée', 'VERB', 'O'), (',', 'PUNCT', 'O'), (\"qu'\", 'SCONJ', 'O'), ('il', 'PRON', 'O'), ('complète', 'VERB', 'O'), ('à', 'ADP', 'O'), ('partir', 'VERB', 'O'), ('de', 'ADP', 'O'), ('1894', 'NUM', 'O'), ('par', 'ADP', 'O'), ('une', 'DET', 'O'), ('conférence', 'NOUN', 'O'), ('sur', 'ADP', 'O'), (\"l'\", 'DET', 'O'), ('iranien', 'NOUN', 'O'), ('.', 'PUNCT', 'O')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7HiiQNu4Vcd"
      },
      "source": [
        "Dans la cellule ci-dessous vous pouvez spécifier la quantité de données du corpus que vous souhaitez utiliser pour votre entraînement. En l'état, il est spécifié 10% pour faire des tests fonctionnels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM9jylsK4Sqf"
      },
      "source": [
        "# shuffle the data\n",
        "#from random import shuffle\n",
        "#shuffle(wikiner_corpus)\n",
        "\n",
        "print ('len(wikiner_corpus):', len(wikiner_corpus))\n",
        "\n",
        "\n",
        "# compute corpus partition sizes \n",
        "_100_percent = int(len(wikiner_corpus)/100*100)\n",
        "\n",
        "_80_percent = int(len(wikiner_corpus)/100*80)\n",
        "_50_percent = int(len(wikiner_corpus)/100*50)\n",
        "_10_percent = int(len(wikiner_corpus)/100*10)\n",
        "\n",
        "# ICI vous pouvez choisir la quantité de données \n",
        "# 10 % vous permet de faire des tests fonctionnels\n",
        "train_sentences = wikiner_corpus[:_10_percent]\n",
        "print ('len(train_sentences):', len(train_sentences))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hd6EG-4AHl9"
      },
      "source": [
        "## Extraction des caractéristiques\n",
        "\n",
        "Ci-dessous vous trouverez la méthode `word2features` qui se charge de transformer les mots en traits que prendra l'algorithme d'apprentissage en entrée.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp9iPdgvVaP6"
      },
      "source": [
        "### QUESTION\n",
        "\n",
        "* Exécutez les différentes cellules jusqu'à l'évaluation. Optez aussi pour un entraînement avec 100 % des données d'entrainement. Les performances produites par les traits disponibles constitueront votre \"baseline\". En attendant que les traitements se terminent (quelques minutes à 100%), répondez aux questions suivantes.\n",
        "* Dans la méthode `word2features`, que signifient `word`, `postag`, `wordminus1`, `postagminus1`, `wordplus1`, `postagplus1` ?\n",
        "* Dans la méthode `word2features`, que signifient les traits `word.lower()`,    `word[-3:]`, `word.isupper()`, `word.istitle()`, `word.isdigit()`, `postag`, `postag[:2]` ?\n",
        "* Reprenez la méthode, ajoutez de nouveaux traits, réécrivez les définitions existantes, votre objectif obtenir le meilleur micro-average f1 score sur winer avant le temps imparti et délivrer votre modèle dans le dépôt indiqué par l'enseignant. Expliquer verbeusement les traits que vous implémentez ! La mise au point de votre jeu de traits doit être éprouver dans l'env gcolab avec les ressources limitées (RAM à 12 Go).\n",
        "* Comparez vos performances avec votre baseline et avec les systèmes état de l'art, que pouvez-vous conclure ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlPPa0Uw1POU"
      },
      "source": [
        "### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFjwE-hYw3gx"
      },
      "source": [
        "# Features definition\n",
        "#\n",
        "# https://eli5.readthedocs.io/en/latest/tutorials/sklearn_crfsuite.html\n",
        "#\n",
        "# Feature extraction\n",
        "# POS tags can be seen as pre-extracted features. \n",
        "# Let’s extract more features (word parts, simplified POS tags, lower/title/upper flags, features of nearby words) \n",
        "# and convert them to sklear-crfsuite format - each sentence should be converted to a list of dicts. \n",
        "# This is a very simple baseline; you certainly can do better.\n",
        "# https://docs.python.org/3/library/stdtypes.html\n",
        "\n",
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "    }\n",
        "    if i > 0:\n",
        "        wordminus1 = sent[i-1][0]\n",
        "        postagminus1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': wordminus1.lower(),\n",
        "            '-1:word.istitle()': wordminus1.istitle(),\n",
        "            '-1:word.isupper()': wordminus1.isupper(),\n",
        "            '-1:postag': postagminus1,\n",
        "            '-1:postag[:2]': postagminus1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-1:\n",
        "        wordplus1 = sent[i+1][0]\n",
        "        postagplus1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': wordplus1.lower(),\n",
        "            '+1:word.istitle()': wordplus1.istitle(),\n",
        "            '+1:word.isupper()': wordplus1.isupper(),\n",
        "            '+1:postag': postagplus1,\n",
        "            '+1:postag[:2]': postagplus1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, postag, label in sent]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO8RKtRRVDzB"
      },
      "source": [
        "# Features extraction\n",
        "\n",
        "tokens_train = [sent2tokens(s) for s in train_sentences]\n",
        "X_train = [sent2features(s) for s in train_sentences]\n",
        "y_train = normalise_labels([sent2labels(s) for s in train_sentences])\n",
        "\n",
        "print (tokens_train[0])\n",
        "print (X_train[0][0])\n",
        "print (X_train[0][1])\n",
        "print (len(y_train))\n",
        "print (y_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex3fq-DEw3g4"
      },
      "source": [
        "## Entraînement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HXREqFMx4Wm"
      },
      "source": [
        "# instanciation du modèle crf et initialisation des hyperparamètres\n",
        "!pip install sklearn_crfsuite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs3p96NnWsXk"
      },
      "source": [
        "import sklearn_crfsuite\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=20,\n",
        "    all_possible_transitions=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndkWV-0KW1P1"
      },
      "source": [
        "import time\n",
        "#\n",
        "start_time = time.time()\n",
        "\n",
        "# train\n",
        "crf.fit(X_train, y_train);\n",
        "\n",
        "#\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time)) #  110s - 230s\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMvE-ZkkW2q8"
      },
      "source": [
        "# Persistence: model dump\n",
        "!mkdir -p models\n",
        "\n",
        "from joblib import dump\n",
        "dump(crf, 'models/wikiner_sklearn-crfsuite.joblib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG1V8aDlXNJN"
      },
      "source": [
        "## Prédiction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyzzltlHw3g4"
      },
      "source": [
        "# Extract features from the test data and predict \n",
        "winer_dev = [sent2features(s) for s in winer_corpus]\n",
        "\n",
        "crf_hyp = crf.predict(winer_dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auOBpjR6XaZk"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eok1LtQriZBk"
      },
      "source": [
        "# Evaluate on data \n",
        "print (results_per_class(labels, winer_ref, crf_hyp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfNyxXecw3g6"
      },
      "source": [
        "## Vérification de ce que le classifieur a appris\n",
        "https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#let-s-check-what-classifier-learned\n",
        "\n",
        "Ce qui suit peut vous donner des idées sur le genre de traits qui peut vous être utiles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypRdwFW-w3g6"
      },
      "source": [
        "# quid des transitions d'une étiquette à l'autre ?\n",
        "from collections import Counter\n",
        "\n",
        "def print_transitions(trans_features):\n",
        "    for (label_from, label_to), weight in trans_features:\n",
        "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
        "\n",
        "print(\"Top likely transitions:\")\n",
        "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
        "\n",
        "print(\"\\nTop unlikely transitions:\")\n",
        "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mftyVhMlw3g7"
      },
      "source": [
        "# quid des caractéristiques ? \n",
        "def print_state_features(state_features):\n",
        "    for (attr, label), weight in state_features:\n",
        "        print(\"%0.6f %-8s %s\" % (weight, label, attr))    \n",
        "\n",
        "print(\"Top positive features:\")\n",
        "print_state_features(Counter(crf.state_features_).most_common(30))\n",
        "\n",
        "print(\"\\nTop negative features:\")\n",
        "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}