{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOzuKynXomyRoMIkdsQPyYp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "440afdde3f7c42fc9fb6a1ed58ec5f43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d0d791fe97f4767a719eb8b315a1e0e",
              "IPY_MODEL_6140147b08cf4e50a555ad1ac9643804",
              "IPY_MODEL_ea46dcb3d96c49af8b46bf2646fb26df"
            ],
            "layout": "IPY_MODEL_46ec4a3c73fe486998f1a646b1b2ed37"
          }
        },
        "1d0d791fe97f4767a719eb8b315a1e0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd36bd6c167341fe8611ffa117c7f4c6",
            "placeholder": "​",
            "style": "IPY_MODEL_f7c79807b39240728a7f1f8f5a67eb73",
            "value": "100%"
          }
        },
        "6140147b08cf4e50a555ad1ac9643804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_566b0d363ff14cd9b96cba43090ad707",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1646772d9e0d4aaaa16575b312fdd049",
            "value": 3
          }
        },
        "ea46dcb3d96c49af8b46bf2646fb26df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b88f8925ba1647e78719db49b39f3333",
            "placeholder": "​",
            "style": "IPY_MODEL_18e33849b075481bb41efc2b0a9890d7",
            "value": " 3/3 [00:00&lt;00:00,  8.91it/s]"
          }
        },
        "46ec4a3c73fe486998f1a646b1b2ed37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd36bd6c167341fe8611ffa117c7f4c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7c79807b39240728a7f1f8f5a67eb73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "566b0d363ff14cd9b96cba43090ad707": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1646772d9e0d4aaaa16575b312fdd049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b88f8925ba1647e78719db49b39f3333": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18e33849b075481bb41efc2b0a9890d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7095815c1344076b60a3e197f0e533a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5932c09cd28b42d19f7636c4bc01f1a6",
              "IPY_MODEL_8453ffbe939547cdbf74e6b3b61f7deb",
              "IPY_MODEL_03398e6d84994a77928b5bf251178af9"
            ],
            "layout": "IPY_MODEL_fc78363ade6b4bf48dc2cebf3adaaede"
          }
        },
        "5932c09cd28b42d19f7636c4bc01f1a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ff392dd52414845b1edae142a0db14e",
            "placeholder": "​",
            "style": "IPY_MODEL_076ffce5626e4d9c9adb972737fb34a3",
            "value": "Downloading: 100%"
          }
        },
        "8453ffbe939547cdbf74e6b3b61f7deb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15463c7e439243e7a49f2ebc62916fb7",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8a7aeea07b64025aaf0118105c0abd4",
            "value": 483
          }
        },
        "03398e6d84994a77928b5bf251178af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_161ed7e5575f41999c2aa576473364a4",
            "placeholder": "​",
            "style": "IPY_MODEL_6ea2ac6037044d6aadd86813d1b82b7b",
            "value": " 483/483 [00:00&lt;00:00, 16.6kB/s]"
          }
        },
        "fc78363ade6b4bf48dc2cebf3adaaede": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ff392dd52414845b1edae142a0db14e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "076ffce5626e4d9c9adb972737fb34a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15463c7e439243e7a49f2ebc62916fb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8a7aeea07b64025aaf0118105c0abd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "161ed7e5575f41999c2aa576473364a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ea2ac6037044d6aadd86813d1b82b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolashernandez/teaching_nlp/blob/main/05_Classification_de_textes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxdW5mtMGqmy"
      },
      "source": [
        "# Classification de textes\n",
        "\n",
        "La **classification** consiste à attribuer une classe à chaque texte (objet, instance, point) à classer. On parle de *classification binaire* (_binary classification_) quand il y a deux classes. On parle de *classification en classes multiples* (_multiclass classification_) pour désigner la répartition d'un lot de textes entre plus de deux ensembles (ou classes). On parle de *classification multi-étiquettes* (_multi-label classification_) pour désigner les problèmes de classification où plusieurs étiquettes (classes) peuvent être assignées à chaque instance.\n",
        "\n",
        "Reconnaître si un email est un spam, si une photo contient une voiture... sont des problèmes de classification (binaire).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7PRm8jl2cE0"
      },
      "source": [
        "# Sentiment analysis as a classification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee8F8DyZ9v7x"
      },
      "source": [
        "\n",
        "La tâche classique d'analyse de sentiment consiste à annoter un texte donné selon une polarité positive ou négative exprimée dans le texte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbuBLXmt3NyE"
      },
      "source": [
        "## QUESTION\n",
        "\n",
        "* Selon vous la tâche d'analyse de sentiment tel que définie juste au dessus peut se définir comme un problème de 1) classification binaire, 2) classification en classes multiples ou 3) en classification multi-étiquettes ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sjOoxLi3m7E"
      },
      "source": [
        "## VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h06bBaA34vf"
      },
      "source": [
        "## Installation et configuration de l'environnement\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WveM1n9KFNx6"
      },
      "source": [
        "### Mode GPU\n",
        "\n",
        "Quel mode d'exécution utilisez-vous (via GPU ou via CPU) ? Que vous dis le code ci-dessous ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwHyWGuWiCUu"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "# https://medium.com/@oribarel/getting-the-most-out-of-your-google-colab-2b0585f82403\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "#\n",
        "\n",
        "if len(GPUs) >0: \n",
        "  gpu = GPUs[0]\n",
        "  printm()\n",
        "else:\n",
        "  print ('no GPU. Are you sure the hardware accelerator is configured to GPU? To do this go to Runtime→Change runtime type and change the Hardware accelerator to GPU.') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxVCJnn3OBq2"
      },
      "source": [
        "Configurez votre mode d'exécution du Google Colab en mode GPU.\n",
        "\n",
        "> To use the google colab in a GPU mode you have to make sure the hardware accelerator is configured to GPU. To do this go to Runtime→Change runtime type and change the Hardware accelerator to GPU. Sometimes, all GPUs are in use and there is no GPU available.\n",
        "\n",
        "Une fois configuré le GPU, vérifiez l'état de la mémoire sur la carte en réexécutant le code \"memory footprint\" ci-dessus. Le message a-t-il changé favorablement ?\n",
        "\n",
        "Le mode d'exécution GPU est nécessaire pour le fine-tuning de BERT ci-dessous. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLfw1G8wMxkz"
      },
      "source": [
        "### Configuration et installation de ktrain\n",
        "\n",
        "[ktrain](https://github.com/amaiya/ktrain) a lightweight wrapper for the deep learning library _TensorFlow Keras_ (and other libraries) to help build, train, and deploy neural networks and other machine learning models. Inspired by ML framework extensions like fastai and ludwig, ktrain is designed to make deep learning and AI more accessible and easier to apply for both newcomers and experienced practitioners.\n",
        "\n",
        "\n",
        "Configuration de l'environnement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ-F6wCZRYpw"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"; "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUvbU_z33-Lw"
      },
      "source": [
        "Installation de la bibliothèque ktrain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxWRMgrKR7Q1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d6e8b93-b95b-4216-c55b-17fa4932a5e2"
      },
      "source": [
        "!pip install ktrain"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ktrain\n",
            "  Downloading ktrain-0.31.10.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.2.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.3.5)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ktrain) (21.3)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 58.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.42.1)\n",
            "Collecting cchardet\n",
            "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
            "\u001b[K     |████████████████████████████████| 263 kB 60.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.0.4)\n",
            "Collecting syntok>1.3.3\n",
            "  Downloading syntok-1.4.4-py3-none-any.whl (24 kB)\n",
            "Collecting transformers==4.17.0\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 51.8 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 66.3 MB/s \n",
            "\u001b[?25hCollecting keras_bert>=0.86.0\n",
            "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
            "Collecting whoosh\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[K     |████████████████████████████████| 468 kB 62.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (6.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 65.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (0.10.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (5.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 55.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0->ktrain) (4.1.1)\n",
            "Collecting keras-transformer==0.40.0\n",
            "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
            "Collecting keras-pos-embd==0.13.0\n",
            "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
            "Collecting keras-multi-head==0.29.0\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "Collecting keras-layer-normalization==0.16.0\n",
            "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
            "Collecting keras-position-wise-feed-forward==0.8.0\n",
            "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
            "Collecting keras-embed-sim==0.10.0\n",
            "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
            "Collecting keras-self-attention==0.51.0\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->ktrain) (2022.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->ktrain) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.17.0->ktrain) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.17.0->ktrain) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ktrain) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ktrain) (1.7.3)\n",
            "Building wheels for collected packages: ktrain, keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, langdetect, sacremoses\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktrain: filename=ktrain-0.31.10-py3-none-any.whl size=25312982 sha256=cf5202e209a6ec5156a9f5444f5c10580527710a6f71a1940e1cccdaa9ffa06f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/1c/1b/6df2db85720b8f5c6ea5e3ae37313cfc656f248abf910b7cfd\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=7932f025e626d1d1f091053e44554dfe237f6a78376ad1060a8cd8d405a339f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/e8/45/842b3a39831261aef9154b907eacbc4ac99499a99ae829b06f\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12305 sha256=6ad4a1889ac51acca6455c47618fb44ed7cf9ef99ac02d40bce2f645b0df362c\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/68/26/692ed21edd832833c3b0a0e21615bcacd99ca458b3f9ed571f\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3960 sha256=a977468a177c7ab14cece3d844deec862e55f835178e6eb6c4cdd55bc2e3108a\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/67/b5/d847588d075895281e1cf5590f819bd4cf076a554872268bd5\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=55ed3af79e60fbf24e7a1ca8e97192dff9ebe3613ccef22b9ed100dba0793157\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/5d/1c/2e619f594f69fbcf8bc20943b27d414871c409be053994813e\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14993 sha256=91cfee5dae2d08ae3838478a9512bdd05474c24e7e48859a96c5ff445ef4548f\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/aa/3c/9d15d24005179dae08ff291ce99c754b296347817d076fd9fb\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6962 sha256=bf15002936c0f5a1f729313b05632be4178bd4e3389ab768c59d0f7caecf7bbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/c1/a0/dc44fcf68c857b7ff6be9a97e675e5adf51022eff1169b042f\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=525da6046af5ec97325ac8f83edd4becb59821adaf5a23be335ce63f3808e910\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/75/6f/d42f6e051506f442daeba53ff1e2d21a5f20ef8c411610f2bb\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=1cfc62f452861798c4b4abbf9887fbcfd78d4badc07c31c828e0e68ff5e90b18\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=cd45cb3a9bf40600de4b52e9f178d57b479aee8dee2bb57ea6e604b973cd1d11\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=2f2b81ba3caeba92df8cdf065582a6f10af27a286e3ff7e73146541082b3bca9\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built ktrain keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention langdetect sacremoses\n",
            "Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, tokenizers, sacremoses, keras-transformer, whoosh, transformers, syntok, sentencepiece, langdetect, keras-bert, cchardet, ktrain\n",
            "Successfully installed cchardet-2.1.7 keras-bert-0.89.0 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 ktrain-0.31.10 langdetect-1.0.9 sacremoses-0.0.53 sentencepiece-0.1.97 syntok-1.4.4 tokenizers-0.13.1 transformers-4.17.0 whoosh-2.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7LRCuPB4p-y"
      },
      "source": [
        "Import de la bibliothèque ktrain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2avp8suURzMT"
      },
      "source": [
        "# Execution time 30 s\n",
        "import ktrain\n",
        "from ktrain import text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE0PnqNnM1tB"
      },
      "source": [
        "##  Allociné dataset\n",
        "\n",
        "The [Allociné dataset](https://huggingface.co/datasets/allocine) is a French-language dataset for sentiment analysis. The texts are movie reviews written between 2006 and 2020 by members of the Allociné.fr community for various films. It contains 100k positive and 100k negative reviews divided into train (160k), validation (20k), and test (20k). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Working with a sample\n",
        "\n",
        "**Skip this part !!**"
      ],
      "metadata": {
        "id": "mq4lRtbKf4Nn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRNckNWIM4N6"
      },
      "source": [
        "# Récupération\n",
        "!wget -nc https://github.com/nicolashernandez/teaching_nlp/raw/main/data/allocine-train-10000.zip -P data\n",
        "!unzip data/allocine-train-10000.zip -d data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_VN1VpGJIMH"
      },
      "source": [
        "from csv import DictReader\n",
        "\n",
        "DATA_PATH = 'data/allocine-train-10000.csv'\n",
        "\n",
        "X = list()\n",
        "Y = list()\n",
        "\n",
        "# open file in read mode\n",
        "with open(DATA_PATH, 'r') as read_obj:\n",
        "    # pass the file object to reader() to get the reader object\n",
        "    csv_dict_reader = DictReader(read_obj)\n",
        "    # get column names from a csv file\n",
        "    column_names = csv_dict_reader.fieldnames\n",
        "    print(column_names)\n",
        "    for row in csv_dict_reader:\n",
        "        #print(row['review'], row['positive'], row['negative'])\n",
        "        X.append(row['review'])\n",
        "        Y.append(row['positive'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data in \n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_valtest, y_train, y_valtest = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_valtest, y_valtest, test_size=0.5, random_state=42)\n",
        "\n",
        "print ('x_train', len(x_train), x_train[:10])\n",
        "print ('y_train',  len(y_train), y_train[:10])\n",
        "print ('x_val', len(x_val), x_val[:10])\n",
        "print ('y_val',  len(y_val), y_val[:10])\n",
        "print ('x_test', len(x_test))\n",
        "print ('y_test',  len(y_test))\n"
      ],
      "metadata": {
        "id": "W63zT-0GDZNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert a list of string into lists of int then into np.array.\n",
        "\n",
        "This step should be skiped when running BERT\n",
        "\n"
      ],
      "metadata": {
        "id": "p5vwyzSffxIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "y_train = np.array(list(map(int, y_train)))\n",
        "y_val = np.array(list(map(int, y_val)))\n",
        "y_test = np.array(list(map(int, y_test)))\n",
        "\n",
        "print ('x_train', len(x_train), x_train[:10])\n",
        "print ('y_train',  len(y_train), y_train[:10])\n",
        "print ('x_val', len(x_val), x_val[:10])\n",
        "print ('y_val',  len(y_val), y_val[:10])\n",
        "print ('x_test', len(x_test))\n",
        "print ('y_test',  len(y_test))"
      ],
      "metadata": {
        "id": "5WRLC6rWxIRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From huggingface datahub"
      ],
      "metadata": {
        "id": "hxo9pzqyVwIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "PXgmkHWnxIS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/datasets/allocine\n",
        "\n",
        "from datasets import load_dataset_builder\n",
        "ds_builder = load_dataset_builder(\"allocine\")\n",
        "\n",
        "# Inspect dataset description\n",
        "print(ds_builder.info.description)\n",
        "\n",
        "# Inspect dataset features\n",
        "print(ds_builder.info.features)\n",
        "\n",
        "# get_dataset_split_names\n",
        "from datasets import get_dataset_split_names\n",
        "get_dataset_split_names(\"allocine\")\n",
        "\n",
        "# load_dataset\n",
        "from datasets import load_dataset\n",
        "allocine_dataset = load_dataset(\"allocine\")\n",
        "allocine_dataset\n",
        "#train_dataset = load_dataset(\"allocine\", split=\"train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416,
          "referenced_widgets": [
            "440afdde3f7c42fc9fb6a1ed58ec5f43",
            "1d0d791fe97f4767a719eb8b315a1e0e",
            "6140147b08cf4e50a555ad1ac9643804",
            "ea46dcb3d96c49af8b46bf2646fb26df",
            "46ec4a3c73fe486998f1a646b1b2ed37",
            "bd36bd6c167341fe8611ffa117c7f4c6",
            "f7c79807b39240728a7f1f8f5a67eb73",
            "566b0d363ff14cd9b96cba43090ad707",
            "1646772d9e0d4aaaa16575b312fdd049",
            "b88f8925ba1647e78719db49b39f3333",
            "18e33849b075481bb41efc2b0a9890d7"
          ]
        },
        "id": "qvYzPpHJwSEu",
        "outputId": "35aa0811-8a22-40d1-a6c8-059109d65164"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Allocine Dataset: A Large-Scale French Movie Reviews Dataset.\n",
            " This is a dataset for binary sentiment classification, made of user reviews scraped from Allocine.fr.\n",
            " It contains 100k positive and 100k negative reviews divided into 3 balanced splits: train (160k reviews), val (20k) and test (20k).\n",
            "\n",
            "{'review': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset allocine (/root/.cache/huggingface/datasets/allocine/allocine/1.0.0/ea86b1dc05eae3a45a07b6281f2d4033b5fe7927b1008d06aa457ca1eae660d0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "440afdde3f7c42fc9fb6a1ed58ec5f43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['review', 'label'],\n",
              "        num_rows: 160000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['review', 'label'],\n",
              "        num_rows: 20000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['review', 'label'],\n",
              "        num_rows: 20000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Raw format\n",
        "\n",
        "**Go to the pandas dataframe format and eventually come back latter if need it...**"
      ],
      "metadata": {
        "id": "VfA43ZILVh_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = allocine_dataset['train']['review']\n",
        "y_train = allocine_dataset['train']['label']\n",
        "x_val = allocine_dataset['validation']['review']\n",
        "y_val = allocine_dataset['validation']['label']\n",
        "x_train = allocine_dataset['train']['review']\n",
        "y_train = allocine_dataset['train']['label']\n",
        "x_test = allocine_dataset['test']['review']\n",
        "y_test = allocine_dataset['test']['label']"
      ],
      "metadata": {
        "id": "akrpuENb0Xvn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('x_train', len(x_train), x_train[:10])\n",
        "print ('y_train',  len(y_train), y_train[:10])\n",
        "print ('x_val', len(x_val), x_val[:10])\n",
        "print ('y_val',  len(y_val), y_val[:10])\n",
        "print ('x_test', len(x_test))\n",
        "print ('y_test',  len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VklitiDmS3ZN",
        "outputId": "2ea913b1-063f-444d-f874-ff3d320e3e51"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train 160000 ['Si vous cherchez du cinéma abrutissant à tous les étages,n\\'ayant aucune peur du cliché en castagnettes et moralement douteux,\"From Paris with love\" est fait pour vous.Toutes les productions Besson,via sa filière EuropaCorp ont de quoi faire naître la moquerie.Paris y est encore une fois montrée comme une capitale exotique,mais attention si l\\'on se dirige vers la banlieue,on y trouve tout plein d\\'intégristes musulmans prêts à faire sauter le caisson d\\'une ambassadrice américaine.Nauséeux.Alors on se dit qu\\'on va au moins pouvoir apprécier la déconnade d\\'un classique buddy-movie avec le jeune agent aux dents longues obligé de faire équipe avec un vieux lou complètement timbré.Mais d\\'un côté,on a un Jonathan Rhys-meyers fayot au possible,et de l\\'autre un John Travolta en total délire narcissico-badass,crâne rasé et bouc proéminent à l\\'appui.Sinon,il n\\'y a aucun scénario.Seulement,des poursuites débiles sur l\\'autoroute,Travolta qui étale 10 mecs à l\\'arme blanche en 8 mouvements(!!)ou laisse son associé se faire démolir la tronche pendant qu\\'il scrute à la jumelle.Ca pourrait être un plaisir coupable,tellement c\\'est \"hénaurme\",c\\'est juste de la daube dans la droite lignée d\\'un \"Transporteur\",\"Taken\"ou \"Banlieue 13\".', \"Trash, re-trash et re-re-trash...! Une horreur sans nom. Imaginez-vous les 20 premières minutes de Orange Mécanique dilatées sur plus de 70 minutes de bande VHS pourrave et revisitées par Korine à la sauce années 2000 : les dandys-punk de Kubrick ont laissé place à des papys lubriques déguisés en sacs-poubelles forniquant les troncs d'arbres, le dispositif esthétique se résume à du filmage-réalité enfilant des scènes de destruction, de soumission, de pornographie ou encore de maltraitance ( youtube, youtube et re-youtube...) et la bande-son se limite à des ricanements malades, des rengaines obsédantes et limitées, de la logorrhée sans queue ni tête surgissant par bribes poétiques ( hummm....) et des explosions de pétard et de débris. Sur le plan émotionnel c'est une réussite complète puisque Korine parvient - avec une économie de moyens évidente dès les premières secondes - à flanquer un malaise d'une rare intensité en reprenant tout un pan des codes du porno-trash voire du snuff-movie. Mais c'est trop. Beaucoup trop. A tel point que cet objet bizarre à plus d'un niveau dérange ad nauseam et ne séduit jamais, même dans son incontestable maîtrise de la caméra et de son langage. Une sorte de Gummo sans l'inspiration ni l'ampleur. Quelque chose d'affreusement délétère se dégage de cette vidéo hallucinante et hallucinée, mise en images d'une bande d'idiots consanguins perdue dans un pitoyable autodafé white trash. Autant revoir le chef d'oeuvre de Lars Von Trier qui - au moins - avait quelque chose à transmettre... Efficace mais profondément débile et perturbant.\", 'Et si, dans les 5 premières minutes du film, la pathétique maquette de train fendant la neige en gros plan laissait bien augurer du reste ? Si cet artifice trop visible illustrait toute la lourdeur des 100 et quelques minutes de bobine, tous ces atermoiements, tout ce sentimentalisme à deux balles… ? Et une fois ce « train » arrivé en gare, qui peut croire honnêtement au coup de foudre du Comte Vronsky pour Anna ?! Or tout le film est axé sur ce moment… Seule le petit délire suicido-ferroviaire final d’Anna est à la hauteur. P… de train !', 'Mon dieu ! Quelle métaphore filée ! Je suis abasourdi ! Mais réellement, ici on a une métaphore filée de l\\'acceptation que sa mère puisse à nouveau avoir une sexualité avec quelqu\\'un d\\'autre que son papounet chéri ! Freud est aux anges et moi aussi ! Sauf qu\\'en fait c\\'est nul... Ce n\\'est pas un Disney Channel Original Movie très intéressant, faut dire qu\\'il est franchement vieux, il date de l\\'an 2000, d\\'un autre millénaire ! Les gens qui ont fait ce film n\\'ont pas connu le 11 septembre ! Vous vous rendez compte ? Toute cette candeur était encore possible ! époque bénie des dieux ! Outre la blague, on retrouve déjà tous les poncifs du genre, la meilleure amie noire, l\\'ennemie qui devient l\\'amie... la belle soeur qui au début énerve et puis finalement on s\\'y attache... oh que c\\'est mignon... ça me donne la nausée. Bref vous n\\'aurez compris rien que du très classique ici, mais ce qui m\\'énerve c\\'est la tête des actrices, sérieusement, c\\'est physique, rien que voir leur tête me donne des envies de meurtre, que je tente de réprimer bien entendu... mais le pire c\\'est le beau père... une catastrophe... c\\'est \"censé\" être le personnage \"comique\", mais on est au niveau Kev Adams de l\\'humour... Un sketch de Norman c\\'est drôle à côté... Oui, parfaitement, j\\'ose aller jusque là ! Je n\\'ai peur de rien ! Du coup c\\'est plus une curiosité qu\\'autre chose... je crois vraiment que l\\'âge d\\'or du DCOM (vers la fin des années 2000 et début 2010), ce qui les rend si frais et si beaux n\\'oeuvre pas encore ici. C\\'est juste long et chiant. Je note néanmoins quelques tentatives d\\'humour, genre le beau gosse qui dit qu\\'il aime faire de la planche à voile (je traduis) car il y a une planche et de la voile... ce à quoi la fille répond les yeux amoureux \"je n\\'ai jamais vu ça sous cet angle\". J\\'ai souri... (pas au premier degré bien sûr). Bon prochaine étape déterrer un DCOM des années 90, je ne suis pas certain d\\'en avoir déjà vu !', 'Premier film de la saga Kozure Okami, \"Le Sabre de la vengeance\" est un très bon film qui mêle drame et action, et qui, en 40 ans, n\\'a pas pris une ride.', \"L'amnésie est un thème en or pour susciter le mystère. Encore faut-il être capable de construire un scénario qui se tienne. Celui-ci est boursouflé et accumule incohérences et invraisemblances. Notons aussi la stupidité du titre français, sans lien avec l'histoire.\", \"Tout commence comme une comédie légère avant un drame soudain qui fait basculer le film dans un...suspense incroyable mâtiné d'une étude psychologique très fine des mécanismes qui régissent un couple, en particulier, et un groupe social, en général. A propos d'Elly est un film qui bat pavillon iranien -l'analyse des comportements de la nouvelle bourgeoisie du pays est remarquable- mais son propos, la quête de sa propre identité, a une portée universelle, à la manière des oeuvres d'Antonioni. La différence, et elle est de taille, c'est le rythme, rapide et nerveux, que Asghar Farhadi imprime au scénario, à l'opposé de la quasi totalité des films iraniens vus jusqu'alors. Le montage, admirable, arrive à faire oublier qu'il s'agit d'un huis clos (étouffant) où les dialogues (formidables) font avancer l'action. Le tout dans une villa sise au bord d'une mer agitée, personnage central du film, la seule à connaître la réponse à l'énigme. Interprété de façon magistral (mention spéciale aux actrices, sublimes), A propos d'Elly est un conte persan moderne auquel on ne voit qu'un seul défaut : ne durer que 116 minutes.\", \"un excellent film qui merite ses quatre étoiles. tellement bon que les ricains en font dejà une suite, qu'il s'empresseront de pourrir...\", 'Deuxième long métrage de Pasolini, Mamma Roma contient déjà la plupart des obsessions de son auteur et notamment la relation si importante dans la construction de l’être humain entre la mère et son fils adolescent. Anna Magnani est déchirante en figure presque universelle de la maman putain représentative de la Ville éternelle. Le film avance à travers des foules de symboles et la fin où le jeune homme termine sa vie en crucifié martyr de la société est d’un implacable réalisme poétique qui annonce les avancées extrêmes ultérieures de Salo. La construction du récit est d’une puissance hors du commun et Pasolini se montre déjà un grand cinéaste qui a assimilé la technique et les possibilités de ce nouvel outil.', 'Créateur de la célèbre série télévisée Kaamelott (2004/2009), Alexandre Astier s’essaie avec plus ou moins de conviction à son premier long-métrage à travers lequel il dépeint une étrange relation entre un thérapeute et une riche patiente atteinte de troubles de la mémoire. Pour sa première incursion dans le cinéma (derrière la caméra), il a vu les choses en grand, peut-être un peu trop (en plus d’assurer la réalisation, il est aussi scénariste, compositeur et acteur principal !). Résultat, son drame ne parvient pas à insuffler toute la dimension et l’émotion requise qu’il aurait dû nous transmettre. Si l’ambiance particulière qui unie les deux acteurs (Isabelle Adjani & Alexandre Astier) est palpable, on regrettera cependant les nombreuses touches d’humour qui n’avaient pas réellement leur place ici. Signalons tout de même une agréable mise en scène, de beaux plans (pas mal de \"dutch angles\"), une séduisante B.O et des acteurs convaincants, même si l’on aurait aimé que le film puisse nous faire vibrer plus que cela.']\n",
            "y_train 160000 [0, 0, 0, 0, 1, 0, 1, 1, 1, 0]\n",
            "x_val 20000 [\"Ce film est tout ce qu'il y a de plus sympa. Même si l'ensemble n'est pas dépourvu de clichés, il serait hypocrite de dire que ce film est ennuyeux à regarder, bien au contraire. Il est très plaisant à regarder et même si l'ensemble est asse convenue, la mise en scène de Peter Chelsom est légère et ce film est une sorte de bouffée d'air frais. Tout à fait estimable.\", \"The Wall a été réalisé par Alan Parker (Fame, Midnight Express, Evita) et Roger Waters (bassiste de Pink Floyd). Le film ne contient quasiment aucun dialogue, il s'agit en fait du double album conceptuel The Wall en images. Pink est une rock star déchue qui utilise tous les paradis artificiels possibles pour fuir le réel, mais cela ne fait que l'enfoncer encore plus dans son état sombre. Ce film est tout autant fascinant que dérangeant : on regarde cet homme sombrer dans la paranoïa, sans pouvoir rien faire, comme les spectateurs pervers d'une mise à mort, fascinés par cette chose incontrôlable, mais dégoûtés en même temps. Certains peuvent penser que le film n'a pas de scénario. En fait, les actions arrivent comme Pink les pense, emmêlées et désordonnées, à l'image de son esprit torturé influencé par la drogue, l'alcool, le sexe, et son vécu. Je trouve que c'est un bon film, à regarder plusieurs fois pour analyser toutes les images.\", 'Encore un film majeur tres mal distribué, comme par hasard... comme si on voulait nous empecher de comprendre comment on peut se faire gangrener par le mal. Est-ce que vous comprenez ? 4,5/5; cinematographiquement parlant.', 'L\\'idée est très bonne mais le film manque de rythme malgré sa courte durée, et il me parait \"vieillot\" et je ne dis pas ça en raison de sa date de sortie.', 'Un petit nanar rigolo a regarder. A voir une fois pour rigoler un peu un dimanche soir ou vous avez rien a faire.', \"Ashley Judd est superbe dans son combat pour sauver son mari. Morgan Freeman est très attachant dans son rôle d'avocat alcoolique mal rasé. Jim Caviezel nous bluffe dans un personnage ingrat. L'intrigue nous tient en haleine d'un bout à l'autre, vraiment on ne s'ennuie jamais : c'est plusqu'un bon polar... ! J'ai beaucoup aimé.\", 'Un excellent \"Dents de la mer\" version ourson. PEDOBEAR dans un parc, un ourson sème la terreur dans un parc, il torture les habitants, il frappe des enfants, il fait peur, très bon film, les acteurs sont très talentieux, RIP Robin williams, les effets spéciaux sont super biens restaurés.', \"Un film magnifique, une histoire d'amitié, une histoire d'hommes et de tout ce qui les touche profondément. Et oui, nous sommes ici bien loin des clichés traditionnels concernant les passions masculines : ni bière ni foot, ni sexe ni exploits de coqs. De la poésie, de l'amour, de l'amitié et ce fameux doute qui hante souvent la vie des hommes. Le tandem Noiret-Troisi est tout à fait exceptionnel . La rencontre entre un homme cultivé et idéaliste avec un type simple et rêveur. Rien ne les rapproche à la base, et pourtant c'est bien sur le thème de l'idéalisme qu'une amitié profonde naît entre ces deux hommes que tout oppose. Paysages sublimes, B.O captivante et très belle histoire. Je crois que c'est un de mes films préférés.\", \"Excellent drame - qui se termine avec une note d'espoir. DrameS entrecroisés en fait : le cadre survolté qui se retrouve à l'hôpital psychiatrique suite à une trahison d'entreprise, la jeune mère à qui on a retiré la garde de son enfant... Le cadre se remémore toutes les trahisons nécessaires à son emploi qu'il a lui-même commises, Hélène noue contact avec lui à l'hôpital pour le sortir de son apathie... puis il élève la voix. Contre la chambre d'isolement à outrance, contre la camisole chimique, contre les mauvais traitements, contre l'absurdité bureaucratique juridique, contre l'abus de pouvoir en général. Le film est aussi un réquisitoire contre le profit inhumain, contre l'égoïsme. Et une proposition de comportement humain et altruiste. Excellents acteurs pour la plupart. Je ne regrette rien.\", 'Un excellent biopic sur une étoile filante du rock \\'n\\' roll impeccablement réalisé. Le rythme très bon dans la première partie s\\'effiloche légèrement dans la seconde mais pas suffisamment pour qu\\'on perde intérêt un seul instant à ce film . L\\'interprétation est en plus tout simplement parfaite. Lou Diamond Phillips crève l\\'écran dans le rôle de Ritchie Valens. Porté par un certain souffle et des chansons aussi mémorables que \"Come on lets go\", \"Oh Donna\" et bien sûr celle qui donne son titre au film \"La Bamba\", ce film est une oeuvre captivante de bout en bout et est peut-être le meilleur biopic consacré à un chanteur.']\n",
            "y_val 20000 [0, 1, 1, 0, 0, 1, 1, 1, 1, 1]\n",
            "x_test 20000\n",
            "y_test 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert a list of string into lists of int then into np.array.\n",
        "\n",
        "This step should be skiped when running BERT\n",
        "\n"
      ],
      "metadata": {
        "id": "-5QELOS7T-3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y_train = np.array(list(map(int, y_train)))\n",
        "y_val = np.array(list(map(int, y_val)))\n",
        "y_test = np.array(list(map(int, y_test)))\n",
        "\n",
        "print ('x_train', len(x_train), x_train[:10])\n",
        "print ('y_train',  len(y_train), y_train[:10])\n",
        "print ('x_val', len(x_val), x_val[:10])\n",
        "print ('y_val',  len(y_val), y_val[:10])\n",
        "print ('x_test', len(x_test))\n",
        "print ('y_test',  len(y_test))"
      ],
      "metadata": {
        "id": "_5znboCET97a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pandas DataFrame format"
      ],
      "metadata": {
        "id": "hvdy-2-DV9zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create DataFrame\n",
        "def huggingface_dataset_split_to_dataframe (huggingface_dataset_split):\n",
        "  df = pd.DataFrame(huggingface_dataset_split)\n",
        "  # https://github.com/amaiya/ktrain/blob/master/examples/text/ArabicHotelReviews-nbsvm.ipynb\n",
        "  df['label'] = df['label'].apply(lambda x: 'negative' if x == 0 else 'positive')\n",
        "  df = pd.concat([df, df.label.astype('str').str.get_dummies()], axis=1, sort=False)\n",
        "  df = train_df[['review', 'negative', 'positive']]\n",
        "  return df\n",
        "\n",
        "train_df = huggingface_dataset_split_to_dataframe(allocine_dataset['train'])\n",
        "val_df = huggingface_dataset_split_to_dataframe(allocine_dataset['validation'])\n",
        "test_df = huggingface_dataset_split_to_dataframe(allocine_dataset['test'])\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BEkNPZg705jM",
        "outputId": "52ebac6c-e745-48ad-a88b-64add64ca37b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  negative  positive\n",
              "0  Si vous cherchez du cinéma abrutissant à tous ...         1         0\n",
              "1  Trash, re-trash et re-re-trash...! Une horreur...         1         0\n",
              "2  Et si, dans les 5 premières minutes du film, l...         1         0\n",
              "3  Mon dieu ! Quelle métaphore filée ! Je suis ab...         1         0\n",
              "4  Premier film de la saga Kozure Okami, \"Le Sabr...         0         1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-28411b6f-7289-46aa-9e2c-de9148d95534\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Si vous cherchez du cinéma abrutissant à tous ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Trash, re-trash et re-re-trash...! Une horreur...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Et si, dans les 5 premières minutes du film, l...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mon dieu ! Quelle métaphore filée ! Je suis ab...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Premier film de la saga Kozure Okami, \"Le Sabr...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28411b6f-7289-46aa-9e2c-de9148d95534')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-28411b6f-7289-46aa-9e2c-de9148d95534 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-28411b6f-7289-46aa-9e2c-de9148d95534');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYGyWUwn9yPS"
      },
      "source": [
        "## Fonctionnement de ktrain\n",
        "\n",
        "\n",
        "Pour réaliser cette tâche de classification nous allons utiliser la bibliothèque ktrain pour faire un apprentissage par transfert.\n",
        "\n",
        "Les étapes sont les suivantes\n",
        "1. chargement des données avec application d'un prétraitement défini à la volée\n",
        "2. construction d'un modèle de classification sur la base d'un modèle pré-entraîné spécifié\n",
        "3. récupération d'une instance du modèle pour la personnalisation de celui-ci\n",
        "4. recherche d'un bon taux d'apprentissage\n",
        "5. entraînement du classifieur i.e. personnalisation du modèle de base à l'aide d'un taux d'apprentissage défini\n",
        "6. utilisation du nouveau modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAvPw6g49WGa"
      },
      "source": [
        "*ETAPE 1 :* \n",
        "\n",
        "Le type de pré-traitement est fonction du modèle pré-entraîné spécifié."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTBW_SEO9mXi"
      },
      "source": [
        "*ETAPE 2 :*\n",
        "\n",
        "ktrain vient avec quelques modèles pré-entraînés packagés. Pour les connaître, exécutez : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7FvrdBC9n2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11cae32d-e740-4736-a7e8-fadec0710964"
      },
      "source": [
        "text.print_text_classifiers()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fasttext: a fastText-like model [http://arxiv.org/pdf/1607.01759.pdf]\n",
            "logreg: logistic regression using a trainable Embedding layer\n",
            "nbsvm: NBSVM model [http://www.aclweb.org/anthology/P12-2018]\n",
            "bigru: Bidirectional GRU with pretrained fasttext word vectors [https://fasttext.cc/docs/en/crawl-vectors.html]\n",
            "standard_gru: simple 2-layer GRU with randomly initialized embeddings\n",
            "bert: Bidirectional Encoder Representations from Transformers (BERT) from keras_bert [https://arxiv.org/abs/1810.04805]\n",
            "distilbert: distilled, smaller, and faster BERT from Hugging Face transformers [https://arxiv.org/abs/1910.01108]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae1TJ5nk_K7G"
      },
      "source": [
        "Vous connaissez **fasttext**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XtFCEfYWchi"
      },
      "source": [
        "[**NBSVM**](https://medium.com/@asmaiya/a-neural-implementation-of-nbsvm-in-keras-d4ef8c96cb7c) is an approach to text classification proposed by [Wang and Manning](https://www.aclweb.org/anthology/P12-2018) that takes a linear model such as SVM (or logistic regression) and infuses it with Bayesian probabilities by replacing word count features with Naive Bayes log-count ratios. Despite its simplicity, NBSVM models have been shown to be both fast and powerful across a wide range of different text classification datasets. \n",
        "Keras offers a NBSVM model implemented as a neural network using two embedding layers. The first stores the Naive Bayes log-count ratios. The second stores learned weights (or coefficients) for each feature (i.e., word) in this linear model. The prediction, then, is simply the dot product of these two vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0H3RJMw_xhB"
      },
      "source": [
        "**BERT** (_Bidirectional Encoder Representations from Transformers_), proposé par [Google AI Language](https://arxiv.org/pdf/1810.04805.pdf) est un encodeur bidirectionnel qui applique un modèle d'attention Transformers à la modélisation du language (_language modeling_). Il représente l'état de l'art.\n",
        "Les Transformers sont un mécanisme d'attention qui apprennent les relations contextuelles entre les mots dans un texte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB3bN2mE_xyi"
      },
      "source": [
        "Il est possible d'[**encapsuler les modèles Transformers du site _hugging face_**](https://github.com/amaiya/ktrain/blob/master/tutorials/tutorial-A3-hugging_face_transformers.ipynb). _hugging face_ diffuse les [modèles Transformers pré-entraînés \"officiels\"](https://huggingface.co/transformers/pretrained_models.html) ainsi que les [modèles Transformers construits par la communauté](https://huggingface.co/models).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wINsCEpD1-s"
      },
      "source": [
        "*ETAPE 4 :*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4syPI6CI6Vt6"
      },
      "source": [
        "Le **taux d'apprentissage (_learning rate_)** est un hyperparamètre qui contrôle combien le modèle doit changer en réponse à l'erreur estimée à chaque fois que les poids du modèles sont mis à jour. Choisir un 'lr' trop petit conduit à une longue phase d'entraînement qui peut resté bloquée. Choisir un 'lr' trop grand conduit à un apprentissage sous-optimal des poids et à une instabilité du processus d'entraînement. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMJHB4zgRCTw"
      },
      "source": [
        "## FastText\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPc3RUc_TYDc"
      },
      "source": [
        "*ETAPE 1 :* \n",
        "\n",
        "Les méthodes [`texts_from_csv`](https://amaiya.github.io/ktrain/text/index.html#ktrain.text.texts_from_csv) ou [`texts_from_df`](https://amaiya.github.io/ktrain/text/index.html#ktrain.text.texts_from_df) charge le corpus, normalise les documents (définit un préprocesseur réutilisable), et découpe la collection en données d'entraînement et données de validation (à moins que des données de validation soient passées en argument).\n",
        "\n",
        "Cette étape est commune aux modèles FastText et NBSVM (`preprocess_mode='standard'`). BERT utilise son propre tokenizer. Il faudra faire un pré-traitement dédié (`preprocess_mode='bert'`).\n",
        "\n",
        "```\n",
        "* train_filepath(str): file path to training CSV\n",
        "* text_column(str): name of column containing the text\n",
        "* label_column(list): list of columns that are to be treated as labels\n",
        "* val_filepath(string): file path to test CSV.  If not supplied, 10% of documents in training CSV will be used for testing/validation.\n",
        "* max_features(int): max num of words to consider in vocabulary ; Note: This is only used for preprocess_mode='standard'.\n",
        "* maxlen(int): each document can be of most <maxlen> words. 0 is used as padding ID.\n",
        "* ngram_range(int): size of multi-word phrases to consider e.g., 2 will consider both 1-word phrases and 2-word phrases limited by max_features\n",
        "* preprocess_mode (str):  Either 'standard' (normal tokenization) or one of {'bert', 'distilbert'} tokenization and preprocessing for use with                BERT/DistilBert text classification model.\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fasttext\n",
        "#NUM_WORDS = 50000\n",
        "#MAXLEN = 150\n",
        "#NGRAMS_SIZE = 1# 1 # 8 minutes avec 2 pour 10000 examples\n",
        "\n",
        "# nbsvm \n",
        "#NUM_WORDS = 80000\n",
        "#MAXLEN = 2000\n",
        "#NGRAMS_SIZE = 3\n",
        "\n",
        "(x_train, y_train), (x_val, y_val), preproc = text.texts_from_df (train_df, \n",
        "                                                                   'review', # name of column containing review text\n",
        "                                                                   label_columns=['negative', 'positive'],\n",
        "                                                                   val_df=val_df, # if None, 10% of data will be used for validation\n",
        "                                          #max_features=NUM_WORDS, \n",
        "                                          #maxlen=MAXLEN,\n",
        "                                          #ngram_range=NGRAMS_SIZE,\n",
        "                                          preprocess_mode='standard' # default\n",
        "                                          )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuKxqzFd3QBU",
        "outputId": "82a5a333-afbd-4824-8a2a-e4b0ad6bf1c7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['negative', 'positive']\n",
            "   negative  positive\n",
            "0         1         0\n",
            "1         1         0\n",
            "2         1         0\n",
            "3         1         0\n",
            "4         0         1\n",
            "['negative', 'positive']\n",
            "   negative  positive\n",
            "0         1         0\n",
            "1         1         0\n",
            "2         1         0\n",
            "3         1         0\n",
            "4         0         1\n",
            "language: fr\n",
            "Word Counts: 186307\n",
            "Nrows: 160000\n",
            "160000 train sequences\n",
            "train sequence lengths:\n",
            "\tmean : 87\n",
            "\t95percentile : 241\n",
            "\t99percentile : 303\n",
            "x_train shape: (160000,400)\n",
            "y_train shape: (160000, 2)\n",
            "Is Multi-Label? False\n",
            "160000 test sequences\n",
            "test sequence lengths:\n",
            "\tmean : 87\n",
            "\t95percentile : 241\n",
            "\t99percentile : 303\n",
            "x_test shape: (160000,400)\n",
            "y_test shape: (160000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LktiVGh8NVSW"
      },
      "source": [
        "Observons les 5 premières des données prétraitées. `x_` représente la donnée et `y_` la classe. On note que les données ont été transformées. Chaque mot est remplacé par un identifiant. On note que la classe est décrite par 2 colonnes avec deux codes \"1 0\" et \"0 1\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzzWhW3ZHAq8",
        "outputId": "560b4987-843b-4ad7-fefb-45339ba14de5"
      },
      "source": [
        "print ('x_train', x_train[:5])\n",
        "print ('y_train', y_train[:5])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train [[    0     0     0 ...    47  2583  2509]\n",
            " [    0     0     0 ...  1221     2 10399]\n",
            " [    0     0     0 ...   907     1  1085]\n",
            " [    0     0     0 ...   177   150    91]\n",
            " [    0     0     0 ...   423    12  3444]]\n",
            "y_train [[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHhCXpcQOPje"
      },
      "source": [
        "*ETAPE 2 et 3:* \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3jEacvRRuih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd2b8aa-1860-4cf6-dfac-01d8d003db8b"
      },
      "source": [
        "# Build and return a text classification model https://amaiya.github.io/ktrain/text/index.html#ktrain.text.text_classifier\n",
        "model = text.text_classifier('fasttext', (x_train, y_train), preproc=preproc)\n",
        "\n",
        "# Returns a Learner instance that can be used to tune and train Keras models https://amaiya.github.io/ktrain/index.html#ktrain.get_learner\n",
        "learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_val, y_val))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Multi-Label? False\n",
            "compiling word ID features...\n",
            "maxlen is 400\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufKylXAeOVCU"
      },
      "source": [
        "*ETAPE 4 :* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9_32BN5Ur7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "f951a0ab-93f2-4744-b944-b8760927e097"
      },
      "source": [
        "# recherche d'un bon taux d'apprentissage \n",
        "learner.lr_find()\n",
        "learner.lr_plot()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "simulating training for different learning rates... this may take a few moments...\n",
            "Epoch 1/1024\n",
            "5000/5000 [==============================] - 9s 2ms/step - loss: 0.8657 - accuracy: 0.5574\n",
            "\n",
            "\n",
            "done.\n",
            "Please invoke the Learner.lr_plot() method to visually inspect the loss plot to help identify the maximal learning rate associated with falling loss.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0eWERAHZ_Go"
      },
      "source": [
        "*ETAPE 5 :* \n",
        "\n",
        "[autofit](https://amaiya.github.io/ktrain/core.html#ktrain.core.Learner.autofit)\n",
        "Automatically train model using a default learning rate schedule shown to work well in practice.  By default, this method currently employs a triangular learning rate policy (https://arxiv.org/abs/1506.01186).  \n",
        "During each epoch, this learning rate policy varies the learning rate from lr/10 to lr and then back to a low learning rate that is near-zero. \n",
        "If epochs is None, then early_stopping and reduce_on_plateau are atomatically\n",
        "set to 6 and 3, respectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOMvkq1ZPXV2"
      },
      "source": [
        "#### QUESTION : training\n",
        "\n",
        "* Sur le graph ci-dessus, repérez approximativement la puissance n de 1/10^n où la chute de la loss devient importante. Testez avec cette valeur comme learning rate, observez votre performance (accuracy) sur le train et le val.\n",
        "\n",
        "Dans l'extrait de log ci-dessous, loss et accuracy concerne le corpus de train tandis que  val_loss et val_accuracy le corpus de validation.\n",
        "```\n",
        "282/282 [==============================] - 7s 25ms/step - loss: 0.1105 - accuracy: 0.9566 - val_loss: 0.3318 - val_accuracy: 0.8911\n",
        "```\n",
        "* Est-ce normal d'observer un écart d'accuracy entre le train et le val ?\n",
        "* Stoquez le score obtenu en dernière étape, et relancez le finetuning sans changer le paramétrage. Obtenez-vous les mêmes résultats ? Pourquoi ? La réponse est à chercher dans l'initiatilisation des poids du réseau neuronal.\n",
        "* Volontairement tester des lr avec d'autres puissances de 10 (0.1, 0.01, 0.001, 0.0001). Est-ce que cela marche mieux ? Attention, suivant votre choix de lr l'entraînement peut prendre quelques minutes à au moins 1h....\n",
        "\n",
        "Mon meilleur score est `loss: 0.1615 - accuracy: 0.9384 - val_loss: 0.2864 - val_accuracy: 0.8931` (non c'est plus vrai) et vous ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vY7oJU7U4G0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e7ae13-6e95-4830-f99b-8689da8a7f04"
      },
      "source": [
        "# wi all allocine training dataset (8 minutes) \n",
        "learner.autofit(0.01)\n",
        "# Epoch 16/1024\n",
        "# 4491/4500 [============================>.] - ETA: 0s - loss: 0.1540 - accuracy: 0.9418Restoring model weights from the end of the best epoch: 11.\n",
        "# 4500/4500 [==============================] - 25s 6ms/step - loss: 0.1539 - accuracy: 0.9418 - val_loss: 0.2173 - val_accuracy: 0.9206\n",
        "# Epoch 16: early stopping\n",
        "\n",
        "# wi all allocine training dataset (at least one hour run) \n",
        "#learner.autofit(0.00001)\n",
        "# Epoch 129/1024\n",
        "# 4497/4500 [============================>.] - ETA: 0s - loss: 0.2197 - accuracy: 0.9135Restoring model weights from the end of the best epoch: 124.\n",
        "# 4500/4500 [==============================] - 29s 6ms/step - loss: 0.2197 - accuracy: 0.9135 - val_loss: 0.2058 - val_accuracy: 0.9204\n",
        "# Epoch 129: early stopping"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "early_stopping automatically enabled at patience=5\n",
            "reduce_on_plateau automatically enabled at patience=2\n",
            "\n",
            "\n",
            "begin training using triangular learning rate policy with max lr of 0.01...\n",
            "Epoch 1/1024\n",
            "5000/5000 [==============================] - 37s 7ms/step - loss: 0.3701 - accuracy: 0.8366 - val_loss: 0.2100 - val_accuracy: 0.9216\n",
            "Epoch 2/1024\n",
            "5000/5000 [==============================] - 36s 7ms/step - loss: 0.2841 - accuracy: 0.8859 - val_loss: 0.1951 - val_accuracy: 0.9320\n",
            "Epoch 3/1024\n",
            "5000/5000 [==============================] - 50s 10ms/step - loss: 0.2643 - accuracy: 0.8951 - val_loss: 0.1778 - val_accuracy: 0.9361\n",
            "Epoch 4/1024\n",
            "5000/5000 [==============================] - 41s 8ms/step - loss: 0.2523 - accuracy: 0.9001 - val_loss: 0.1701 - val_accuracy: 0.9404\n",
            "Epoch 5/1024\n",
            "5000/5000 [==============================] - 39s 8ms/step - loss: 0.2417 - accuracy: 0.9052 - val_loss: 0.1572 - val_accuracy: 0.9443\n",
            "Epoch 6/1024\n",
            "5000/5000 [==============================] - 41s 8ms/step - loss: 0.2339 - accuracy: 0.9093 - val_loss: 0.1498 - val_accuracy: 0.9480\n",
            "Epoch 7/1024\n",
            "5000/5000 [==============================] - 46s 9ms/step - loss: 0.2277 - accuracy: 0.9125 - val_loss: 0.1466 - val_accuracy: 0.9494\n",
            "Epoch 8/1024\n",
            "5000/5000 [==============================] - 42s 8ms/step - loss: 0.2188 - accuracy: 0.9163 - val_loss: 0.1392 - val_accuracy: 0.9519\n",
            "Epoch 9/1024\n",
            "5000/5000 [==============================] - 53s 11ms/step - loss: 0.2149 - accuracy: 0.9180 - val_loss: 0.1335 - val_accuracy: 0.9539\n",
            "Epoch 10/1024\n",
            "5000/5000 [==============================] - 54s 11ms/step - loss: 0.2124 - accuracy: 0.9197 - val_loss: 0.1284 - val_accuracy: 0.9556\n",
            "Epoch 11/1024\n",
            "5000/5000 [==============================] - 37s 7ms/step - loss: 0.2079 - accuracy: 0.9209 - val_loss: 0.1236 - val_accuracy: 0.9569\n",
            "Epoch 12/1024\n",
            "5000/5000 [==============================] - 38s 8ms/step - loss: 0.2028 - accuracy: 0.9232 - val_loss: 0.1210 - val_accuracy: 0.9594\n",
            "Epoch 13/1024\n",
            "5000/5000 [==============================] - 41s 8ms/step - loss: 0.2004 - accuracy: 0.9245 - val_loss: 0.1147 - val_accuracy: 0.9612\n",
            "Epoch 14/1024\n",
            "5000/5000 [==============================] - 36s 7ms/step - loss: 0.1968 - accuracy: 0.9262 - val_loss: 0.1120 - val_accuracy: 0.9622\n",
            "Epoch 15/1024\n",
            "5000/5000 [==============================] - 37s 7ms/step - loss: 0.1913 - accuracy: 0.9282 - val_loss: 0.1066 - val_accuracy: 0.9638\n",
            "Epoch 16/1024\n",
            "5000/5000 [==============================] - 54s 11ms/step - loss: 0.1904 - accuracy: 0.9283 - val_loss: 0.1041 - val_accuracy: 0.9648\n",
            "Epoch 17/1024\n",
            "5000/5000 [==============================] - 51s 10ms/step - loss: 0.1887 - accuracy: 0.9301 - val_loss: 0.1034 - val_accuracy: 0.9661\n",
            "Epoch 18/1024\n",
            "5000/5000 [==============================] - 37s 7ms/step - loss: 0.1827 - accuracy: 0.9313 - val_loss: 0.1022 - val_accuracy: 0.9667\n",
            "Epoch 19/1024\n",
            "5000/5000 [==============================] - 45s 9ms/step - loss: 0.1814 - accuracy: 0.9330 - val_loss: 0.0983 - val_accuracy: 0.9671\n",
            "Epoch 20/1024\n",
            "5000/5000 [==============================] - 38s 8ms/step - loss: 0.1786 - accuracy: 0.9339 - val_loss: 0.0935 - val_accuracy: 0.9689\n",
            "Epoch 21/1024\n",
            "5000/5000 [==============================] - 39s 8ms/step - loss: 0.1791 - accuracy: 0.9336 - val_loss: 0.0977 - val_accuracy: 0.9689\n",
            "Epoch 22/1024\n",
            "5000/5000 [==============================] - 40s 8ms/step - loss: 0.1744 - accuracy: 0.9355 - val_loss: 0.0914 - val_accuracy: 0.9702\n",
            "Epoch 23/1024\n",
            "5000/5000 [==============================] - 41s 8ms/step - loss: 0.1725 - accuracy: 0.9368 - val_loss: 0.0890 - val_accuracy: 0.9711\n",
            "Epoch 24/1024\n",
            "5000/5000 [==============================] - 44s 9ms/step - loss: 0.1695 - accuracy: 0.9377 - val_loss: 0.0868 - val_accuracy: 0.9719\n",
            "Epoch 25/1024\n",
            "5000/5000 [==============================] - 37s 7ms/step - loss: 0.1709 - accuracy: 0.9371 - val_loss: 0.0849 - val_accuracy: 0.9729\n",
            "Epoch 26/1024\n",
            "5000/5000 [==============================] - 41s 8ms/step - loss: 0.1671 - accuracy: 0.9401 - val_loss: 0.0820 - val_accuracy: 0.9734\n",
            "Epoch 27/1024\n",
            "5000/5000 [==============================] - 53s 11ms/step - loss: 0.1672 - accuracy: 0.9387 - val_loss: 0.0804 - val_accuracy: 0.9743\n",
            "Epoch 28/1024\n",
            "5000/5000 [==============================] - 38s 8ms/step - loss: 0.1638 - accuracy: 0.9410 - val_loss: 0.0764 - val_accuracy: 0.9751\n",
            "Epoch 29/1024\n",
            "5000/5000 [==============================] - 37s 7ms/step - loss: 0.1643 - accuracy: 0.9412 - val_loss: 0.0766 - val_accuracy: 0.9754\n",
            "Epoch 30/1024\n",
            "5000/5000 [==============================] - 37s 7ms/step - loss: 0.1618 - accuracy: 0.9415 - val_loss: 0.0755 - val_accuracy: 0.9760\n",
            "Epoch 31/1024\n",
            "5000/5000 [==============================] - 40s 8ms/step - loss: 0.1576 - accuracy: 0.9430 - val_loss: 0.0716 - val_accuracy: 0.9767\n",
            "Epoch 32/1024\n",
            "5000/5000 [==============================] - 40s 8ms/step - loss: 0.1595 - accuracy: 0.9419 - val_loss: 0.0721 - val_accuracy: 0.9772\n",
            "Epoch 33/1024\n",
            "5000/5000 [==============================] - 39s 8ms/step - loss: 0.1575 - accuracy: 0.9437 - val_loss: 0.0706 - val_accuracy: 0.9774\n",
            "Epoch 34/1024\n",
            "5000/5000 [==============================] - 37s 7ms/step - loss: 0.1550 - accuracy: 0.9447 - val_loss: 0.0720 - val_accuracy: 0.9785\n",
            "Epoch 35/1024\n",
            "5000/5000 [==============================] - 42s 8ms/step - loss: 0.1528 - accuracy: 0.9439 - val_loss: 0.0669 - val_accuracy: 0.9792\n",
            "Epoch 36/1024\n",
            "5000/5000 [==============================] - 42s 8ms/step - loss: 0.1534 - accuracy: 0.9451 - val_loss: 0.0701 - val_accuracy: 0.9788\n",
            "Epoch 37/1024\n",
            "5000/5000 [==============================] - 41s 8ms/step - loss: 0.1548 - accuracy: 0.9447 - val_loss: 0.0661 - val_accuracy: 0.9786\n",
            "Epoch 38/1024\n",
            "5000/5000 [==============================] - 42s 8ms/step - loss: 0.1511 - accuracy: 0.9462 - val_loss: 0.0656 - val_accuracy: 0.9802\n",
            "Epoch 39/1024\n",
            "5000/5000 [==============================] - 56s 11ms/step - loss: 0.1506 - accuracy: 0.9467 - val_loss: 0.0648 - val_accuracy: 0.9806\n",
            "Epoch 40/1024\n",
            "5000/5000 [==============================] - 39s 8ms/step - loss: 0.1456 - accuracy: 0.9475 - val_loss: 0.0626 - val_accuracy: 0.9806\n",
            "Epoch 41/1024\n",
            "5000/5000 [==============================] - 38s 8ms/step - loss: 0.1456 - accuracy: 0.9474 - val_loss: 0.0604 - val_accuracy: 0.9812\n",
            "Epoch 42/1024\n",
            "5000/5000 [==============================] - 38s 8ms/step - loss: 0.1484 - accuracy: 0.9473 - val_loss: 0.0627 - val_accuracy: 0.9817\n",
            "Epoch 43/1024\n",
            "5000/5000 [==============================] - 38s 8ms/step - loss: 0.1472 - accuracy: 0.9472 - val_loss: 0.0575 - val_accuracy: 0.9818\n",
            "Epoch 44/1024\n",
            "5000/5000 [==============================] - 38s 8ms/step - loss: 0.1449 - accuracy: 0.9490 - val_loss: 0.0590 - val_accuracy: 0.9822\n",
            "Epoch 45/1024\n",
            "4998/5000 [============================>.] - ETA: 0s - loss: 0.1450 - accuracy: 0.9483\n",
            "Epoch 00045: Reducing Max LR on Plateau: new max lr will be 0.005 (if not early_stopping).\n",
            "5000/5000 [==============================] - 37s 7ms/step - loss: 0.1451 - accuracy: 0.9482 - val_loss: 0.0611 - val_accuracy: 0.9820\n",
            "Epoch 46/1024\n",
            "5000/5000 [==============================] - 38s 8ms/step - loss: 0.1317 - accuracy: 0.9532 - val_loss: 0.0544 - val_accuracy: 0.9835\n",
            "Epoch 47/1024\n",
            "5000/5000 [==============================] - 39s 8ms/step - loss: 0.1285 - accuracy: 0.9546 - val_loss: 0.0526 - val_accuracy: 0.9844\n",
            "Epoch 48/1024\n",
            "5000/5000 [==============================] - 39s 8ms/step - loss: 0.1267 - accuracy: 0.9552 - val_loss: 0.0504 - val_accuracy: 0.9850\n",
            "Epoch 49/1024\n",
            "5000/5000 [==============================] - 41s 8ms/step - loss: 0.1229 - accuracy: 0.9562 - val_loss: 0.0484 - val_accuracy: 0.9858\n",
            "Epoch 50/1024\n",
            "5000/5000 [==============================] - 44s 9ms/step - loss: 0.1235 - accuracy: 0.9560 - val_loss: 0.0468 - val_accuracy: 0.9862\n",
            "Epoch 51/1024\n",
            "5000/5000 [==============================] - 42s 8ms/step - loss: 0.1210 - accuracy: 0.9570 - val_loss: 0.0465 - val_accuracy: 0.9865\n",
            "Epoch 52/1024\n",
            "5000/5000 [==============================] - 52s 10ms/step - loss: 0.1191 - accuracy: 0.9575 - val_loss: 0.0444 - val_accuracy: 0.9869\n",
            "Epoch 53/1024\n",
            "5000/5000 [==============================] - 55s 11ms/step - loss: 0.1203 - accuracy: 0.9579 - val_loss: 0.0423 - val_accuracy: 0.9871\n",
            "Epoch 54/1024\n",
            "5000/5000 [==============================] - 40s 8ms/step - loss: 0.1186 - accuracy: 0.9582 - val_loss: 0.0437 - val_accuracy: 0.9875\n",
            "Epoch 55/1024\n",
            "5000/5000 [==============================] - ETA: 0s - loss: 0.1180 - accuracy: 0.9575\n",
            "Epoch 00055: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n",
            "5000/5000 [==============================] - 42s 8ms/step - loss: 0.1180 - accuracy: 0.9575 - val_loss: 0.0427 - val_accuracy: 0.9870\n",
            "Epoch 56/1024\n",
            "5000/5000 [==============================] - 42s 8ms/step - loss: 0.1100 - accuracy: 0.9610 - val_loss: 0.0398 - val_accuracy: 0.9876\n",
            "Epoch 57/1024\n",
            "5000/5000 [==============================] - 38s 8ms/step - loss: 0.1086 - accuracy: 0.9613 - val_loss: 0.0405 - val_accuracy: 0.9886\n",
            "Epoch 58/1024\n",
            "4860/5000 [============================>.] - ETA: 0s - loss: 0.1083 - accuracy: 0.9618"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqItionrS_z0"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySqltk4KTHJ7"
      },
      "source": [
        "*ETAPE 6 :* Le code ci-dessous permet d'utiliser le modèle. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOiht1hPani6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "334cad03-437f-456c-ee63-4a3e21798686"
      },
      "source": [
        "predictor = ktrain.get_predictor(learner.model, preproc)\n",
        "\n",
        "data = [ \"Ce film était horrible ! L'intrigue était ennuyeuse. Le jeu d'acteur était correct, cependant.\",\n",
        "         \"Le film est vraiment nul. Je veux qu'on me rende mon argent.\",\n",
        "        \"Quelle belle comédie romantique. 10/10 à revoir !\"]\n",
        "\n",
        "# Makes predictions for a list of strings where each string is a document or text snippet.\n",
        "print (predictor.predict(data))\n",
        "\n",
        "# If return_proba is True, returns probabilities of each class.\n",
        "print (predictor.predict(data,  return_proba=True))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['negative', 'negative', 'positive']\n",
            "[[9.9996245e-01 3.7598063e-05]\n",
            " [9.9657214e-01 3.4278736e-03]\n",
            " [3.4658754e-01 6.5341240e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5l8TyRWThZF"
      },
      "source": [
        "#### QUESTION : évaluation qualitative légère\n",
        "\n",
        "* Tester le modèle. Arrivez-vous à piéger le modèle ? Avec quelle phrase (donnez le code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkowscJhTiPh"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bA5hf8I1dXul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QUESTION : Evaluation quantitative\n",
        "\n",
        "Prediction sur le corpus de test"
      ],
      "metadata": {
        "id": "QNZjuU8Bc4qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_hyp = predictor.predict(allocine_dataset['test']['review'])"
      ],
      "metadata": {
        "id": "32x1JdOCc5Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "[SemEval 2022 isarcasmeval task](https://sites.google.com/view/semeval2022-isarcasmeval) used the F1-score for the sarcastic class. This metric should not be confused with the regular macro-F1. Here we choose the positive class for illustration"
      ],
      "metadata": {
        "id": "fNPvtEt0PzK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "f1_positive = f1_score(y_test, y_hyp, average = \"binary\", pos_label = 1)\n",
        "\n",
        "#OR\n",
        "#p_score_positive = precision_score(y_test, y_hyp, average = \"binary\", pos_label = 1)\n",
        "#r_score_positive = recall_score(y_test, y_hyp, average = \"binary\", pos_label = 1)\n",
        "#f1_positive = (2*p_score_positive*r_score_positive)/(p_score_positive +r_score_positive)"
      ],
      "metadata": {
        "id": "YI0p4xFFPzch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use regular F1 score"
      ],
      "metadata": {
        "id": "T9A7_Tm1cpPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "allocine_dataset['test']['review']\n",
        "y_test = allocine_dataset['test']['label']\n",
        "f1 = f1_score(y_test, y_hyp)\n",
        "f1"
      ],
      "metadata": {
        "id": "GphE8fJSaogx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Theophile Blard rapporte des performances avec différents modèles (CamemBERT, RNN, TF-IDF + LogReg, CNN, fastText (unigrams)) sur le dataset Allociné](https://github.com/TheophileBlard/french-sentiment-analysis-with-bert). Comment vous positionnez-vous par rapport à ses résultats ?   "
      ],
      "metadata": {
        "id": "jQ1neD23ao8T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApxsjcDHE4Im"
      },
      "source": [
        "## NBSVM\n",
        "\n",
        "Le code ci-dessous utilise le même pré-traitement que précédemment et applique un modèle neuronal plus \"simple\" que fasttext.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGp2mrBsg17I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47af0454-b5d9-4768-e796-5dc66a3e4489"
      },
      "source": [
        "# load an NBSVM model\n",
        "model = text.text_classifier('nbsvm', (x_train, y_train), preproc=preproc)\n",
        "learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_val, y_val))\n",
        "\n",
        "# fine tune\n",
        "LEARNING_RATE = 0.01\n",
        "learner.autofit(LEARNING_RATE)\n",
        "# Epoch 6/1024\n",
        "#4498/4500 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9544Restoring model weights from the end of the best epoch: 1.\n",
        "#4500/4500 [==============================] - 24s 5ms/step - loss: 0.1264 - accuracy: 0.9544 - val_loss: 0.2677 - val_accuracy: 0.9161\n",
        "#Epoch 6: early stopping\n",
        "\n",
        "# Finally, we will fit our model using and [SGDR learning rate](https://github.com/amaiya/ktrain/blob/master/example-02-tuning-learning-rates.ipynb) schedule by invoking the fit method with the cycle_len parameter (along with the cycle_mult parameter).\n",
        "# learner.fit(0.001, 3, cycle_len=1, cycle_mult=2)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Multi-Label? False\n",
            "compiling word ID features...\n",
            "maxlen is 400\n",
            "building document-term matrix... this may take a few moments...\n",
            "rows: 1-10000\n",
            "rows: 10001-20000\n",
            "rows: 20001-30000\n",
            "rows: 30001-40000\n",
            "rows: 40001-50000\n",
            "rows: 50001-60000\n",
            "rows: 60001-70000\n",
            "rows: 70001-80000\n",
            "rows: 80001-90000\n",
            "rows: 90001-100000\n",
            "rows: 100001-110000\n",
            "rows: 110001-120000\n",
            "rows: 120001-130000\n",
            "rows: 130001-140000\n",
            "rows: 140001-144000\n",
            "computing log-count ratios...\n",
            "done.\n",
            "early_stopping automatically enabled at patience=5\n",
            "reduce_on_plateau automatically enabled at patience=2\n",
            "\n",
            "\n",
            "begin training using triangular learning rate policy with max lr of 0.01...\n",
            "Epoch 1/1024\n",
            "4500/4500 [==============================] - 22s 5ms/step - loss: 0.2423 - accuracy: 0.9141 - val_loss: 0.2210 - val_accuracy: 0.9214\n",
            "Epoch 2/1024\n",
            "4500/4500 [==============================] - 21s 5ms/step - loss: 0.1881 - accuracy: 0.9321 - val_loss: 0.2428 - val_accuracy: 0.9193\n",
            "Epoch 3/1024\n",
            "4496/4500 [============================>.] - ETA: 0s - loss: 0.1757 - accuracy: 0.9369\n",
            "Epoch 00003: Reducing Max LR on Plateau: new max lr will be 0.005 (if not early_stopping).\n",
            "4500/4500 [==============================] - 21s 5ms/step - loss: 0.1759 - accuracy: 0.9369 - val_loss: 0.2607 - val_accuracy: 0.9168\n",
            "Epoch 4/1024\n",
            "4500/4500 [==============================] - 21s 5ms/step - loss: 0.1477 - accuracy: 0.9463 - val_loss: 0.2585 - val_accuracy: 0.9169\n",
            "Epoch 5/1024\n",
            "4500/4500 [==============================] - ETA: 0s - loss: 0.1401 - accuracy: 0.9486\n",
            "Epoch 00005: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n",
            "4500/4500 [==============================] - 21s 5ms/step - loss: 0.1401 - accuracy: 0.9486 - val_loss: 0.2680 - val_accuracy: 0.9156\n",
            "Epoch 6/1024\n",
            "4498/4500 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9544Restoring model weights from the end of the best epoch: 1.\n",
            "4500/4500 [==============================] - 24s 5ms/step - loss: 0.1264 - accuracy: 0.9544 - val_loss: 0.2677 - val_accuracy: 0.9161\n",
            "Epoch 6: early stopping\n",
            "Weights from best epoch have been loaded into model.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5c264a4f50>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xveDFwSnUzj1"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "* Le modèle nbsvm est-il plus performant que le précédent sur les données de validations ? Vous pouvez tester aussi différents learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilof7YlQNtfz"
      },
      "source": [
        "## BERT \n",
        "\n",
        "L'usage du modèle bert requiert que l'on change le prétraitement des données en entrée. Le code suivant réalise le prétraitement, charge un modèle bert et lance la personnalisation (fine tuning) sur 1 cycle avec taux d'apprentissage fixé.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kpI2LG_Wvqe"
      },
      "source": [
        "#### QUESTION\n",
        "* Exécutez le code en observant l'occupation de la RAM et attendez jusqu'à voir le temps prévisionnel s'afficher. Etes-vous choqué par le temps affiché ? Bert est très gros. Il requiert un peu de temps...\n",
        "* Stoppez l'exécution dans la cellule, et tentez de changer la taille du batch_size (quantité de données traitées en même temps). Vous pouvez tester 128 et 12 comme valeurs. Ne lachez pas la barre de la RAM des yeux. Ça passe ? Quel problème rencontrez-vous ? \n",
        "* Passez à la suite..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ETAPE 1 \n",
        "(x_train, y_train), (x_val, y_val), preproc = text.texts_from_df (train_df, \n",
        "                      'review',\n",
        "                      label_columns = [\"negative\", \"positive\"],\n",
        "                      val_df= val_df, # if None, 10% of data will be used for validation\n",
        "                      ##max_features=NUM_WORDS, \n",
        "                      #maxlen=MAXLEN,\n",
        "                      preprocess_mode='bert' \n",
        "                      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "iAZQI0ucQWSa",
        "outputId": "fe78b442-2b72-48d6-90af-a25cc1c260db"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['negative', 'positive']\n",
            "        negative  positive\n",
            "3661           0         1\n",
            "72776          1         0\n",
            "96097          0         1\n",
            "129808         1         0\n",
            "106107         1         0\n",
            "['negative', 'positive']\n",
            "        negative  positive\n",
            "131304         1         0\n",
            "1761           1         0\n",
            "15952          1         0\n",
            "117463         1         0\n",
            "43800          0         1\n",
            "downloading pretrained BERT model (multi_cased_L-12_H-768_A-12.zip)...\n",
            "[██████████████████████████████████████████████████]\n",
            "extracting pretrained BERT model...\n",
            "done.\n",
            "\n",
            "cleanup downloaded zip...\n",
            "done.\n",
            "\n",
            "preprocessing train...\n",
            "language: fr\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "done."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Multi-Label? False\n",
            "preprocessing test...\n",
            "language: fr\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "done."
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0ujB3xihhnG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "e4586cab-9f7b-435d-91e3-39b799c3866e"
      },
      "source": [
        "# ETAPE 2 et 3\n",
        "model = text.text_classifier('bert', (x_train, y_train) , preproc=preproc)\n",
        "learner = ktrain.get_learner(model, \n",
        "                             train_data=(x_train, y_train), \n",
        "                             val_data=(x_val, y_val), \n",
        "                             batch_size=6)\n",
        "# ETAPE 5\n",
        "learner.fit_onecycle(2e-5, 1)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Multi-Label? False\n",
            "maxlen is 400\n",
            "done.\n",
            "\n",
            "\n",
            "begin training using onecycle policy with max lr of 2e-05...\n",
            "  318/24000 [..............................] - ETA: 4:11:52 - loss: 0.6092 - accuracy: 0.6667"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-3ae65c458532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                              batch_size=6)\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# ETAPE 5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_onecycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mfit_onecycle\u001b[0;34m(self, lr, epochs, checkpoint_folder, cycle_momentum, max_momentum, min_momentum, class_weight, callbacks, steps_per_epoch, verbose)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m         )\n\u001b[1;32m   1047\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lr, n_cycles, cycle_len, cycle_mult, lr_decay, checkpoint_folder, early_stopping, verbose, class_weight, callbacks, steps_per_epoch)\u001b[0m\n\u001b[1;32m   1402\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m                 \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1404\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1405\u001b[0m             )\n\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0r6CivJbBUF"
      },
      "source": [
        "## Modèle \"à la bert\" issu de hugging face\n",
        "\n",
        "On va tester un modèle encore plus léger 'distilbert-base-uncased'. Exécutez toutes les cellules de code ci-dessous. Laissez tourner et répondez aux questions.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = allocine_dataset['train']['review']\n",
        "y_train = allocine_dataset['train']['label']\n",
        "x_val = allocine_dataset['validation']['review']\n",
        "y_val = allocine_dataset['validation']['label']\n",
        "x_train = allocine_dataset['train']['review']\n",
        "y_train = allocine_dataset['train']['label']\n",
        "x_test = allocine_dataset['test']['review']\n",
        "y_test = allocine_dataset['test']['label']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5QVq5wITxKT",
        "outputId": "0cf55385-b6ae-428b-afc1-a61c00767cd1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[  101,   178,   112, ...,     0,     0,     0],\n",
              "        [  101, 12653, 28341, ...,     0,     0,     0],\n",
              "        [  101, 11681, 58817, ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [  101,   192, 33646, ...,     0,     0,     0],\n",
              "        [  101, 10231, 15975, ...,     0,     0,     0],\n",
              "        [  101, 10322, 11639, ...,     0,     0,     0]]),\n",
              " array([[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]])]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52DqPKJE9d4s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448,
          "referenced_widgets": [
            "a7095815c1344076b60a3e197f0e533a",
            "5932c09cd28b42d19f7636c4bc01f1a6",
            "8453ffbe939547cdbf74e6b3b61f7deb",
            "03398e6d84994a77928b5bf251178af9",
            "fc78363ade6b4bf48dc2cebf3adaaede",
            "0ff392dd52414845b1edae142a0db14e",
            "076ffce5626e4d9c9adb972737fb34a3",
            "15463c7e439243e7a49f2ebc62916fb7",
            "e8a7aeea07b64025aaf0118105c0abd4",
            "161ed7e5575f41999c2aa576473364a4",
            "6ea2ac6037044d6aadd86813d1b82b7b"
          ]
        },
        "outputId": "4f92c0b0-7064-41a5-b401-2a432044bbef"
      },
      "source": [
        "import ktrain\n",
        "from ktrain import text\n",
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "#MODEL_NAME = 'albert-base-v2'\n",
        "CLASS_NAMES = [\"negative\", \"positive\"]\n",
        "\n",
        "t = text.Transformer(MODEL_NAME, maxlen=500, class_names=CLASS_NAMES)\n",
        "trn = t.preprocess_train(x_train, y_train)\n",
        "val = t.preprocess_test(x_val, y_val)\n",
        "#print (type(trn))\n",
        "#x_train, y_train = trn\n",
        "#x_test, y_test = val  \n",
        "model = t.get_classifier()\n",
        "# batch_size (int):              Batch size to use in training. default:32  \n",
        "#learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6) # 128 dépend de la ram dispo 13 Go par défaut\n",
        "learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=12)\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7095815c1344076b60a3e197f0e533a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocessing train...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-41eb4e5432a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASS_NAMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#print (type(trn))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ktrain/text/preprocessor.py\u001b[0m in \u001b[0;36mpreprocess_train\u001b[0;34m(self, texts, y, mode, verbose)\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m         \u001b[0mtseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_with_learner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ktrain/text/preprocessor.py\u001b[0m in \u001b[0;36mpreprocess_train\u001b[0;34m(self, texts, y, mode, verbose)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0;31m# detect sentence pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0mis_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_text_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_array\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ktrain/text/preprocessor.py\u001b[0m in \u001b[0;36mdetect_text_format\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mis_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_is_sentence_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_pair\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mis_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid text format: texts should be list of strings or list of sentence pairs in form of tuples (str, str)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqM1-j84-4eD"
      },
      "source": [
        "learner.fit_onecycle(0.01, 1)\n",
        "#learner.fit_onecycle(8e-5, 4)\n",
        "#8e-5 = 8 + 10^(-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeM-jNIyb_7M"
      },
      "source": [
        "#### QUESTION\n",
        "* Quelle performance j'obtiens comparativement aux 2 modèles précédents nbsvm et fasttext ?\n",
        "* Quelles risques je prends à ne pas réaliser les mêmes prétraitements ou  normalisations (voire utiliser des outils différents pour réaliser les mêmes prétraitements ou normalisations supposées) que ceux réalisés sur les corpus ayant servis à construire les modèles ? \n",
        "* En résumé, en mettant dans la balance les questions de performance, les questions de taille de modèles, de temps de \"fine-tuning\"... quelles conclusions faites-vous de l'usage des modèles simples vs les modèles plus complexes à la BERT ?\n",
        "\n",
        "Si le code tourne toujours passez à la question suivante pour gagner du temps..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qni8ELuBcBB1"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Devoir à rendre \n",
        "[Theophile Blard rapporte des performances avec différents modèles (CamemBERT, RNN, TF-IDF + LogReg, CNN, fastText (unigrams)) sur le dataset Allociné](https://github.com/TheophileBlard/french-sentiment-analysis-with-bert). \n",
        "\n",
        "Arriverez-vous à faire mieux ?\n",
        "\n",
        "Bien entendu vous avez le droit de choisir les modèles et leur configuration.\n",
        "\n",
        "Explorez au choix un ou plusieurs des thèmes suivants\n",
        "- data augmentation\n",
        "- model ensembling \n",
        "\n",
        "Ecrivez un rapport d'expériences.\n",
        "\n",
        "## Topics\n",
        "\n",
        "### Data augmentation in NLP\n",
        "TextAttack is a Python framework for adversarial attacks, adversarial training, and [data augmentation in NLP](https://textattack.readthedocs.io/en/latest/2notebook/3_Augmentations.html).\n",
        "\n",
        "Here some hints for [multi-language attacks](https://textattack.readthedocs.io/en/latest/2notebook/Example_4_CamemBERT.html)\n",
        "\n",
        "### Model ensembling \n",
        "\n",
        "- Train/fine-tune multiple models from various architectures and/or dataset folds\n",
        "- Then ensemble them by voting mechanism or something else\n",
        "\n",
        "## VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ],
      "metadata": {
        "id": "VzOyw5WCCJX4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dU6olWGuGzN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tzxoRA_RGl4"
      },
      "source": [
        "# Références\n",
        "* Text Classification Example: Sentiment Analysis with IMDb Movie Reviews¶ https://nbviewer.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-04-text-classification.ipynb\n",
        "* https://nbviewer.org/github/amaiya/ktrain/blob/master/examples/text/IMDb-BERT.ipynb\n",
        "* ktrain examples of Binary text Classification (Sentiment Analysis with IMDb Movie Reviews) with a nbsvm model (also a bit of bert)  and \n",
        "Multi-Label Text Classification (toxic comments) with fasttext https://nbviewer.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-04-text-classification.ipynb\n",
        "* Text Classification with Hugging Face Transformers in ktrain https://github.com/amaiya/ktrain/blob/master/tutorials/tutorial-A3-hugging_face_transformers.ipynb\n",
        "* ktrain api documentation https://amaiya.github.io/ktrain/\n",
        "* Evaluation of various models (CamemBERT, RNN, TF-IDF + LogReg, CNN, fastText (unigrams)) on allocine dataset https://github.com/TheophileBlard/french-sentiment-analysis-with-bert\n",
        "* The Allociné dataset is a French-language dataset for sentiment analysis. The texts are movie reviews written between 2006 and 2020 by members of the Allociné.fr community for various films. It contains 100k positive and 100k negative reviews divided into train (160k), validation (20k), and test (20k). \n",
        "https://huggingface.co/datasets/allocine\n",
        "* Amazon reviews for three product categories: books, DVD, and music. Each sample contains a review text and the associated rating from 1 to 5 stars. Reviews rated above 3 is labeled as positive, and those rated less than 3 is labeled as negative. https://github.com/getalp/Flaubert/tree/master/flue\n",
        "* Twitter API between May and September 2018. The sentiment was generated thanks to AWS Comprehend API. For Spanish and French, tweets were first translated to English using Google Translate, and then analyzed with AWS Comprehend. https://github.com/charlesmalafosse/open-dataset-for-sentiment-analysis/\n",
        "* French dataset for sentiment analysis (Data translated from English to French).  A collection of over 1.5 Million tweets data translated to French, with their sentiment. The data has two columns, polarity and status https://github.com/gamebusterz/French-Sentiment-Analysis-Dataset\n",
        "* English Classification datasets https://ludwig.ai/latest/examples/text_classification/"
      ]
    }
  ]
}