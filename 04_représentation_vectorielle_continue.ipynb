{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMj9NqwjcoG+oVdEkYAXRhe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolashernandez/teaching_nlp/blob/main/04_repr%C3%A9sentation_vectorielle_continue.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKxD0JXTjZvM"
      },
      "source": [
        "# Représentation vectorielle continue \n",
        "\n",
        "Les [**plongements de mots** (_word embeddings_ en anglais)](https://fr.wikipedia.org/wiki/Word_embedding) désignent le résultat de techniques récentes de vectorisation qui produisent des vecteurs denses de dimensions réduites, prédéfinies, et non corrélées avec la taille du vocabulaire (e.g. 100, 300, 500...). \n",
        "Ces techniques reposent sur l'hypothèse distributionnelle de Harris qui veut que les mots apparaissant dans des contextes similaires ont des significations apparentées. \n",
        "\n",
        "La méthode de référence est connue sous le nom [**Word2Vec** est attribuée à Mikolov et ses collègues (Google) en 2013](https://github.com/tmikolov/word2vec).\n",
        "> Tomas Mikolov, Kai Chen, Greg Corrado et Jeffrey Dean, « Efficient Estimation of Word Representations in Vector Space », arXiv:1301.3781 [cs],‎ 16 janvier 2013\n",
        "\n",
        "Les auteurs proposent deux architectures neuronales à 2 couches [CBOW (_continuous bag of words_) et SkipGram](https://fr.wikipedia.org/wiki/Word2vec#/media/Fichier:CBOW_eta_Skipgram.png).\n",
        "\n",
        "> Le CBOW vise à prédire un mot étant donné son contexte. Skip-gram a une architecture symétrique visant à prédire les mots du contexte étant donné un mot en entrée. En pratique, le modèle CBOW est plus rapide à apprendre, mais le modèle skip-gram donne généralement de meilleurs résultats.\n",
        "\n",
        "> La couche cachée contient quelques centraines de neurones et constitue à l'issue de l'entraînement le plongement représentant un mot. La couche de sortie permet d'implémenter une tâche de classification au moyen d'une softmax.\n",
        "\n",
        "> L'apprentissage ne nécessite néanmoins aucun label, la vérité terrain étant directement déduite des données et plus particulièrement de la proximité des mots au sein du corpus d'entraînement. En ce sens, l'apprentissage de Word2vec constitue un apprentissage auto-supervisé. \n",
        "\n",
        "Les vecteurs obtenus sont dits statiques (ou non contextuels) car ils restent tel quel quelle que soit l'occurrence du mot en contexte.\n",
        "\n",
        "Un second modèle bien connu est celui de [**FastText** (Facebook)](https://github.com/facebookresearch/fastText) qui propose de traiter la variabilité morphologique des mots en construisant des vecteurs non pas pour des mots mais pour des sous-mots (séquence de caractères). Le lecteur d'un mot est la somme de tous les vecteurs des sous-mots le composant. \n",
        "\n",
        "Cette approche est indépendante de la langue, et montre de meilleurs résultats que word2vec sur des tâches syntaxiques, surtout quand le corpus d'entraînement est petit. Word2vec est légèrement meilleur pour des tâches sémantiques. Un des avantage de FastText est de pouvoir fournir des vecteurs mêmes pour les mots hors vocabulaires.\n",
        "\n",
        "Plusieurs **librairies permettent de créer, charger, sauver et manipuler des modèles de plongements de mots**. Nous allons utiliser _gensim_ qui permet aussi bien de travailler avec des modèles [word2vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py) que [fasttext](https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py).  \n",
        "\n",
        "Ici une [Comparison of FastText and Word2Vec](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1UAG8aqs52c"
      },
      "source": [
        "## Utiliser un modèle word2vec existant avec gensim \n",
        "\n",
        "Le dépôt [gensim-data](https://github.com/RaRe-Technologies/gensim-data) contient quelques corpus et modèles pré-entraînés librement disponibles. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niTFE-e0q6xz"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "* Exécutez le code ci-dessous ou consulter le lien gensim-data et indiquez le nom d'un modèle construit sur la base de tweets si cela existe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YToqA7TYmZWz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ac47e48-b865-411e-b11b-791b84f479e8"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "api.info()  # show info about available models/datasets\n",
        "# model = api.load(\"glove-twitter-25\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'corpora': {'semeval-2016-2017-task3-subtaskBC': {'num_records': -1,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 6344358,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskB-eng/__init__.py',\n",
              "   'license': 'All files released for the task are free for general research use',\n",
              "   'fields': {'2016-train': ['...'],\n",
              "    '2016-dev': ['...'],\n",
              "    '2017-test': ['...'],\n",
              "    '2016-test': ['...']},\n",
              "   'description': 'SemEval 2016 / 2017 Task 3 Subtask B and C datasets contain train+development (317 original questions, 3,169 related questions, and 31,690 comments), and test datasets in English. The description of the tasks and the collected data is given in sections 3 and 4.1 of the task paper http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf linked in section “Papers” of https://github.com/RaRe-Technologies/gensim-data/issues/18.',\n",
              "   'checksum': '701ea67acd82e75f95e1d8e62fb0ad29',\n",
              "   'file_name': 'semeval-2016-2017-task3-subtaskBC.gz',\n",
              "   'read_more': ['http://alt.qcri.org/semeval2017/task3/',\n",
              "    'http://alt.qcri.org/semeval2017/task3/data/uploads/semeval2017-task3.pdf',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
              "    'https://github.com/Witiko/semeval-2016_2017-task3-subtaskB-english'],\n",
              "   'parts': 1},\n",
              "  'semeval-2016-2017-task3-subtaskA-unannotated': {'num_records': 189941,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 234373151,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskA-unannotated-eng/__init__.py',\n",
              "   'license': 'These datasets are free for general research use.',\n",
              "   'fields': {'THREAD_SEQUENCE': '',\n",
              "    'RelQuestion': {'RELQ_CATEGORY': 'question category, according to the Qatar Living taxonomy',\n",
              "     'RELQ_DATE': 'date of posting',\n",
              "     'RELQ_ID': 'question indentifier',\n",
              "     'RELQ_USERID': 'identifier of the user asking the question',\n",
              "     'RELQ_USERNAME': 'name of the user asking the question',\n",
              "     'RelQBody': 'body of question',\n",
              "     'RelQSubject': 'subject of question'},\n",
              "    'RelComments': [{'RelCText': 'text of answer',\n",
              "      'RELC_USERID': 'identifier of the user posting the comment',\n",
              "      'RELC_ID': 'comment identifier',\n",
              "      'RELC_USERNAME': 'name of the user posting the comment',\n",
              "      'RELC_DATE': 'date of posting'}]},\n",
              "   'description': 'SemEval 2016 / 2017 Task 3 Subtask A unannotated dataset contains 189,941 questions and 1,894,456 comments in English collected from the Community Question Answering (CQA) web forum of Qatar Living. These can be used as a corpus for language modelling.',\n",
              "   'checksum': '2de0e2f2c4f91c66ae4fcf58d50ba816',\n",
              "   'file_name': 'semeval-2016-2017-task3-subtaskA-unannotated.gz',\n",
              "   'read_more': ['http://alt.qcri.org/semeval2016/task3/',\n",
              "    'http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
              "    'https://github.com/Witiko/semeval-2016_2017-task3-subtaskA-unannotated-english'],\n",
              "   'parts': 1},\n",
              "  'patent-2017': {'num_records': 353197,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 3087262469,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/patent-2017/__init__.py',\n",
              "   'license': 'not found',\n",
              "   'description': \"Patent Grant Full Text. Contains the full text including tables, sequence data and 'in-line' mathematical expressions of each patent grant issued in 2017.\",\n",
              "   'checksum-0': '818501f0b9af62d3b88294d86d509f8f',\n",
              "   'checksum-1': '66c05635c1d3c7a19b4a335829d09ffa',\n",
              "   'file_name': 'patent-2017.gz',\n",
              "   'read_more': ['http://patents.reedtech.com/pgrbft.php'],\n",
              "   'parts': 2},\n",
              "  'quora-duplicate-questions': {'num_records': 404290,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 21684784,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/quora-duplicate-questions/__init__.py',\n",
              "   'license': 'probably https://www.quora.com/about/tos',\n",
              "   'fields': {'question1': 'the full text of each question',\n",
              "    'question2': 'the full text of each question',\n",
              "    'qid1': 'unique ids of each question',\n",
              "    'qid2': 'unique ids of each question',\n",
              "    'id': 'the id of a training set question pair',\n",
              "    'is_duplicate': 'the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise'},\n",
              "   'description': 'Over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line contains a duplicate pair or not.',\n",
              "   'checksum': 'd7cfa7fbc6e2ec71ab74c495586c6365',\n",
              "   'file_name': 'quora-duplicate-questions.gz',\n",
              "   'read_more': ['https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs'],\n",
              "   'parts': 1},\n",
              "  'wiki-english-20171001': {'num_records': 4924894,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 6516051717,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/wiki-english-20171001/__init__.py',\n",
              "   'license': 'https://dumps.wikimedia.org/legal.html',\n",
              "   'fields': {'section_texts': 'list of body of sections',\n",
              "    'section_titles': 'list of titles of sections',\n",
              "    'title': 'Title of wiki article'},\n",
              "   'description': 'Extracted Wikipedia dump from October 2017. Produced by `python -m gensim.scripts.segment_wiki -f enwiki-20171001-pages-articles.xml.bz2 -o wiki-en.gz`',\n",
              "   'checksum-0': 'a7d7d7fd41ea7e2d7fa32ec1bb640d71',\n",
              "   'checksum-1': 'b2683e3356ffbca3b6c2dca6e9801f9f',\n",
              "   'checksum-2': 'c5cde2a9ae77b3c4ebce804f6df542c2',\n",
              "   'checksum-3': '00b71144ed5e3aeeb885de84f7452b81',\n",
              "   'file_name': 'wiki-english-20171001.gz',\n",
              "   'read_more': ['https://dumps.wikimedia.org/enwiki/20171001/'],\n",
              "   'parts': 4},\n",
              "  'text8': {'num_records': 1701,\n",
              "   'record_format': 'list of str (tokens)',\n",
              "   'file_size': 33182058,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
              "   'license': 'not found',\n",
              "   'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
              "   'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
              "   'file_name': 'text8.gz',\n",
              "   'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
              "   'parts': 1},\n",
              "  'fake-news': {'num_records': 12999,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 20102776,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py',\n",
              "   'license': 'https://creativecommons.org/publicdomain/zero/1.0/',\n",
              "   'fields': {'crawled': 'date the story was archived',\n",
              "    'ord_in_thread': '',\n",
              "    'published': 'date published',\n",
              "    'participants_count': 'number of participants',\n",
              "    'shares': 'number of Facebook shares',\n",
              "    'replies_count': 'number of replies',\n",
              "    'main_img_url': 'image from story',\n",
              "    'spam_score': 'data from webhose.io',\n",
              "    'uuid': 'unique identifier',\n",
              "    'language': 'data from webhose.io',\n",
              "    'title': 'title of story',\n",
              "    'country': 'data from webhose.io',\n",
              "    'domain_rank': 'data from webhose.io',\n",
              "    'author': 'author of story',\n",
              "    'comments': 'number of Facebook comments',\n",
              "    'site_url': 'site URL from BS detector',\n",
              "    'text': 'text of story',\n",
              "    'thread_title': '',\n",
              "    'type': 'type of website (label from BS detector)',\n",
              "    'likes': 'number of Facebook likes'},\n",
              "   'description': \"News dataset, contains text and metadata from 244 websites and represents 12,999 posts in total from a specific window of 30 days. The data was pulled using the webhose.io API, and because it's coming from their crawler, not all websites identified by their BS Detector are present in this dataset. Data sources that were missing a label were simply assigned a label of 'bs'. There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.\",\n",
              "   'checksum': '5e64e942df13219465927f92dcefd5fe',\n",
              "   'file_name': 'fake-news.gz',\n",
              "   'read_more': ['https://www.kaggle.com/mrisdal/fake-news'],\n",
              "   'parts': 1},\n",
              "  '20-newsgroups': {'num_records': 18846,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 14483581,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/__init__.py',\n",
              "   'license': 'not found',\n",
              "   'fields': {'topic': 'name of topic (20 variant of possible values)',\n",
              "    'set': \"marker of original split (possible values 'train' and 'test')\",\n",
              "    'data': '',\n",
              "    'id': 'original id inferred from folder name'},\n",
              "   'description': 'The notorious collection of approximately 20,000 newsgroup posts, partitioned (nearly) evenly across 20 different newsgroups.',\n",
              "   'checksum': 'c92fd4f6640a86d5ba89eaad818a9891',\n",
              "   'file_name': '20-newsgroups.gz',\n",
              "   'read_more': ['http://qwone.com/~jason/20Newsgroups/'],\n",
              "   'parts': 1},\n",
              "  '__testing_matrix-synopsis': {'description': '[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.',\n",
              "   'checksum': '1767ac93a089b43899d54944b07d9dc5',\n",
              "   'file_name': '__testing_matrix-synopsis.gz',\n",
              "   'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis'],\n",
              "   'parts': 1},\n",
              "  '__testing_multipart-matrix-synopsis': {'description': '[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.',\n",
              "   'checksum-0': 'c8b0c7d8cf562b1b632c262a173ac338',\n",
              "   'checksum-1': '5ff7fc6818e9a5d9bc1cf12c35ed8b96',\n",
              "   'checksum-2': '966db9d274d125beaac7987202076cba',\n",
              "   'file_name': '__testing_multipart-matrix-synopsis.gz',\n",
              "   'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis'],\n",
              "   'parts': 3}},\n",
              " 'models': {'fasttext-wiki-news-subwords-300': {'num_records': 999999,\n",
              "   'file_size': 1005007116,\n",
              "   'base_dataset': 'Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py',\n",
              "   'license': 'https://creativecommons.org/licenses/by-sa/3.0/',\n",
              "   'parameters': {'dimension': 300},\n",
              "   'description': '1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).',\n",
              "   'read_more': ['https://fasttext.cc/docs/en/english-vectors.html',\n",
              "    'https://arxiv.org/abs/1712.09405',\n",
              "    'https://arxiv.org/abs/1607.01759'],\n",
              "   'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af',\n",
              "   'file_name': 'fasttext-wiki-news-subwords-300.gz',\n",
              "   'parts': 1},\n",
              "  'conceptnet-numberbatch-17-06-300': {'num_records': 1917247,\n",
              "   'file_size': 1225497562,\n",
              "   'base_dataset': 'ConceptNet, word2vec, GloVe, and OpenSubtitles 2016',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py',\n",
              "   'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt',\n",
              "   'parameters': {'dimension': 300},\n",
              "   'description': 'ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.',\n",
              "   'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972',\n",
              "    'https://github.com/commonsense/conceptnet-numberbatch',\n",
              "    'http://conceptnet.io/'],\n",
              "   'checksum': 'fd642d457adcd0ea94da0cd21b150847',\n",
              "   'file_name': 'conceptnet-numberbatch-17-06-300.gz',\n",
              "   'parts': 1},\n",
              "  'word2vec-ruscorpora-300': {'num_records': 184973,\n",
              "   'file_size': 208427381,\n",
              "   'base_dataset': 'Russian National Corpus (about 250M words)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py',\n",
              "   'license': 'https://creativecommons.org/licenses/by/4.0/deed.en',\n",
              "   'parameters': {'dimension': 300, 'window_size': 10},\n",
              "   'description': 'Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.',\n",
              "   'preprocessing': 'The corpus was lemmatized and tagged with Universal PoS',\n",
              "   'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models',\n",
              "    'http://rusvectores.org/en/',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/3'],\n",
              "   'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4',\n",
              "   'file_name': 'word2vec-ruscorpora-300.gz',\n",
              "   'parts': 1},\n",
              "  'word2vec-google-news-300': {'num_records': 3000000,\n",
              "   'file_size': 1743563840,\n",
              "   'base_dataset': 'Google News (about 100 billion words)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py',\n",
              "   'license': 'not found',\n",
              "   'parameters': {'dimension': 300},\n",
              "   'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
              "   'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
              "    'https://arxiv.org/abs/1301.3781',\n",
              "    'https://arxiv.org/abs/1310.4546',\n",
              "    'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
              "   'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
              "   'file_name': 'word2vec-google-news-300.gz',\n",
              "   'parts': 1},\n",
              "  'glove-wiki-gigaword-50': {'num_records': 400000,\n",
              "   'file_size': 69182535,\n",
              "   'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 50},\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
              "   'file_name': 'glove-wiki-gigaword-50.gz',\n",
              "   'parts': 1},\n",
              "  'glove-wiki-gigaword-100': {'num_records': 400000,\n",
              "   'file_size': 134300434,\n",
              "   'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 100},\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': '40ec481866001177b8cd4cb0df92924f',\n",
              "   'file_name': 'glove-wiki-gigaword-100.gz',\n",
              "   'parts': 1},\n",
              "  'glove-wiki-gigaword-200': {'num_records': 400000,\n",
              "   'file_size': 264336934,\n",
              "   'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 200},\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': '59652db361b7a87ee73834a6c391dfc1',\n",
              "   'file_name': 'glove-wiki-gigaword-200.gz',\n",
              "   'parts': 1},\n",
              "  'glove-wiki-gigaword-300': {'num_records': 400000,\n",
              "   'file_size': 394362229,\n",
              "   'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 300},\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': '29e9329ac2241937d55b852e8284e89b',\n",
              "   'file_name': 'glove-wiki-gigaword-300.gz',\n",
              "   'parts': 1},\n",
              "  'glove-twitter-25': {'num_records': 1193514,\n",
              "   'file_size': 109885004,\n",
              "   'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 25},\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': '50db0211d7e7a2dcd362c6b774762793',\n",
              "   'file_name': 'glove-twitter-25.gz',\n",
              "   'parts': 1},\n",
              "  'glove-twitter-50': {'num_records': 1193514,\n",
              "   'file_size': 209216938,\n",
              "   'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 50},\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': 'c168f18641f8c8a00fe30984c4799b2b',\n",
              "   'file_name': 'glove-twitter-50.gz',\n",
              "   'parts': 1},\n",
              "  'glove-twitter-100': {'num_records': 1193514,\n",
              "   'file_size': 405932991,\n",
              "   'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 100},\n",
              "   'description': 'Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': 'b04f7bed38756d64cf55b58ce7e97b15',\n",
              "   'file_name': 'glove-twitter-100.gz',\n",
              "   'parts': 1},\n",
              "  'glove-twitter-200': {'num_records': 1193514,\n",
              "   'file_size': 795373100,\n",
              "   'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 200},\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': 'e52e8392d1860b95d5308a525817d8f9',\n",
              "   'file_name': 'glove-twitter-200.gz',\n",
              "   'parts': 1},\n",
              "  '__testing_word2vec-matrix-synopsis': {'description': '[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.',\n",
              "   'parameters': {'dimensions': 50},\n",
              "   'preprocessing': 'Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.',\n",
              "   'read_more': [],\n",
              "   'checksum': '534dcb8b56a360977a269b7bfc62d124',\n",
              "   'file_name': '__testing_word2vec-matrix-synopsis.gz',\n",
              "   'parts': 1}}}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR0U0UhfnCiC"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehusd1BGmaVk"
      },
      "source": [
        "Nous allons testé un modèle construit sur le **français**. Il ne fait pas partie de ceux de la gensim-data mais il fait parti des [modèles mis à disposition par Jean-Philippe Fauconnier](https://fauconnier.github.io/#data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Urd1Uc6ohlx"
      },
      "source": [
        "### Récupération et chargement d'un modèle pré-entraîné"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0DeEOGwrsRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84cd0039-6de8-4958-a76c-fa9e00bea2c0"
      },
      "source": [
        "# 2021\n",
        "#!wget -nc https://s3.us-east-2.amazonaws.com/embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin -P model\n",
        "#!wget -nc https://s3.us-east-2.amazonaws.com/embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_500_skip_cut100.bin -P model\n",
        "# 2022\n",
        "#!wget -nc https://embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_500_cbow_cut100.bin -P model\n",
        "#!wget -nc https://embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_500_skip_cut100.bin -P model\n",
        "!wget -nc https://embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin -P model\n",
        "!wget -nc https://embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_200_skip_cut100.bin -P model"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-19 20:48:15--  https://embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin\n",
            "Resolving embeddings.net (embeddings.net)... 212.107.17.115, 2a02:4780:8:832:0:2384:2470:1\n",
            "Connecting to embeddings.net (embeddings.net)|212.107.17.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 126052447 (120M) [application/octet-stream]\n",
            "Saving to: ‘model/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin’\n",
            "\n",
            "frWac_non_lem_no_po 100%[===================>] 120.21M  1.25MB/s    in 96s     \n",
            "\n",
            "2022-09-19 20:49:52 (1.26 MB/s) - ‘model/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin’ saved [126052447/126052447]\n",
            "\n",
            "File ‘model/frWac_non_lem_no_postag_no_phrase_200_skip_cut100.bin’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFyhPoUIsQk5"
      },
      "source": [
        "# Chargement du modèle\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "#w2v_model_path =\"model/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin\"\n",
        "w2v_model_path = \"model/frWac_non_lem_no_postag_no_phrase_500_skip_cut100.bin\"\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(w2v_model_path, binary=True, unicode_errors=\"ignore\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNTxexplsc0p"
      },
      "source": [
        "### Obtenir les mots similaires \n",
        "\n",
        "Pour chaque question ci-dessous, jouez le jeu et prenez le temps faire des propositions de réponses avant d'exécuter le code qui permettra de consulter la connaissance du modèle et connaître ce qu'il répondrait."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NneaJ99LwgB0"
      },
      "source": [
        "Si je vous dis 'roi', vous pensez à quoi ? Faire quelques propositions de synonymes ou de mots substituables sémantiquement proches. La méthode `most_similar` affichera les 10 mots les plus proches d'un mot donné, du plus similaire au moins similaire, avec pour chacun un score de similarité avec le mot donné (scores décroissants donc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTQ2GmzhsdaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "714776ed-eda6-47f1-e85a-926560b4ed6e"
      },
      "source": [
        "# obtenir les mots similaires à 1 mot\n",
        "model.most_similar(\"roi\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('trône', 0.743701159954071),\n",
              " ('régente', 0.705065906047821),\n",
              " ('duc', 0.703737735748291),\n",
              " ('chambellan', 0.6829988956451416),\n",
              " ('prince', 0.6821834444999695),\n",
              " ('reine', 0.6814997792243958),\n",
              " ('rois', 0.6601805686950684),\n",
              " ('connétable', 0.6411278247833252),\n",
              " ('souverain', 0.6409915685653687),\n",
              " ('charles-quint', 0.6395503282546997)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Kz9RfpxT3R"
      },
      "source": [
        "Si je vous demande de me donner des mots relatifs à des 'palais' et 'paris', à quoi pensez-vous ? Pour information, la méthode accepte une liste de mots en paramètres. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uxDAKRdtUQ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f5251e-277d-4efa-8fd0-e469b59c4798"
      },
      "source": [
        "# obtenir les mots similaires relatifs à une liste\n",
        "model.most_similar(['palais', 'paris'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('palais-royal', 0.5767269730567932),\n",
              " ('champs-elysées', 0.5534590482711792),\n",
              " ('trocadéro', 0.5271068215370178),\n",
              " ('carrousel', 0.5267265439033508),\n",
              " ('rivoli', 0.517971396446228),\n",
              " ('tuileries', 0.5163304805755615),\n",
              " ('m°', 0.5153165459632874),\n",
              " ('bastille', 0.5104326009750366),\n",
              " ('chaillot', 0.5056830048561096),\n",
              " ('vaugirard', 0.500359833240509)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfifn3_1vhCf"
      },
      "source": [
        "Si j'ajoute les vecteurs de roi et de femme et que je retire le vecteur homme qu'est ce que j'obtiens ? Répondez avant d'exécuter le code ci-dessous.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDV5EriBtOAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e79331d9-ca25-4da1-8ff2-6310d09530cd"
      },
      "source": [
        "# Si j'ajoute les vecteurs de roi et de femme et que je retire le vecteur homme qu'est ce que j'obtiens ?\n",
        "model.most_similar(positive = ['roi', 'femme'], negative = ['homme'])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('désespoir', 0.7093222737312317),\n",
              " ('bonheur', 0.7016983032226562),\n",
              " ('allégresse', 0.7003583908081055),\n",
              " ('larmes', 0.6927555799484253),\n",
              " ('amour', 0.6797648668289185),\n",
              " ('tendresse', 0.6784462928771973),\n",
              " ('émotion', 0.6661418676376343),\n",
              " ('gaieté', 0.6592296361923218),\n",
              " ('chagrin', 0.6360461711883545),\n",
              " ('compassion', 0.6322124600410461)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1S5RZkrv1Sh"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "* Même question mais si j'ajoute les vecteurs de 'paris' et de 'japon' et que je retire le vecteur de 'france'. Faites une proposition et écrivez le code pour vérifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAv_BML1Ji-w"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3tDW4NMwG5b"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGnMqJr0w23I"
      },
      "source": [
        "#### QUESTION\n",
        "* Que pensez-vous de la \"puissance\" de ce type de modèle ? Vos propositions se trouvaient-elles parmis les suggestions du modèle ?\n",
        "* Reprenez ces cas d'usage et faites quelques expériences pour essayer de piéger le modèle. Quelles erreurs ou limites trouvez-vous ? Donnez les codes qui vous conduisent à vos conclusions.\n",
        "\n",
        "Au passage, si vous êtes \"joueurs\" de jeux de société, je vous recommande [Just one word](https://www.letempledujeu.fr/zoom61293.html). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UkU7YsaxLbT"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urxNEHJUbzkc"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY9Io_UjzESH"
      },
      "source": [
        "### Visualiser les plongements lexicaux dans un graph en 2D\n",
        "\n",
        "Pour ce faire, il faut transformer les vecteurs de n dimensions à des vecteurs à 2 dimensions. La réduction des dimensions est effectuée à l'aide d'une [analyse en composantes principales (ACP ou PCA pour _Principal Component Analysis_ en anglais)](https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KogxHwaszDVm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "5e2d7173-9805-4732-adab-8409d597b0e4"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#plt.style.use('ggplot')\n",
        "#words = sum([[k] + v for k, v in similar_words.items()], [])\n",
        "#words = ['roi', 'reine']\n",
        "# soit une liste de mots à projeter\n",
        "words = ['palais', 'église', 'cathédrale', 'monastère', 'route', 'train', 'bateau', 'calèche', 'voiture', 'armée', 'soldat', 'bataille']\n",
        "wvs = model[words]\n",
        "\n",
        "# Application de la transformation PCA qui réduit les vecteurs à 2 dimensions\n",
        "pca = PCA(n_components=2)\n",
        "#np.set_printoptions(suppress=True)\n",
        "P = pca.fit_transform(wvs)\n",
        "labels = words\n",
        "\n",
        "# Affichage \n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "plt.scatter(P[:, 0], P[:, 1], c='lightgreen', edgecolors='g')\n",
        "for label, x, y in zip(labels, P[:, 0], P[:, 1]):\n",
        "    plt.annotate(label, xy=(x*1.05, y*1.05), xytext=(0, 0), textcoords='offset points')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-K7O0aq0U_4"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "* Est-ce que les synonymes se retrouvent bien dans les mêmes zones spatiales ? Vous pouvez tester avec d'autres listes de mots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "507NKhVH0Taq"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzUeK8RlV4Ud"
      },
      "source": [
        "### Visualiser les plongements lexicaux en 3D dynamique à l'aide du _projector de tensorflow_\n",
        "\n",
        "Sur https://radimrehurek.com/gensim/scripts/word2vec2tensor.html, on peut lire comment convertir un modèle w2v (éventuellement produit par gensim) en modèle tsv, puis comment le visualiser avec le projector de tensorflow : \n",
        "\n",
        "1. Convert your word-vector with word2vec2tensor method ou le script gensim.scripts.word2vec2tensor \n",
        "2. Open http://projector.tensorflow.org/\n",
        "3. Click “Load Data” button from the left menu.\n",
        "4. Select “Choose file” in “Load a TSV file of vectors.” and choose “/tmp/my_model_prefix_tensor.tsv” file.\n",
        "5.  Select “Choose file” in “Load a TSV file of metadata.” and choose “/tmp/my_model_prefix_metadata.tsv” file.\n",
        "6. ???\n",
        "7. PROFIT!\n",
        "\n",
        "Le code ci-dessous définit des fonctions de conversion au format de tensorflow soit depuis le format gensim-w2v soit le format w2v binaire natif. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCIzCPsHl8RJ"
      },
      "source": [
        "import gensim\n",
        "from gensim.scripts.word2vec2tensor import word2vec2tensor\n",
        "\n",
        "def convert_gensim_w2v_to_w2v (gensim_w2v_in_path, w2v_out_path):\n",
        "  \"\"\"\n",
        "  convert a model from gensim_w2v format to w2v (orginal) format\n",
        "  \"\"\"\n",
        "  w2v_model = KeyedVectors.load(gensim_w2v_in_path)\n",
        "  vectors = w2v_model.wv\n",
        "  # save memory\n",
        "  # del model\n",
        "\n",
        "  # The trained word vectors can also be stored/loaded from a format compatible\n",
        "  # with the original word2vec implementation via Word2Vec.wv.save_word2vec_format \n",
        "  # and gensim.models.keyedvectors.KeyedVectors.load_word2vec_format().\n",
        "  vectors.save_word2vec_format(w2v_out_path, binary = True)\n",
        "\n",
        "def convert_w2v_to_tsv (w2v_in_path, tsv_out_path):\n",
        "  \"\"\"\n",
        "  convert a model from w2v original format to tsv format\n",
        "  \"\"\"\n",
        "  # When running word2vec2tensor with a file resulting from \n",
        "  # save_word2vec_format, we obtain the following error:\n",
        "  # UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbd in position 0: invalid start byte\n",
        "  # To solve the issue, I have to load with load_word2vec_format the saved file \n",
        "  # and save it again with save_word2vec_format\n",
        "  w2v_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_in_path, binary=True, unicode_errors='ignore')   \n",
        "  w2v_model.wv.save_word2vec_format(w2v_in_path+\".tmp\", binary = True)\n",
        "  word2vec2tensor(w2v_in_path+\".tmp\", tsv_out_path,  binary = True)\n",
        "\n",
        "def convert_gensim_w2v_to_tsv (gensim_w2v_in_path, tsv_out_path):\n",
        "  \"\"\"\n",
        "  convert a model from gensim w2v format to tsv format\n",
        "  \"\"\"\n",
        "  convert_gensim_w2v_to_w2v (gensim_w2v_in_path, gensim_w2v_in_path+\".tmp\")\n",
        "  convert_w2v_to_tsv (gensim_w2v_in_path+\".tmp\", tsv_out_path)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFb7NDjszD-f"
      },
      "source": [
        "Convertissons le modèle public récupéré"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNHxotHYzENQ",
        "outputId": "b322f8f1-b09d-4c96-8bd5-a844ac86b270"
      },
      "source": [
        "convert_w2v_to_tsv(w2v_model_path, w2v_model_path+\".tsv\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AsqSRYCrDgq"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "Télécharger les 3? fichiers produits et chargez les dans projector tensorflow. Si c'est trop compliqué, le projector vient avec des modèles préchargés. \n",
        "\n",
        "* Observez-vous des zones plus denses que d'autres ? Qu'est ce que cela peut vouloir signifier ? \n",
        "* Testez les labels 3D, de cliquer sur un point/mot (fixer le voisinage à la valeur minimale) pour observer l'illumination d'une zone, chercher un mot, visualiser 'isolate 6 points'. \n",
        "* Testez aussi un des tensors found disponible comme Word2Vec 10K ou all.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y7ghTpGvAP2"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irvZq8LXc6Mj"
      },
      "source": [
        "## Construire un modèle word2vec et fasttext avec gensim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqjuUMhvdXhq"
      },
      "source": [
        "Word2Vec et FastText prennent tous deux un corpus normalisé segmenté en phrases et tokenisés en mots. \n",
        "\n",
        "On pourrait très bien utiliser spaCy ou nltk pour ce faire, mais ce type de pré-traitement prend \"un peu de temps\". On va directement utilisé un corpus de la base nltk disponible avec la segmentation en phrases et la tokenization mots.\n",
        "\n",
        "Le code ci-dessous utilise le sélection du corpus gutenberg segmentée en phrases et en tokens par nltk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ4817rGdX-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a909847c-328a-4899-83b5-e978d4d9ddcc"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk_gutenberg_corpus = list()\n",
        "word_counter = 0\n",
        "\n",
        "for fileid in nltk.corpus.gutenberg.fileids():\n",
        "  segmented_and_tokenized_doc = nltk.corpus.gutenberg.sents(fileid)\n",
        "  for sent in segmented_and_tokenized_doc:\n",
        "    words = [re.sub(r'[^a-zA-Z\\s]', '', word, re.I|re.A).lower() for word in sent]\n",
        "    # 98552 2621785\n",
        "    words = [word for word in words if len(word) > 3]\n",
        "    # 95804 1154977\n",
        "    if len(words)>0: \n",
        "      nltk_gutenberg_corpus.append(words)\n",
        "      word_counter += len(words)\n",
        "      \n",
        "print ('sentences_len:', len(nltk_gutenberg_corpus), 'words_len:', word_counter)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentences_len: 95801 words_len: 1154977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI2LwobJfVqO"
      },
      "source": [
        "Hyper-paramètres les plus communs du constructeur de la [classe Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec) :\n",
        "\n",
        "* `corpus`: List of tokenized sentences \n",
        "* `size` : Dimensionality of the word vectors (default: 100)\n",
        "* `window` : Maximum distance between the current and predicted word within a sentence\n",
        "* `sg` : Training algorithm: 1 for skip-gram; otherwise CBOW\n",
        "* `iter` :  Number of iterations (epochs) over the corpus\n",
        "* `workers` Use these many worker threads to train the model (=faster training with multicore machines)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUUD2EMae1y9"
      },
      "source": [
        "# Set values for various parameters\n",
        "lr = 0.05   # Learning rate\n",
        "dim = 100   # Word vector dimensionality  \n",
        "ws = 5      # Context window size    \n",
        "epoch = 5\n",
        "minCount = 5 # Minimum word count \n",
        "neg = 5\n",
        "loss = 'ns'\n",
        "t = 1e-4\n",
        "#sample = 1e-3   # Downsample setting for frequent words\n",
        "sg=1 \n",
        "\n",
        "params = {\n",
        "    'alpha': lr,\n",
        "    'size': dim,\n",
        "    'window': ws,\n",
        "    'iter': epoch,\n",
        "    'min_count': minCount,\n",
        "    'sample': t,\n",
        "    'sg': 1,\n",
        "    'hs': 0,\n",
        "    'negative': neg\n",
        "}"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1R9Ee81fRw7"
      },
      "source": [
        "Construction des modèles. Observez la rapidité compte tenu du nombre de mots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvJPnIjme657",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b90a64a-a535-47bc-8034-05a5448956c1"
      },
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "# \n",
        "%time w2v_model = Word2Vec(nltk_gutenberg_corpus, **params) \n",
        "\n",
        "# save the model\n",
        "!mkdir -p models\n",
        "w2v_model_path = 'models/w2v_nltk-gutenberg_100_5_5_sg.gensim-bin'\n",
        "w2v_model.save(w2v_model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 36.2 s, sys: 147 ms, total: 36.3 s\n",
            "Wall time: 20.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_SvnXRufQk5"
      },
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "#\n",
        "%time ft_model = FastText(nltk_gutenberg_corpus, **params)\n",
        "\n",
        "# save the model\n",
        "!mkdir -p models\n",
        "ft_model_path = 'models/ft_nltk-gutenberg_100_5_5_sg.gensim-bin'\n",
        "ft_model.save(ft_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6BI2xNr3Cch"
      },
      "source": [
        "Si vous souhaitez visualiser le modèle construit et sauvé au format gensim-w2v via projector tensorflow, vous pouvez exécuter la commande suivante et vous rendre à la section précédente \"visualiser un modèle w2v en 3D via projector tensorflow\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqJSbyzWpefJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70cf085a-231a-4c17-8cf6-f06ca2f62216"
      },
      "source": [
        "w2v_model_path = 'models/w2v_nltk-gutenberg_100_5_5_sg.gensim-bin'\n",
        "tensor_filename = 'models/tensor_nltk-gutenberg_100_5_5_sg.tsv'\n",
        "\n",
        "convert_gensim_w2v_to_tsv(w2v_model_path, tensor_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fo1FcwYfeCt"
      },
      "source": [
        "## Comparer et évaluer deux modèles\n",
        "\n",
        "[`gensim` implémente la comparaison de modèles](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb) selon la tâche de **raisonnement analogique** telle que décrite à la [section 4.1 du papier de 2013 de Mikolov et al.](https://arxiv.org/pdf/1301.3781v3.pdf).\n",
        "\n",
        "```\n",
        ":capital-common-countries\n",
        "Athens Greece Baghdad Iraq\n",
        "Athens Greece Bangkok Thailand\n",
        "...\n",
        ":capital-world\n",
        "Algiers Algeria Baghdad Iraq\n",
        "Ankara Turkey Dublin Ireland\n",
        "...\n",
        ": city-in-state\n",
        "Chicago Illinois Houston Texas\n",
        "Chicago Illinois Philadelphia Pennsylvania\n",
        "...\n",
        ": gram1-adjective-to-adverb\n",
        "amazing amazingly apparent apparently\n",
        "amazing amazingly calm calmly\n",
        "...\n",
        "```\n",
        "\n",
        "D'autres évaluations intrinsèques sont possibles comme le [calcul d'un coefficient de corrélation entre un taux de similarité calculée sur la base d'une appréciation humaine et un score de similarité cosinus entre des représentations Word2Vec](https://nlp-ensae.github.io/materials/course2/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI_ZHPkjiday"
      },
      "source": [
        "Ci-dessous nous mettons en oeuvre la tâche de raisonnement analogique de Mikolov et al."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7WhKE0_fglD"
      },
      "source": [
        "# download the file questions-words.txt to be used for comparing word embeddings\n",
        "!wget https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC2EoBqyiwbr"
      },
      "source": [
        "# un oeil sur les n premières lignes du fichier\n",
        "!head questions-words.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZcqh-0Ni8Ms"
      },
      "source": [
        "Définition de la méthode de calcul de la performance de resolution de la tâche d'analogie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbiByLHzi8y1"
      },
      "source": [
        "def print_accuracy(model, questions_file):\n",
        "    print('Evaluating...\\n')\n",
        "    acc = model.accuracy(questions_file)\n",
        "    #acc = model.wv.evaluate_word_analogies(questions_file)\n",
        "\n",
        "    sem_correct = sum((len(acc[i]['correct']) for i in range(5)))\n",
        "    sem_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5))\n",
        "    sem_acc = 100*float(sem_correct)/sem_total\n",
        "    print('\\nSemantic: {:d}/{:d}, Accuracy: {:.2f}%'.format(sem_correct, sem_total, sem_acc))\n",
        "    \n",
        "    syn_correct = sum((len(acc[i]['correct']) for i in range(5, len(acc)-1)))\n",
        "    syn_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5,len(acc)-1))\n",
        "    syn_acc = 100*float(syn_correct)/syn_total\n",
        "    print('Morphologic: {:d}/{:d}, Accuracy: {:.2f}%\\n'.format(syn_correct, syn_total, syn_acc))\n",
        "    return (sem_acc, syn_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fULOXjn8jOK_"
      },
      "source": [
        "Exécution de l'évaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw8fHge1jIw7"
      },
      "source": [
        "#\n",
        "word_analogies_file = 'questions-words.txt'\n",
        "\n",
        "print('\\nLoading Word2Vec embeddings')\n",
        "w2v_model = KeyedVectors.load(w2v_model_path)\n",
        "print('Accuracy for Word2Vec:')\n",
        "print_accuracy(w2v_model, word_analogies_file)\n",
        "\n",
        "print('\\nLoading FastText embeddings')\n",
        "ft_model = KeyedVectors.load(ft_model_path)\n",
        "print('Accuracy for FastText (with n-grams):')\n",
        "print_accuracy(ft_model, word_analogies_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N11hzoM9koW2"
      },
      "source": [
        "#### QUESTION\n",
        "* Lequel des deux modèles donnent les meilleurs résultats sur l'analyse morphologique ? Sur l'analyse sémantique ? Est-ce cohérent de ce que vous connaissez des modèles ? \n",
        "* Relancez la construction des modèles puis leur comparaison. Obtenez-vous les mêmes scores de performance ? Pourquoi ?\n",
        "* Les données d'entraînement sont des romans classiques issus de la collection Gutenberg. Si les données avaient été issues de la Wikipedia, quels résultats auraient pu changer ? Si vous souhaitez tester, ci-dessous je vous donne un snippet de code qui récupère une version normalisée de la wikipédia et qui lance la contruction des modèles w2v et ft. Cela prendra qqs minutes... \n",
        "* Selon vous, dans une perspective de comparaison de modèles, est-ce important de construire ceux-ci sur les mêmes données ? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wfDsIPk8oiq"
      },
      "source": [
        "# WARNING: ce qui suit est optionnel !\n",
        "\n",
        "# récupération d'un corpus normalisé de la wikipédia-en\n",
        "# une string tokenisée avec le caractère espace de mots pleins \n",
        "!mkdir data\n",
        "!wget -nc http://mattmahoney.net/dc/text8.zip -P data\n",
        "!unzip data/text8.zip -d data\n",
        "\n",
        "text8_path = 'data/text8'\n",
        "\n",
        "# Text8Corpus class for reading space-separated words file\n",
        "from gensim.models.word2vec import Text8Corpus\n",
        "\n",
        "# Construction des modèles w2v et ft avec text8\n",
        "%time w2v_model = Word2Vec(Text8Corpus(text8_path), **params)\n",
        "%time ft_model = FastText(Text8Corpus(text8_path), **params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a42nxk1zldic"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzrX5PKl62WQ"
      },
      "source": [
        "## Construire une représentation continue de document\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x1z2XpF9Vim"
      },
      "source": [
        "> Une approche naïve de la _représentation du sens d'un document revient à faire la moyenne des vecteurs des mots qui le composent_. [Le and Mikolov in 2014](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) \n",
        "\n",
        "L'API de spaCY permet aisément d'accéder aux embeddings des mots d'un document. Elle propose aussi une représentation vectorielle des documents sous la forme de la moyenne des vecteurs de mots qui les composent. L'API permet aussi de calculer des similarités entre ces vecteurs (mot/mot, document/document, document/mot).\n",
        "\n",
        "Les modèles de spacy embarquent généralement des  vecteurs de mots de type Word2Vec (éviter un modèle _sm/small_). L'import des formats gensim, fastext ou Word2Vec est possible. \n",
        "\n",
        "Pour ces raisons nous utiliserons ici [spaCy](https://spacy.io/usage/linguistic-features#vectors-similarity). Remarquez que gensim offre les mêmes fonctionnalités sous le nom de [doc2vec et de paragraph vector](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H7DDOkl4-Hl"
      },
      "source": [
        "# sélectionner le modèle à installer\n",
        "#!python -m spacy download en_core_web_lg\n",
        "#!python -m spacy download en_core_web_trf\n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "# redémarrer l'environnement d'exécution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdbx-UPc2ylv"
      },
      "source": [
        "# charger le modèle\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_md')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G-J2HGZFFsI"
      },
      "source": [
        "Après traitement (via la méthode `nlp`), on accède au vecteur d'un mot ou du document via la propriété `vector`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V86z9Wk43Eww"
      },
      "source": [
        "# trois documents traités avec spaCy\n",
        "doc1 = nlp('The dog is lazy but the brown fox is quick!')\n",
        "doc2 = nlp('The quick brown fox jumps over the lazy dog.')\n",
        "doc3 = nlp('The sky is blue and beautiful.')\n",
        "\n",
        "# trois mots du 1er document\n",
        "dog = doc1[1]\n",
        "lazy = doc1[3]\n",
        "fox = doc1[7]\n",
        "\n",
        "# la représentation vectorielle continue d'un mot\n",
        "print(dog.text, dog.vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdv8c0a_G7UK"
      },
      "source": [
        "SpaCy permet aussi d'accéder facilement à l'embedding d'un document (résultant de la moyenne des embeddings des mots qui le composent). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPubw8HiHx1h"
      },
      "source": [
        "# la représentation vectorielle continue d'un document\n",
        "print (doc1.text, doc1.vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOK9dK_i83mP"
      },
      "source": [
        "L'API offre une fonction de calcul de similarité [`similarity`](https://spacy.io/usage/vectors-similarity), ci-dessous illustrée sur la comparaison de mots. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns33bJBM9i9f"
      },
      "source": [
        "print(dog.similarity(lazy))\n",
        "print(dog.similarity(fox))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2Bvump3IOsh"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "* En écrivant le code qui va bien, indiquez quelle est la dimension des vecteurs embarqués par le modèle de spaCy chargé ? Vérifiez que les documents font la même taille. Est-ce d'ailleurs normal ?\n",
        "\n",
        "* Expérimentez la fonction de calcul de similarité sur les documents courts doc1/doc2 et doc1/doc3. Donnez le code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifnqjF86Jfjf"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzYB6IiGKoE8"
      },
      "source": [
        "#TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n65vzdOuKzb4"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "nltk embarque des corpus de textes libres notamment des textes de la [collection gutenberg](https://www.gutenberg.org/about/).\n",
        "\n",
        "* Le code ci-dessous permet de charger trois textes dans un format brut. Deux sont de Shakespeare à savoir _Hamlet_ et _Macbeth_ et un est de Lewis Carroll à savoir _Alice aux pays des merveilles_. Ajouter le code permettant de calculer la similarité des textes hamlet/macbeth et hamlet/alice.\n",
        "* En observant les résultats des calculs de similarité entre petits documents et entre documents, trouvez-vous que la représentation des documents calculée comme une moyenne des vecteurs des mots qui les composent est fiable ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF7LOHcu_hcB"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# lister les textes présents dans la collection gutenberg\n",
        "print(nltk.corpus.gutenberg.fileids())\n",
        "\n",
        "# récupère le texte de trois oeuvres\n",
        "hamlet = nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt') # .words() pour le texte tokenisé\n",
        "macbeth = nltk.corpus.gutenberg.raw('shakespeare-macbeth.txt')\n",
        "alice = nltk.corpus.gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "\n",
        "print ('hamlet', len(hamlet), hamlet[:100], '----')\n",
        "print ('macbeth', len(macbeth), macbeth[:100], '----')\n",
        "print ('alice', len(alice), alice[:100], '----')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCjvRijAMpi9"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbeBgRqXAEiY"
      },
      "source": [
        "# TODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XyFa9V0O7sG"
      },
      "source": [
        "## Partitionnement sur la base d'une représentation document-embeddings\n",
        "\n",
        "Cette section fait écho aux sections de partionnements sur la base de représentation des documents de la forme document-terms et document-thèmes d'un notebook antérieul. La méthode de partitionnement kmeans est utilisée.\n",
        "\n",
        "Les lignes de code ci-dessous déclare un corpus, le pré-traite (i.e. calcul les vecteurs continus des documents) et affiche le corpus sous la forme d'une dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVrMBBMkCWy1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# déclaration de la donnée\n",
        "corpus = ['The sky is blue and beautiful.',\n",
        "          'Love this blue and beautiful sky!',\n",
        "          'The quick brown fox jumps over the lazy dog.',\n",
        "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
        "          'I love green eggs, ham, sausages and bacon!',\n",
        "          'The brown fox is quick and the blue dog is lazy!',\n",
        "          'The sky is very blue and the sky is very beautiful today',\n",
        "          'The dog is lazy but the brown fox is quick!'    \n",
        "]\n",
        "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
        "\n",
        "# pré-traitement\n",
        "dt_matrix = [nlp(doc).vector for doc in corpus]\n",
        "\n",
        "# affichage visuel\n",
        "corpus_df = pd.DataFrame({'Document': corpus, 'Category': labels})\n",
        "corpus_df = corpus_df[['Document', 'Category']]\n",
        "corpus_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyl9aamcQlkc"
      },
      "source": [
        "Le code suivant opère le clustering et l'affichage des classes prédites."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0npcaqrZB4eT"
      },
      "source": [
        "# clustering des documents selon la méthode kmeans \n",
        "# en utilisant la distribution des thèmes et non la distrubution des termes \n",
        "# comme traits discriminants entre documents\n",
        "from sklearn.cluster import KMeans\n",
        "# Warning: ici n_clusters ne doit pas être touché\n",
        "km = KMeans(n_clusters=3, random_state=0)\n",
        "#km.fit_transform(features)\n",
        "km.fit_transform(dt_matrix)\n",
        "\n",
        "# affichage des clusters\n",
        "cluster_labels = km.labels_\n",
        "import pandas as pd\n",
        "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
        "pd.concat([corpus_df, cluster_labels], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T21SodpxQx__"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "* De prime abord, est-ce que ce type de représentation vous semble exploitable avec des textes courts ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bno3jlT4QywJ"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTautWC03PT9"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKdXVnDprkjH"
      },
      "source": [
        "# Références\n",
        "* représentation continue de document https://github.com/clement-plancq/outils-corpus/blob/master/outils_corpus-7.ipynb\n",
        "* https://nlp-ensae.github.io/materials/course2/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pour aller plus loin\n",
        "\n",
        "[Le sujet suivant se fonde sur une proposition de 2021 de Nicolas Dugué](https://github.com/nicolasdugue/atal/blob/master/README.md#lapproche-textrank-pour-le-r%C3%A9sum%C3%A9-automatique).\n",
        "\n",
        "## L'approche TextRank pour le résumé automatique\n",
        "\n",
        "Pour ce travail, vous avez besoin de : \n",
        "- scipy, numpy\n",
        "- gensim\n",
        "- nltk/spacy\n",
        "- networkx\n",
        "\n",
        "L'approche *Textrank* pour le résumé automatique est une baseline très efficace et très communément utilisée **[Mihalcea et Tarau]**. L'approche *Textrank* est une approche de résumé automatique dite **extractive** : il s'agit de résumer un document via **l'extraction** de phrases considérées comme caractéristiques du contenu du document. *Textrank* permet également d'extraire les mots-clés pour un document afin de l'indexer, mais nous nous concentrons ici sur l'approche de résumé automatique.\n",
        " \n",
        "Cette approche est basée sur l'algorithme du *Pagerank*, l'algorithme qui a notamment rendu célèbre le moteur de recherche Google **[Brin et Page]**. Ce dernier  s'applique sur des données structurées sous forme de graphe, et la construction de ce graphe est donc l'une des premières étapes de l'approche *Textrank* que nous détaillons ci-après : \n",
        "1. Séparation du document en phrases (`nltk.sent_tokenize` vous aidera) ;\n",
        "2. Nettoyage des phrases (lowercase avec `.lower()`, suppression ponctuation avec `table = str.maketrans(dict.fromkeys(string.punctuation))`)\n",
        "3. Représentation vectorielle des phrases avec **word2vec** : chaque phrase est représentée par la moyenne des vecteurs des mots (`.get_vector(mot)` avec l'objet gensim) qui la composent. Certains mots de la phrase peuvent ne pas être dans le vocabulaire du modèle pré-appris (exception `KeyError`), on ne tient pas compte de ces mots pour le calcul.\n",
        "4. Calcul de la similarité cosine (`scipy.spatial.distance.cosine`) de chacune des paires de phrases : matrice de similarité ;\n",
        "5. Création d'un graphe valuée en utilisant la matrice de similarité (`networkx.from_numpy_array`) ;\n",
        "6. Run de l'algorithme du Pagerank (`networkx.pagerank`);\n",
        "7. Ranking des phrases dans l'ordre décroissant du score de Pagerank.\n",
        "\n",
        "Dans l'algorithme original de *Textrank*, les étapes 3 et 4 n'utilisent pas d'approches de plongements lexicaux (word embeddings). Nous proposons donc l'implémentation d'une approche originale de *Textrank* exploitant les plongements lexicaux. \n",
        "\n",
        "Testez votre approche sur n'importe quel document et comparez là avec *gensim.summarization.summarizer*. \n",
        "Vous pouvez également tenter de résumer une publication scientifique, et comparer votre résumé avec *l'abstract* humain qui en est le résumé produit par les auteurs. Pour comparer deux résumés, vous pouvez considérer la métrique *Rouge* : https://pypi.org/project/rouge/\n",
        "\n",
        "\n",
        "**[Mihalcea et Tarau]** Mihalcea, R., & Tarau, P. (2004). Textrank: Bringing order into text. In Proceedings of the 2004 conference on empirical methods in natural language processing (pp. 404-411).\n",
        "\n",
        "**[Brin et Page]** Brin, S., & Page, L. (1998). The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems, 30(1-7), 107-117.\n",
        "\n"
      ],
      "metadata": {
        "id": "ydC-9CuRfOu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### VOTRE CODE\n",
        "\n",
        "**TODO**"
      ],
      "metadata": {
        "id": "l0a4bzXphiwx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PZaYG3_TfSSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### VOTRE RETOUR D'EXPÉRIENCE\n",
        "\n",
        "**TODO**"
      ],
      "metadata": {
        "id": "ETwEEce-hlCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Références\n",
        "\n",
        "* Si des phrases ou des mots sont pondérées, des outils pythons peuvent les sélectionner cf. `heapq.nlargest()` https://medium.com/analytics-vidhya/text-summarization-using-spacy-ca4867c6b744"
      ],
      "metadata": {
        "id": "acubHt4xkHPc"
      }
    }
  ]
}