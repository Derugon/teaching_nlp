{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolashernandez/teaching_nlp/blob/main/05_Classification_de_textes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxdW5mtMGqmy"
      },
      "source": [
        "# Classification de textes\n",
        "\n",
        "La **classification** consiste à attribuer une classe à chaque texte (objet, instance, point) à classer. On parle de *classification binaire* (_binary classification_) quand il y a deux classes exclusives. On parle de *classification en classes multiples* (_multiclass classification_) pour désigner la répartition d'un lot de textes entre plus de deux ensembles (ou classes), une classe par instance. On parle de *classification multi-étiquettes* (_multi-label classification_) pour désigner les problèmes de classification où plusieurs étiquettes (classes) peuvent être assignées à une même instance.\n",
        "\n",
        "Reconnaître si un email est un spam, si une photo contient une voiture... sont des problèmes de classification (binaire).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7PRm8jl2cE0"
      },
      "source": [
        "# Sentiment analysis as a classification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee8F8DyZ9v7x"
      },
      "source": [
        "\n",
        "La tâche classique d'analyse de sentiment consiste à annoter un texte donné selon une polarité positive ou négative exprimée dans le texte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbuBLXmt3NyE"
      },
      "source": [
        "## QUESTION\n",
        "\n",
        "* Selon vous la tâche d'analyse de sentiment tel que définie juste au dessus peut se définir comme un problème de 1) classification binaire, 2) classification en classes multiples ou 3) en classification multi-étiquettes ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sjOoxLi3m7E"
      },
      "source": [
        "## VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h06bBaA34vf"
      },
      "source": [
        "# Configuration de l'environnement Google Colab en mode GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WveM1n9KFNx6"
      },
      "source": [
        "Quel mode d'exécution utilisez-vous (via GPU ou via CPU) ? Que vous dis le code ci-dessous ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwHyWGuWiCUu"
      },
      "outputs": [],
      "source": [
        "# memory footprint support libraries/code\n",
        "# https://medium.com/@oribarel/getting-the-most-out-of-your-google-colab-2b0585f82403\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "#\n",
        "\n",
        "if len(GPUs) >0: \n",
        "  gpu = GPUs[0]\n",
        "  printm()\n",
        "else:\n",
        "  print ('no GPU. Are you sure the hardware accelerator is configured to GPU? To do this go to Runtime→Change runtime type and change the Hardware accelerator to GPU.') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxVCJnn3OBq2"
      },
      "source": [
        "Configurez votre mode d'exécution du Google Colab en mode GPU.\n",
        "\n",
        "> To use the google colab in a GPU mode you have to make sure the hardware accelerator is configured to GPU. To do this go to Runtime→Change runtime type and change the Hardware accelerator to GPU. Sometimes, all GPUs are in use and there is no GPU available.\n",
        "\n",
        "Une fois configuré le GPU, vérifiez l'état de la mémoire sur la carte en réexécutant le code \"memory footprint\" ci-dessus. Le message a-t-il changé favorablement ?\n",
        "\n",
        "Le mode d'exécution GPU est nécessaire pour le fine-tuning de BERT ci-dessous. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE0PnqNnM1tB"
      },
      "source": [
        "# Allociné dataset\n",
        "\n",
        "The [Allociné dataset](https://huggingface.co/datasets/allocine) is a French-language dataset for sentiment analysis. The texts are movie reviews written between 2006 and 2020 by members of the Allociné.fr community for various films. It contains 100k positive and 100k negative reviews divided into train (160k), validation (20k), and test (20k). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxo9pzqyVwIG"
      },
      "source": [
        "### From huggingface datahub\n",
        "\n",
        "After execution of this cell, you must restart the runtime in order to use  newly installed versions.\n",
        "\n",
        "Just click on the button on the suggester \"restart runtime\" button (or go the runtime menu)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXgmkHWnxIS8"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve the [allociné dataset](https://huggingface.co/datasets/allocine) from the huggingface hub."
      ],
      "metadata": {
        "id": "1eCtRF0tbm5S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvYzPpHJwSEu"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/datasets/allocine\n",
        "\n",
        "from datasets import load_dataset_builder\n",
        "ds_builder = load_dataset_builder(\"allocine\")\n",
        "\n",
        "# Inspect dataset description\n",
        "print(ds_builder.info.description)\n",
        "\n",
        "# Inspect dataset features\n",
        "print(ds_builder.info.features)\n",
        "\n",
        "# get_dataset_split_names\n",
        "from datasets import get_dataset_split_names\n",
        "get_dataset_split_names(\"allocine\")\n",
        "\n",
        "# load_dataset\n",
        "from datasets import load_dataset\n",
        "allocine_dataset = load_dataset(\"allocine\")\n",
        "allocine_dataset\n",
        "#train_dataset = load_dataset(\"allocine\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvdy-2-DV9zX"
      },
      "source": [
        "#### Select data ratio to process and create Pandas DataFrame\n",
        "\n",
        "Definition of a method to select a ratio of a dataset split. By default return 100 % of the split."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_dataset_split_ratio(dataset_split, seed=42, ratio=100):\n",
        "  if ratio > 100 or ratio < 1: ratio = 100\n",
        "  reviews = dataset_split['review']\n",
        "  labels = dataset_split['label']\n",
        "  \n",
        "  data = [pair for pair in zip(reviews, labels)]\n",
        "  \n",
        "  np.random.seed(seed)\n",
        "  np.random.shuffle(data)\n",
        "  \n",
        "  ratio_len = int(len(data)*ratio/100)\n",
        "  \n",
        "  data = list(zip(*data))\n",
        "\n",
        "  return {'review':data[0][:ratio_len], 'label':data[1][:ratio_len]}\n"
      ],
      "metadata": {
        "id": "AIdQWqGa7WmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of the method to create a dataframe from a huggingface dataset split (`{'review':[], 'label':[]}`) :"
      ],
      "metadata": {
        "id": "2v5boWFkDIhy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEkNPZg705jM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# From huggingface dataset split to DataFrame\n",
        "def hf_dataset_split_to_df (huggingface_dataset_split):\n",
        "  df = pd.DataFrame(huggingface_dataset_split)\n",
        "  # https://github.com/amaiya/ktrain/blob/master/examples/text/ArabicHotelReviews-nbsvm.ipynb\n",
        "  df['label'] = df['label'].apply(lambda x: 'negative' if x == 0 else 'positive')\n",
        "  df = pd.concat([df, df.label.astype('str').str.get_dummies()], axis=1, sort=False)\n",
        "  df = df[['review', 'negative', 'positive']]\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sélectionner la taille des données sur lesquelles vous voulez travailler et construire la dataframe correspondante.\n",
        "\n",
        "Le code ci-dessous extrait 5 % du corpus. C'est suffisant pour étudier comment fonctionne les modèles présentés ci-après sans attendre trop de temps d'entraînement."
      ],
      "metadata": {
        "id": "uqrzl25ODe2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratio = 100 # percent of the data\n",
        "\n",
        "train_df = hf_dataset_split_to_df(get_dataset_split_ratio(allocine_dataset['train'], 42, ratio))\n",
        "val_df = hf_dataset_split_to_df(get_dataset_split_ratio(allocine_dataset['validation'], 42, ratio))\n",
        "test_df = hf_dataset_split_to_df(get_dataset_split_ratio(allocine_dataset['test'], 42, ratio))\n",
        "\n",
        "print(train_df.head())\n",
        "print(train_df.describe())\n",
        "print('len(train_df) :', len(train_df))\n",
        "print('len(val_df) :', len(val_df))\n",
        "print('len(test_df) :', len(test_df))"
      ],
      "metadata": {
        "id": "jmwbpxGDCl66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLfw1G8wMxkz"
      },
      "source": [
        "# ktrain\n",
        "\n",
        "[ktrain](https://github.com/amaiya/ktrain) a lightweight wrapper for the deep learning library _TensorFlow Keras_ (and other libraries) to help build, train, and deploy neural networks and other machine learning models. Inspired by ML framework extensions like fastai and ludwig, ktrain is designed to make deep learning and AI more accessible and easier to apply for both newcomers and experienced practitioners.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration et installation de ktrain\n",
        "\n",
        "Configuration de l'environnement"
      ],
      "metadata": {
        "id": "OMQniIpmoI6r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZ-F6wCZRYpw"
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"; "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUvbU_z33-Lw"
      },
      "source": [
        "Installation de la bibliothèque ktrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxWRMgrKR7Q1",
        "outputId": "0aa7ba68-a129-4ee4-fb37-0c7621b31b5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ktrain\n",
            "  Downloading ktrain-0.31.10.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 90.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.2.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.3.5)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ktrain) (21.3)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.42.1)\n",
            "Collecting cchardet\n",
            "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
            "\u001b[K     |████████████████████████████████| 263 kB 57.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.0.4)\n",
            "Collecting syntok>1.3.3\n",
            "  Downloading syntok-1.4.4-py3-none-any.whl (24 kB)\n",
            "Collecting transformers==4.17.0\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 50.8 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 50.7 MB/s \n",
            "\u001b[?25hCollecting keras_bert>=0.86.0\n",
            "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
            "Collecting whoosh\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[K     |████████████████████████████████| 468 kB 46.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (0.10.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (5.0.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (2022.6.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 27.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0->ktrain) (4.1.1)\n",
            "Collecting keras-transformer==0.40.0\n",
            "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
            "Collecting keras-pos-embd==0.13.0\n",
            "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
            "Collecting keras-multi-head==0.29.0\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "Collecting keras-layer-normalization==0.16.0\n",
            "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
            "Collecting keras-position-wise-feed-forward==0.8.0\n",
            "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
            "Collecting keras-embed-sim==0.10.0\n",
            "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
            "Collecting keras-self-attention==0.51.0\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->ktrain) (2022.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->ktrain) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.17.0->ktrain) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.17.0->ktrain) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ktrain) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ktrain) (1.7.3)\n",
            "Building wheels for collected packages: ktrain, keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, langdetect, sacremoses\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktrain: filename=ktrain-0.31.10-py3-none-any.whl size=25312982 sha256=70227a5d72e44facc31d88ed61acafb9231dcc674c43989265ef46cd25c1602c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/1c/1b/6df2db85720b8f5c6ea5e3ae37313cfc656f248abf910b7cfd\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=6eccc19a3d1f34016b0f543583479d15b4ff8cb3e7a450efd7d8f8effb741458\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/e8/45/842b3a39831261aef9154b907eacbc4ac99499a99ae829b06f\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12305 sha256=93d7627d165ec5dcd7fc3f4f1fba6f225ba1d1d7f20ac9b1f1a2146902388d1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/68/26/692ed21edd832833c3b0a0e21615bcacd99ca458b3f9ed571f\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3960 sha256=de4f5b1ead6c579ced538cc75a6e0973b2554cf22322b7a831a508d0d02e57c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/67/b5/d847588d075895281e1cf5590f819bd4cf076a554872268bd5\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=fbe5eacc0af8beb58bfd58e006845546cc5f5ba38f75e0a282288420316760a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/5d/1c/2e619f594f69fbcf8bc20943b27d414871c409be053994813e\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14993 sha256=b963eff51d5c16363e1051514c20cdb4984bb204892fffdebedd9887c92b4be5\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/aa/3c/9d15d24005179dae08ff291ce99c754b296347817d076fd9fb\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6962 sha256=3a514c38e3a757572b475b9b357b38baa7aa0c33d26dbc2c3c1133a5c191acbf\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/c1/a0/dc44fcf68c857b7ff6be9a97e675e5adf51022eff1169b042f\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=0055375c99ca577487c9d0960e1353b8d61e3d4c9648a89787c80690b464700c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/75/6f/d42f6e051506f442daeba53ff1e2d21a5f20ef8c411610f2bb\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=032270745a292d782e160d2f546caec9512022cfd185c794d544e57136aa9e96\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=3a6088b15680c89dadc00b4d4fec46e411f92e249ba325aaf375fd842778f378\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=3cfe684c7d35e98155059aaf43b2695be3cd1ad8d53260ed5b247581cdec0ece\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built ktrain keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention langdetect sacremoses\n",
            "Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, tokenizers, sacremoses, keras-transformer, whoosh, transformers, syntok, sentencepiece, langdetect, keras-bert, cchardet, ktrain\n",
            "Successfully installed cchardet-2.1.7 keras-bert-0.89.0 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 ktrain-0.31.10 langdetect-1.0.9 sacremoses-0.0.53 sentencepiece-0.1.97 syntok-1.4.4 tokenizers-0.13.1 transformers-4.17.0 whoosh-2.7.4\n"
          ]
        }
      ],
      "source": [
        "!pip install ktrain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7LRCuPB4p-y"
      },
      "source": [
        "Import de la bibliothèque ktrain dédié aux traitements de la modalité textuelle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2avp8suURzMT"
      },
      "outputs": [],
      "source": [
        "# Execution time 30 s\n",
        "import ktrain\n",
        "from ktrain import text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYGyWUwn9yPS"
      },
      "source": [
        "## Fonctionnement de ktrain\n",
        "\n",
        "\n",
        "Pour réaliser cette tâche de classification nous allons utiliser la bibliothèque ktrain pour faire un apprentissage par transfert.\n",
        "\n",
        "Les étapes sont les suivantes\n",
        "1. chargement des données avec application d'un prétraitement défini à la volée\n",
        "2. construction d'un modèle de classification sur la base d'un modèle pré-entraîné spécifié\n",
        "3. récupération d'une instance du modèle pour la personnalisation de celui-ci\n",
        "4. recherche d'un bon taux d'apprentissage\n",
        "5. entraînement du classifieur i.e. personnalisation du modèle de base à l'aide d'un taux d'apprentissage défini\n",
        "6. utilisation du nouveau modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAvPw6g49WGa"
      },
      "source": [
        "*ETAPE 1 :* \n",
        "\n",
        "Le type de pré-traitement est fonction du modèle pré-entraîné spécifié."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTBW_SEO9mXi"
      },
      "source": [
        "*ETAPE 2 :*\n",
        "\n",
        "ktrain vient avec quelques modèles pré-entraînés packagés. Pour les connaître, exécutez : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7FvrdBC9n2W",
        "outputId": "f026f082-cb4f-4dba-8030-b1a136f71a04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fasttext: a fastText-like model [http://arxiv.org/pdf/1607.01759.pdf]\n",
            "logreg: logistic regression using a trainable Embedding layer\n",
            "nbsvm: NBSVM model [http://www.aclweb.org/anthology/P12-2018]\n",
            "bigru: Bidirectional GRU with pretrained fasttext word vectors [https://fasttext.cc/docs/en/crawl-vectors.html]\n",
            "standard_gru: simple 2-layer GRU with randomly initialized embeddings\n",
            "bert: Bidirectional Encoder Representations from Transformers (BERT) from keras_bert [https://arxiv.org/abs/1810.04805]\n",
            "distilbert: distilled, smaller, and faster BERT from Hugging Face transformers [https://arxiv.org/abs/1910.01108]\n"
          ]
        }
      ],
      "source": [
        "text.print_text_classifiers()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae1TJ5nk_K7G"
      },
      "source": [
        "Vous connaissez **[fasttext](https://github.com/facebookresearch/fastText)** de (Facebook/Meta) car vous l'avez déjà utilisé dans un [précédent TP](https://github.com/nicolashernandez/teaching_nlp/blob/main/04_repr%C3%A9sentation_vectorielle_continue.ipynb)... fasttext permet de produire une représentation continue des mots en suivant une approche à la word2vec. Sa spécificité est qu'il propose de traiter la variabilité morphologique des mots en construisant des vecteurs non pas pour des mots mais pour des sous-mots (séquence de caractères). Le vecteur d'un mot est la somme de tous les vecteurs des sous-mots le composant. Cette approche est indépendante de la langue, et montre de meilleurs résultats que word2vec sur des tâches syntaxiques, surtout quand le corpus d'entraînement est petit. Word2vec est légèrement meilleur pour des tâches sémantiques. Un des avantage de FastText est de pouvoir fournir des vecteurs mêmes pour les mots hors vocabulaires.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XtFCEfYWchi"
      },
      "source": [
        "[**NBSVM**](https://medium.com/@asmaiya/a-neural-implementation-of-nbsvm-in-keras-d4ef8c96cb7c) is an approach to text classification proposed by [Wang and Manning](https://www.aclweb.org/anthology/P12-2018) that takes a linear model such as SVM (or logistic regression) and infuses it with Bayesian probabilities by replacing word count features with Naive Bayes log-count ratios. Despite its simplicity, NBSVM models have been shown to be both fast and powerful across a wide range of different text classification datasets. \n",
        "Keras offers a NBSVM model implemented as a neural network using two embedding layers. The first stores the Naive Bayes log-count ratios. The second stores learned weights (or coefficients) for each feature (i.e., word) in this linear model. The prediction, then, is simply the dot product of these two vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0H3RJMw_xhB"
      },
      "source": [
        "**BERT** (_Bidirectional Encoder Representations from Transformers_), proposé par [Google AI Language](https://arxiv.org/pdf/1810.04805.pdf) est un encodeur bidirectionnel qui applique un modèle d'attention Transformers à la modélisation du language (_language modeling_). Il représente l'état de l'art.\n",
        "Les Transformers sont un mécanisme d'attention qui apprennent les relations contextuelles entre les mots dans un texte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB3bN2mE_xyi"
      },
      "source": [
        "Il est possible d'[**encapsuler les modèles Transformers du site _hugging face_**](https://github.com/amaiya/ktrain/blob/master/tutorials/tutorial-A3-hugging_face_transformers.ipynb). _hugging face_ diffuse les [modèles Transformers pré-entraînés \"officiels\"](https://huggingface.co/transformers/pretrained_models.html) ainsi que les [modèles Transformers construits par la communauté](https://huggingface.co/models).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wINsCEpD1-s"
      },
      "source": [
        "*ETAPE 4 :*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4syPI6CI6Vt6"
      },
      "source": [
        "Le **taux d'apprentissage (_learning rate_)** est un hyperparamètre qui contrôle combien le modèle doit changer en réponse à l'erreur estimée à chaque fois que les poids du modèles sont mis à jour. Choisir un 'lr' trop petit conduit à une longue phase d'entraînement qui peut resté bloquée. Choisir un 'lr' trop grand conduit à un apprentissage sous-optimal des poids et à une instabilité du processus d'entraînement. \n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMJHB4zgRCTw"
      },
      "source": [
        "## FastText\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPc3RUc_TYDc"
      },
      "source": [
        "*ETAPE 1 :* \n",
        "\n",
        "Les méthodes [`texts_from_csv`](https://amaiya.github.io/ktrain/text/index.html#ktrain.text.texts_from_csv) ou [`texts_from_df`](https://amaiya.github.io/ktrain/text/index.html#ktrain.text.texts_from_df) charge le corpus, normalise les documents (définit un préprocesseur réutilisable), et découpe la collection en données d'entraînement et données de validation (à moins que des données de validation soient passées en argument).\n",
        "\n",
        "Cette étape est commune aux modèles FastText et NBSVM (`preprocess_mode='standard'`). BERT utilise son propre tokenizer. Il faudra faire un pré-traitement dédié (`preprocess_mode='bert'`).\n",
        "\n",
        "```\n",
        "* train_filepath(str): file path to training CSV\n",
        "* text_column(str): name of column containing the text\n",
        "* label_column(list): list of columns that are to be treated as labels\n",
        "* val_filepath(string): file path to test CSV.  If not supplied, 10% of documents in training CSV will be used for testing/validation.\n",
        "* max_features(int): max num of words to consider in vocabulary ; Note: This is only used for preprocess_mode='standard'.\n",
        "* maxlen(int): each document can be of most <maxlen> words. 0 is used as padding ID.\n",
        "* ngram_range(int): size of multi-word phrases to consider e.g., 2 will consider both 1-word phrases and 2-word phrases limited by max_features\n",
        "* preprocess_mode (str):  Either 'standard' (normal tokenization) or one of {'bert', 'distilbert'} tokenization and preprocessing for use with                BERT/DistilBert text classification model.\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuKxqzFd3QBU"
      },
      "outputs": [],
      "source": [
        "# fasttext\n",
        "#NUM_WORDS = 50000\n",
        "#MAXLEN = 150\n",
        "#NGRAMS_SIZE = 1# 1 # 8 minutes avec 2 pour 10000 examples\n",
        "\n",
        "# nbsvm \n",
        "#NUM_WORDS = 80000\n",
        "#MAXLEN = 2000\n",
        "#NGRAMS_SIZE = 3\n",
        "\n",
        "(x_train_preproc, y_train_preproc), (x_val_preproc, y_val_preproc), preproc = text.texts_from_df (train_df, \n",
        "                                                                   'review', # name of column containing review text\n",
        "                                                                   label_columns=['negative', 'positive'],\n",
        "                                                                   val_df=val_df, # if None, 10% of data will be used for validation\n",
        "                                          #max_features=NUM_WORDS, \n",
        "                                          #maxlen=MAXLEN,\n",
        "                                          #ngram_range=NGRAMS_SIZE,\n",
        "                                          preprocess_mode='standard' # default\n",
        "                                          )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LktiVGh8NVSW"
      },
      "source": [
        "Observons les 5 premières des données prétraitées. `x_` représente la donnée et `y_` la classe. On note que les données ont été transformées. Chaque mot est remplacé par un identifiant. On note que la classe est décrite par 2 colonnes avec deux codes \"1 0\" et \"0 1\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzzWhW3ZHAq8"
      },
      "outputs": [],
      "source": [
        "print ('x_train_preproc', x_train_preproc[:5])\n",
        "print ('y_train_preproc', y_train_preproc[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHhCXpcQOPje"
      },
      "source": [
        "*ETAPE 2 et 3:* \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3jEacvRRuih"
      },
      "outputs": [],
      "source": [
        "# Build and return a text classification model https://amaiya.github.io/ktrain/text/index.html#ktrain.text.text_classifier\n",
        "fasttext_model = text.text_classifier('fasttext', (x_train_preproc, y_train_preproc), preproc=preproc)\n",
        "\n",
        "# Returns a Learner instance that can be used to tune and train Keras models https://amaiya.github.io/ktrain/index.html#ktrain.get_learner\n",
        "fasttext_learner = ktrain.get_learner(fasttext_model, train_data=(x_train_preproc, y_train_preproc), val_data=(x_val_preproc, y_val_preproc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufKylXAeOVCU"
      },
      "source": [
        "*ETAPE 4 :* \n",
        "\n",
        "La méthode [`lr_find`](https://amaiya.github.io/ktrain/core.html#ktrain.core.Learner.lr_find) implémente la méthode [*Cyclical Learning Rates*](https://arxiv.org/abs/1506.01186) qui permet d'estimer un lr sans une exploration systématique d'un lr qui décroit. L'approche peut prendre seulement le temps de quelques itérations (i.e. du traitement de quelques batchs) sans même tourner sur plusieurs époques. Pour rappel, une époque correspond au traitement de l'ensemble des données d'entrainements par lot/batch d'exemples. *For example,\n",
        "CIFAR-10 has 50, 000 training images and the batchsize is\n",
        "100 so an epoch = 50, 000/100 = 500 iterations*. \n",
        "\n",
        "Then plots loss as learning rate is increased.\n",
        "\n",
        "*Highest learning rate corresponding to a still falling loss should be chosen*. \"[The point of optimality [...] is the point on the training rate plot where the slope is steepest in the downwards direction. That's because this plot shows the loss after one epoch. Points before the steepest slope are training too slowly. Points after the steepest slope are at risk of training too quickly: usually, but not always (it didn't happen in this demo case), they will fall off the mountain in terms of loss because they jump past the point of optimality](https://www.kaggle.com/code/residentmario/finding-an-optimal-learning-rate-with-lr-finder)\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9_32BN5Ur7e"
      },
      "outputs": [],
      "source": [
        "# recherche d'un bon taux d'apprentissage \n",
        "# you can set max_epochs (e.g., max_epochs=5) to estimate LR\n",
        "fasttext_learner.lr_find() # \n",
        "fasttext_learner.lr_plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0eWERAHZ_Go"
      },
      "source": [
        "*ETAPE 5 :* \n",
        "\n",
        "[autofit](https://amaiya.github.io/ktrain/core.html#ktrain.core.Learner.autofit)\n",
        "Automatically train model using a default learning rate schedule shown to work well in practice.  By default, this method currently employs a triangular learning rate policy (https://arxiv.org/abs/1506.01186).  \n",
        "During each epoch, this learning rate policy varies the learning rate from lr/10 to lr and then back to a low learning rate that is near-zero. \n",
        "If epochs is None, then early_stopping and reduce_on_plateau are atomatically\n",
        "set to 6 and 3, respectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOMvkq1ZPXV2"
      },
      "source": [
        "#### QUESTION : training\n",
        "\n",
        "* Sur le graph ci-dessus, repérez approximativement la puissance `n` de `1/10^n` où la chute de la loss devient importante. Testez avec cette valeur comme learning rate, observez votre performance (accuracy) sur le train et le val.\n",
        "\n",
        "Dans l'extrait de log ci-dessous, loss et accuracy concerne le corpus de train tandis que  val_loss et val_accuracy le corpus de validation (sur un corpus sample).\n",
        "```\n",
        "Epoch 12/1024\n",
        "244/250 [============================>.] - ETA: 0s - loss: 0.1109 - accuracy: 0.9589\n",
        "Restoring model weights from the end of the best epoch: 7.\n",
        "250/250 [==============================] - 1s 6ms/step - loss: 0.1108 - accuracy: 0.9589 - val_loss: 0.3864 - val_accuracy: 0.8820\n",
        "Epoch 12: early stopping\n",
        "```\n",
        "* Est-ce normal d'observer un écart d'accuracy entre le train et le val ?\n",
        "* Stoquez le score obtenu en dernière étape, et relancez le finetuning sans changer le paramétrage. Obtenez-vous les mêmes résultats ? Pourquoi ? La réponse est à chercher dans l'initiatilisation des poids du réseau neuronal.\n",
        "* Volontairement (et selon le temps qu'a pris votre entraînement) testez des lr avec d'autres puissances de 10 (0.1, 0.01, 0.001, 0.0001). Attention les lr les plus petites prendront le plus de temps !!! Est-ce que cela marche mieux ? Attention, suivant votre choix de lr l'entraînement peut prendre quelques minutes à au moins 1h....\n",
        "\n",
        "Mon meilleur score est `loss: 0.1108 - accuracy: 0.9589 - val_loss: 0.3864 - val_accuracy: 0.8820` sur le corpus sample et vous ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vY7oJU7U4G0"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.01\n",
        "\n",
        "fasttext_learner.autofit(LEARNING_RATE)\n",
        "# Epoch 16/1024\n",
        "# 4491/4500 [============================>.] - ETA: 0s - loss: 0.1540 - accuracy: 0.9418Restoring model weights from the end of the best epoch: 11.\n",
        "# 4500/4500 [==============================] - 25s 6ms/step - loss: 0.1539 - accuracy: 0.9418 - val_loss: 0.2173 - val_accuracy: 0.9206\n",
        "# Epoch 16: early stopping\n",
        "\n",
        "# wi all allocine training dataset (at least one hour run) \n",
        "#learner.autofit(0.00001)\n",
        "# Epoch 129/1024\n",
        "# 4497/4500 [============================>.] - ETA: 0s - loss: 0.2197 - accuracy: 0.9135Restoring model weights from the end of the best epoch: 124.\n",
        "# 4500/4500 [==============================] - 29s 6ms/step - loss: 0.2197 - accuracy: 0.9135 - val_loss: 0.2058 - val_accuracy: 0.9204\n",
        "# Epoch 129: early stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqItionrS_z0"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save/reload a model\n",
        "\n",
        "https://github.com/amaiya/ktrain/blob/master/FAQ.md#how-do-i-resume-training-from-a-saved-checkpoint"
      ],
      "metadata": {
        "id": "ad4FYJW1GBy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save Predictor (i.e., model and Preprocessor instance) after partially training\n",
        "ktrain.get_predictor(fasttext_learner.model, preproc).save('fasttext_allocine.model+preproc')\n",
        "\n",
        "# reload Predictor and extract model\n",
        "#model = ktrain.load_predictor('/tmp/my_predictor').model"
      ],
      "metadata": {
        "id": "2v3AG0hQGBzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySqltk4KTHJ7"
      },
      "source": [
        "*ETAPE 6 :* Le code ci-dessous permet d'utiliser le modèle. Notez que le paramétrage `return_proba=True` permet d'obtenir les probas... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOiht1hPani6"
      },
      "outputs": [],
      "source": [
        "fasttext_predictor = ktrain.get_predictor(fasttext_learner.model, preproc)\n",
        "\n",
        "data = [ \"Ce film était horrible ! L'intrigue était ennuyeuse. Le jeu d'acteur était correct, cependant.\",\n",
        "         \"Le film est vraiment nul. Je veux qu'on me rende mon argent.\",\n",
        "        \"Quelle belle comédie romantique. 10/10 à revoir !\"]\n",
        "\n",
        "# Makes predictions for a list of strings where each string is a document or text snippet.\n",
        "print (fasttext_predictor.predict(data))\n",
        "\n",
        "# If return_proba is True, returns probabilities of each class.\n",
        "print (fasttext_predictor.predict(data,  return_proba=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5l8TyRWThZF"
      },
      "source": [
        "#### QUESTION : évaluation qualitative légère\n",
        "\n",
        "* Tester le modèle. Arrivez-vous à piéger le modèle ? Avec quelle phrase (donnez le code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkowscJhTiPh"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA5hf8I1dXul"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNZjuU8Bc4qI"
      },
      "source": [
        "### QUESTION : Evaluation quantitative\n",
        "\n",
        "Génération d'une hypothèse pour les *review* du corpus de test (`list(test_df['review'])`)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reviews \n",
        "x_test = list(test_df['review'])\n",
        "\n",
        "# labels (gold) \n",
        "y_test = list(test_df['positive'])"
      ],
      "metadata": {
        "id": "V6BGVxPPvzvd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32x1JdOCc5Dw"
      },
      "outputs": [],
      "source": [
        "# hypothèse\n",
        "y_hyp = [0 if h == 'negative' else 1 for h in fasttext_predictor.predict(x_test) ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNPvtEt0PzK4"
      },
      "source": [
        "\n",
        "\n",
        "[SemEval 2022 isarcasmeval task](https://sites.google.com/view/semeval2022-isarcasmeval) used the F1-score for the sarcastic class. sklearn permet de spécifier cette mesure avec le paramètre `average = 'binary'` éventuellement accompagné du paramètre qui indique la classe à considérer `pos_label = 1`. This metric should not be confused with the regular macro-F1. sklearn permet de calculer un score macro à l'aide de la valeur `average = 'macro'`.\n",
        "Sans paramétrage c'est le F1-score d'une classe qui est retourné. Les performances rapportées par [Theophile Blart utilise aussi ce paramétrage par défaut](https://github.com/TheophileBlard/french-sentiment-analysis-with-bert)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YI0p4xFFPzch",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "73ff7458-7b43-43d7-ab5a-a5b3c21696d7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-db23f447e36c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf1_positive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mf1_positive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#OR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_hyp' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "f1_positive = f1_score(y_test, y_hyp, average = \"binary\", pos_label = 1)\n",
        "f1_positive\n",
        "#OR\n",
        "#p_score_positive = precision_score(y_test, y_hyp, average = \"binary\", pos_label = 1)\n",
        "#r_score_positive = recall_score(y_test, y_hyp, average = \"binary\", pos_label = 1)\n",
        "#f1_positive = (2*p_score_positive*r_score_positive)/(p_score_positive +r_score_positive)\n",
        "\n",
        "# 5 %   0.8922764227642277\n",
        "# 100 % 0.9181408981471789"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ1neD23ao8T"
      },
      "source": [
        "[Theophile Blard rapporte des performances avec différents modèles (CamemBERT, RNN, TF-IDF + LogReg, CNN, fastText (unigrams)) sur le dataset Allociné](https://github.com/TheophileBlard/french-sentiment-analysis-with-bert). Comment vous positionnez-vous par rapport à ses résultats ?   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApxsjcDHE4Im"
      },
      "source": [
        "## NBSVM\n",
        "\n",
        "Le code ci-dessous utilise le même pré-traitement que précédemment et applique un modèle neuronal plus \"simple\" que fasttext.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGp2mrBsg17I"
      },
      "outputs": [],
      "source": [
        "# load an NBSVM model\n",
        "nbsvm_model = text.text_classifier('nbsvm', (x_train_preproc, y_train_preproc), preproc=preproc)\n",
        "nbsvm_learner = ktrain.get_learner(nbsvm_model, train_data=(x_train_preproc, y_train_preproc), val_data=(x_val_preproc, y_val_preproc))\n",
        "\n",
        "# fine tune\n",
        "LEARNING_RATE = 0.01\n",
        "nbsvm_learner.autofit(LEARNING_RATE)\n",
        "# Epoch 6/1024\n",
        "#4498/4500 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9544Restoring model weights from the end of the best epoch: 1.\n",
        "#4500/4500 [==============================] - 24s 5ms/step - loss: 0.1264 - accuracy: 0.9544 - val_loss: 0.2677 - val_accuracy: 0.9161\n",
        "#Epoch 6: early stopping\n",
        "\n",
        "# Finally, we will fit our model using and [SGDR learning rate](https://github.com/amaiya/ktrain/blob/master/example-02-tuning-learning-rates.ipynb) schedule by invoking the fit method with the cycle_len parameter (along with the cycle_mult parameter).\n",
        "# learner.fit(0.001, 3, cycle_len=1, cycle_mult=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "évaluation"
      ],
      "metadata": {
        "id": "JsEYbrTGxtv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# données\n",
        "x_test = list(test_df['review']) # reviews \n",
        "y_test = list(test_df['positive']) # labels (gold) \n",
        "\n",
        "# prédiction\n",
        "nbsvm_predictor = ktrain.get_predictor(nbsvm_learner.model, preproc)\n",
        "y_hyp = [0 if h == 'negative' else 1 for h in nbsvm_predictor.predict(x_test) ]\n",
        "\n",
        "# évaluation\n",
        "print(f1_score(y_test, y_hyp))\n",
        "\n",
        "# 5   % 0.903981180311341\n",
        "# 100 % 0.9203585411085926\n"
      ],
      "metadata": {
        "id": "uQhc_d1F_St9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "save/reload a model\n",
        "\n",
        "https://github.com/amaiya/ktrain/blob/master/FAQ.md#how-do-i-resume-training-from-a-saved-checkpoint"
      ],
      "metadata": {
        "id": "j85WVlKO54h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save Predictor (i.e., model and Preprocessor instance) after partially training\n",
        "ktrain.get_predictor(nbsvm_learner.model, preproc).save('nbsvm_allocine.model+preproc')\n",
        "\n",
        "# reload Predictor and extract model\n",
        "#model = ktrain.load_predictor('/tmp/my_predictor').model"
      ],
      "metadata": {
        "id": "vZ8IjKQWFhFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xveDFwSnUzj1"
      },
      "source": [
        "#### QUESTION\n",
        "\n",
        "* Le modèle nbsvm est-il plus performant que le précédent sur les données de validations ? Vous pouvez tester aussi différents learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilof7YlQNtfz"
      },
      "source": [
        "## BERT \n",
        "\n",
        "L'usage du modèle bert requiert que l'on change le prétraitement des données en entrée. Le code suivant réalise le prétraitement, charge un modèle bert et lance la personnalisation (fine tuning) sur 1 cycle avec taux d'apprentissage fixé.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "iAZQI0ucQWSa",
        "outputId": "9d8fcd20-9fdb-4bb8-b7a8-cfe913292e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['negative', 'positive']\n",
            "   negative  positive\n",
            "0         0         1\n",
            "1         1         0\n",
            "2         1         0\n",
            "3         0         1\n",
            "4         1         0\n",
            "['negative', 'positive']\n",
            "   negative  positive\n",
            "0         0         1\n",
            "1         0         1\n",
            "2         1         0\n",
            "3         1         0\n",
            "4         1         0\n",
            "downloading pretrained BERT model (multi_cased_L-12_H-768_A-12.zip)...\n",
            "[██████████████████████████████████████████████████]\n",
            "extracting pretrained BERT model...\n",
            "done.\n",
            "\n",
            "cleanup downloaded zip...\n",
            "done.\n",
            "\n",
            "preprocessing train...\n",
            "language: fr\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "done."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Multi-Label? False\n",
            "preprocessing test...\n",
            "language: fr\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "done."
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ETAPE 1 \n",
        "(x_train_preproc, y_train_preproc), (x_val_preproc, y_val_preproc), preproc = text.texts_from_df (train_df, \n",
        "                      'review',\n",
        "                      label_columns = [\"negative\", \"positive\"],\n",
        "                      val_df= val_df, # if None, 10% of data will be used for validation\n",
        "                      ##max_features=NUM_WORDS, \n",
        "                      #maxlen=MAXLEN,\n",
        "                      preprocess_mode='bert' \n",
        "                      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kpI2LG_Wvqe"
      },
      "source": [
        "#### QUESTION\n",
        "* Exécutez le code en observant l'occupation de la RAM et attendez jusqu'à voir le temps prévisionnel s'afficher. Etes-vous choqué par le temps affiché ? BERT est très gros. Il requiert un peu de temps... Sur l'extrait le temps devrait être de 15 minutes à quelques heures sur le corpus complet.\n",
        "* Si vous êtes en phase d'exploration et que vous ne travaillez pas sur un extrait des données, stoppez l'exécution dans la cellule, et tentez de changer la taille du batch_size (quantité de données traitées en même temps). Vous pouvez tester 12 et éventuellement 128... comme valeurs. Ne lachez pas la barre de la RAM des yeux. Ça passe ? Quel problème rencontrez-vous ? \n",
        "* Reprenez... et passez à la suite..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0ujB3xihhnG",
        "outputId": "7cf8b989-59c1-42b3-9708-6861a89e1433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Multi-Label? False\n",
            "maxlen is 400\n",
            "done.\n",
            "\n",
            "\n",
            "begin training using onecycle policy with max lr of 2e-05...\n",
            "26667/26667 [==============================] - 18569s 695ms/step - loss: 0.1856 - accuracy: 0.9256 - val_loss: 0.1116 - val_accuracy: 0.9588\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f72f1272b10>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# ETAPE 2 et 3\n",
        "bert_model = text.text_classifier('bert', (x_train_preproc, y_train_preproc) , preproc=preproc)\n",
        "bert_learner = ktrain.get_learner(bert_model, \n",
        "                             train_data=(x_train_preproc, y_train_preproc), \n",
        "                             val_data=(x_val_preproc, y_val_preproc), \n",
        "                             batch_size=6)\n",
        "# ETAPE 5\n",
        "bert_learner.fit_onecycle(2e-5, 1)\n",
        "\n",
        "# Is Multi-Label? False\n",
        "# maxlen is 400\n",
        "# begin training using onecycle policy with max lr of 2e-05...\n",
        "# 26667/26667 [==============================] - 18569s 695ms/step - loss: 0.1856 - accuracy: 0.9256 - val_loss: 0.1116 - val_accuracy: 0.9588\n",
        "# <keras.callbacks.History at 0x7f72f1272b10>\n",
        "# Exécution ~ 5h"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "évaluation"
      ],
      "metadata": {
        "id": "N3x1kk3U0P2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# données\n",
        "x_test = list(test_df['review'])   # reviews \n",
        "y_test = list(test_df['positive']) # labels (gold) \n",
        "\n",
        "# prédiction\n",
        "bert_predictor = ktrain.get_predictor(bert_learner.model, preproc)\n",
        "y_hyp = [0 if h == 'negative' else 1 for h in bert_predictor.predict(x_test) ]\n",
        "\n",
        "# évaluation\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "print(f1_score(y_test, y_hyp))\n",
        "# 100 % 0.9581906836808797"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bbb8db3-71dc-4816-a2b7-73cb7be5f6a4",
        "id": "NH7qjU610P2d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9581906836808797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save/reload a model\n",
        "\n",
        "https://github.com/amaiya/ktrain/blob/master/FAQ.md#how-do-i-resume-training-from-a-saved-checkpoint"
      ],
      "metadata": {
        "id": "0xEUlatHGQzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save Predictor (i.e., model and Preprocessor instance) after partially training\n",
        "ktrain.get_predictor(bert_learner.model, preproc).save('bert_allocine.model+preproc')\n",
        "\n",
        "# reload Predictor and extract model\n",
        "#model = ktrain.load_predictor('/tmp/my_predictor').model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSdge2OQzCRf",
        "outputId": "d931a005-1e6a-42cf-93bb-4f4419fa929a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) Input-Token, Input-Segment with unsupported characters which will be renamed to input_token, input_segment in the SavedModel.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f1aed390> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72f1ae1690> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f180c810> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f18ffd10> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72f1a5e690> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f1909a50> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f1742090> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72f18f2bd0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f0f1fe90> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f1a5e750> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72f19bcd50> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f19c2590> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f159fe90> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72f15a26d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f0f00d10> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f12f69d0> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72f1304350> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f1309bd0> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f10273d0> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72f0fb93d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f0fc5290> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f193df50> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72f1706d50> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f17a39d0> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f1636ed0> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72f1645ed0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f1652cd0> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f14cea50> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72f156ee50> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f14bc290> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f13dca90> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72f13eb950> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f14eedd0> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f12c4fd0> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f72f13c91d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'keras_multi_head.multi_head_attention.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f1264a90> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f72f11d71d0> has the same name 'LayerNormalization' as a built-in Keras object. Consider renaming <class 'keras_layer_normalization.layer_normalization.LayerNormalization'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0r6CivJbBUF"
      },
      "source": [
        "## Modèle \"à la bert\" issu de hugging face\n",
        "\n",
        "De nombreux [modèles sont disponibles sur HuggingFace](https://huggingface.co/models). \n",
        "\n",
        "On va tester un modèle plus léger que BERT à savoir 'distilbert-base-uncased'. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = list(train_df['review'])\n",
        "y_train = list(train_df['positive'])\n",
        "\n",
        "x_val = list(val_df['review'])\n",
        "y_val = list(val_df['positive'])\n",
        "\n",
        "x_test = list(test_df['review'])\n",
        "y_test = list(test_df['positive'])\n",
        "\n",
        "## Do not consider the following\n",
        "\n",
        "## Convert a list of string into lists of int then into np.array.\n",
        "## This step should be skiped when running BERT\n",
        "\n",
        "#import numpy as np\n",
        "#y_train = np.array(list(map(int, y_train)))\n",
        "#y_val = np.array(list(map(int, y_val)))\n",
        "#y_test = np.array(list(map(int, y_test)))\n",
        "\n",
        "#print ('x_train', len(x_train), x_train[:10])\n",
        "#print ('y_train',  len(y_train), y_train[:10])\n",
        "#print ('x_val', len(x_val), x_val[:10])\n",
        "#print ('y_val',  len(y_val), y_val[:10])\n",
        "#print ('x_test', len(x_test))\n",
        "#print ('y_test',  len(y_test))"
      ],
      "metadata": {
        "id": "kDBwr4fvzOCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52DqPKJE9d4s"
      },
      "outputs": [],
      "source": [
        "import ktrain\n",
        "from ktrain import text\n",
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "#MODEL_NAME = 'albert-base-v2'\n",
        "#MODEL_NAME = 'camembert-base'\n",
        "\n",
        "CLASS_NAMES = [\"negative\", \"positive\"]\n",
        "\n",
        "distilbert_preproc = text.Transformer(MODEL_NAME, maxlen=500, class_names=CLASS_NAMES)\n",
        "train_preproc = distilbert_preproc.preprocess_train(x_train, y_train)\n",
        "val_preproc = distilbert_preproc.preprocess_test(x_val, y_val)\n",
        "#print (type(trn))\n",
        "#x_train, y_train = trn\n",
        "#x_test, y_test = val  \n",
        "distilbert_model = distilbert_preproc.get_classifier()\n",
        "# batch_size (int):              Batch size to use in training. default:32  \n",
        "#learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6) # 128 dépend de la ram dispo 13 Go par défaut\n",
        "distilbert_learner = ktrain.get_learner(distilbert_model, train_data=train_preproc, val_data=val_preproc, batch_size=12)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#del distilbert_preproc\n",
        "#del train_preproc\n",
        "#del val_preproc\n",
        "#del distilbert_model\n",
        "#del distilbert_learner"
      ],
      "metadata": {
        "id": "VrmoU03yM6-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqM1-j84-4eD"
      },
      "outputs": [],
      "source": [
        "distilbert_learner.fit_onecycle(0.01, 1)\n",
        "#learner.fit_onecycle(8e-5, 4)\n",
        "#8e-5 = 8 + 10^(-5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# données\n",
        "x_test = list(test_df['review'])   # reviews \n",
        "y_test = list(test_df['positive']) # labels (gold) \n",
        "\n",
        "# prédiction\n",
        "distilbert_predictor = ktrain.get_predictor(distilbert_learner.model, distilbert_preproc)\n",
        "y_hyp = [0 if h == 'negative' else 1 for h in distilbert_predictor.predict(x_test) ]\n",
        "\n",
        "# évaluation\n",
        "print(f1_score(y_test, y_hyp))\n",
        "# distilbert-base-uncased train sur 5 % 0.6577181208053691\n",
        "# distilbert-base-uncased train sur 100 % 0.6482833198161666\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH253Lbk_yXi",
        "outputId": "65b35bcb-b002-476a-e5fe-60a312ec9538"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6482833198161666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "backup model"
      ],
      "metadata": {
        "id": "qw8UvziY5vmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('distilbert_allocine.model', 'wb') as model_file:\n",
        "  pickle.dump(distilbert_learner, model_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3mr7y5I5eLf",
        "outputId": "55ca1691-af49-45e9-a231-a33ca0e65a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, transformer_layer_call_fn, transformer_layer_call_and_return_conditional_losses, LayerNorm_layer_call_fn while saving (showing 5 of 164). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeM-jNIyb_7M"
      },
      "source": [
        "#### QUESTION\n",
        "* Quelle performance obtenez-vous avec BERT et distilBERT pour quel temps d'entraînement ? Quelle performance comparativement aux 2 modèles précédents nbsvm et fasttext ?\n",
        "* Quelles risques prenez-vous à ne pas réaliser les mêmes prétraitements ou normalisations (voire utiliser des outils différents pour réaliser les mêmes prétraitements ou normalisations supposées) que ceux réalisés sur les corpus ayant servis à construire les modèles ? \n",
        "* En résumé, en mettant dans la balance les questions de performance, les questions de taille de modèles, de temps de \"fine-tuning\"... quelles conclusions faites-vous de l'usage des modèles simples vs les modèles plus complexes à la BERT ?\n",
        "\n",
        "Si le code tourne toujours passez à la question suivante pour gagner du temps..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qni8ELuBcBB1"
      },
      "source": [
        "#### VOTRE REPONSE\n",
        "\n",
        "**TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzOyw5WCCJX4"
      },
      "source": [
        "# Devoir à rendre \n",
        "[Theophile Blard rapporte des performances avec différents modèles (CamemBERT, RNN, TF-IDF + LogReg, CNN, fastText (unigrams)) sur le dataset Allociné](https://github.com/TheophileBlard/french-sentiment-analysis-with-bert). \n",
        "\n",
        "\n",
        "| Model                                        | Validation Accuracy | Validation F1-Score | Test Accuracy | Test F1-Score |\n",
        "| :--------------------------------------------|--------------------:| -------------------:| -------------:|--------------:|\n",
        "| **[CamemBERT][bert.ipynb]**                  |           **97.39** |           **97.36** |     **97.44** |     **97.34** |\n",
        "| [RNN]                    |               94.39 |               94.34 |         94.58 |         94.39 |\n",
        "| [TF-IDF + LogReg]              |               94.35 |               94.29 |         94.38 |         94.19 |\n",
        "| [CNN]                    |               93.69 |               93.72 |         94.10 |         93.98 |\n",
        "| [fastText (unigrams)]    |               92.88 |               92.75 |         92.90 |         92.57 |\n",
        "\n",
        "\n",
        "Arriverez-vous à faire mieux ?\n",
        "\n",
        "Bien entendu vous avez le droit de choisir les modèles et leur configuration.\n",
        "\n",
        "Explorez les thèmes suivants\n",
        "- data augmentation\n",
        "- model ensembling \n",
        "\n",
        "Ecrivez un rapport d'expériences.\n",
        "\n",
        "## Topics\n",
        "\n",
        "### Data augmentation in NLP\n",
        "TextAttack is a Python framework for adversarial attacks, adversarial training, and [data augmentation in NLP](https://textattack.readthedocs.io/en/latest/2notebook/3_Augmentations.html).\n",
        "\n",
        "Here some hints for [multi-language attacks](https://textattack.readthedocs.io/en/latest/2notebook/Example_4_CamemBERT.html)\n",
        "\n",
        "### Model ensembling \n",
        "\n",
        "- Train/fine-tune multiple models from various architectures and/or dataset folds\n",
        "- Then ensemble them by (hard/soft) voting mechanism or something else...\n",
        "\n",
        "\n",
        "\n",
        "## VOTRE REPONSE\n",
        "\n",
        "**TODO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU6olWGuGzN6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tzxoRA_RGl4"
      },
      "source": [
        "# Références\n",
        "* Text Classification Example: Sentiment Analysis with IMDb Movie Reviews¶ https://nbviewer.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-04-text-classification.ipynb\n",
        "* https://nbviewer.org/github/amaiya/ktrain/blob/master/examples/text/IMDb-BERT.ipynb\n",
        "* ktrain examples of Binary text Classification (Sentiment Analysis with IMDb Movie Reviews) with a nbsvm model (also a bit of bert)  and \n",
        "Multi-Label Text Classification (toxic comments) with fasttext https://nbviewer.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-04-text-classification.ipynb\n",
        "* Text Classification with Hugging Face Transformers in ktrain https://github.com/amaiya/ktrain/blob/master/tutorials/tutorial-A3-hugging_face_transformers.ipynb\n",
        "* ktrain api documentation https://amaiya.github.io/ktrain/\n",
        "* Evaluation of various models (CamemBERT, RNN, TF-IDF + LogReg, CNN, fastText (unigrams)) on allocine dataset https://github.com/TheophileBlard/french-sentiment-analysis-with-bert\n",
        "* The Allociné dataset is a French-language dataset for sentiment analysis. The texts are movie reviews written between 2006 and 2020 by members of the Allociné.fr community for various films. It contains 100k positive and 100k negative reviews divided into train (160k), validation (20k), and test (20k). \n",
        "https://huggingface.co/datasets/allocine\n",
        "* Amazon reviews for three product categories: books, DVD, and music. Each sample contains a review text and the associated rating from 1 to 5 stars. Reviews rated above 3 is labeled as positive, and those rated less than 3 is labeled as negative. https://github.com/getalp/Flaubert/tree/master/flue\n",
        "* Twitter API between May and September 2018. The sentiment was generated thanks to AWS Comprehend API. For Spanish and French, tweets were first translated to English using Google Translate, and then analyzed with AWS Comprehend. https://github.com/charlesmalafosse/open-dataset-for-sentiment-analysis/\n",
        "* French dataset for sentiment analysis (Data translated from English to French).  A collection of over 1.5 Million tweets data translated to French, with their sentiment. The data has two columns, polarity and status https://github.com/gamebusterz/French-Sentiment-Analysis-Dataset\n",
        "* English Classification datasets https://ludwig.ai/latest/examples/text_classification/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyPZ+gUgScB+yVQUs6rAur9g",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}